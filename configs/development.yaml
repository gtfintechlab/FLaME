# Development Configuration using Ollama for local testing
# This saves API costs by using local models

# Use Ollama's qwen model for all development
model: "ollama/qwen2.5:1.5b"

# Ollama-specific settings
api_base: "http://localhost:11434"

# Keep the same task settings as default
tasks:
  - fomc
  # Add more tasks as needed for testing

# Generation parameters (optimized for local testing)
max_tokens: 128
temperature: 0.0  # Deterministic for reproducible tests
top_p: 0.9
top_k: null
repetition_penalty: 1.0

# Can use smaller batches for local testing
batch_size: 20
prompt_format: zero_shot

# Timeout for Ollama (first inference can be slow)
timeout: 120

# Logging configuration (more verbose for development)
logging:
  level: "DEBUG"           # More verbose for development
  console:
    enabled: true
    level: "INFO"
  file:
    enabled: true
    level: "DEBUG"
    max_size_mb: 10
    backup_count: 5
  components:
    litellm: "INFO"        # See LiteLLM's Ollama interactions
    batch_utils: "DEBUG"
    inference: "DEBUG"
    evaluation: "DEBUG"