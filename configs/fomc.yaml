# Core settings
dataset: fomc
mode: inference
file_name: null

# Model parameters
models:
  inference: "together_ai/meta-llama/Llama-2-70b"  # Model for generating responses
  extraction: "together_ai/meta-llama/Llama-2-7b"  # Model for extracting/evaluating responses
max_tokens: 128
temperature: 0.0
top_p: 0.9
top_k: null
repetition_penalty: 1.0
batch_size: 250

# Dataset parameters
dataset_org: "gtfintechlab"
sample_size: 1
method: null
seeds: null
splits: null

# Other parameters
prompt_format: "superflue"