# Core settings
dataset: mmlu
mode: inference  # Options: inference, evaluate
file_name: null

# Model parameters
model: "together_ai/meta-llama/Llama-2-7b"
max_tokens: 32
temperature: 0.0  # Use 0 for deterministic answers
top_p: 0.9
top_k: null
repetition_penalty: 1.0
batch_size: 10  # Process questions in batches

# MMLU specific parameters
mmlu_subjects:
  - "econometrics"
  - "high_school_macroeconomics"
  - "high_school_microeconomics"
mmlu_split: "test"  # Options: dev, validation, test
mmlu_num_few_shot: 5  # Number of few-shot examples

# Other parameters
prompt_format: "ferrari"

# Logging and output
save_raw_responses: true  # Save complete model responses for debugging
# verbose_logging: true  # Enable detailed logging

# Example usage:
# Inference:
#   python main.py --config configs/mmlu.yaml --mode inference
# 
# Evaluation:
#   python main.py --config configs/mmlu.yaml --mode evaluate --file_name path/to/inference_results.csv