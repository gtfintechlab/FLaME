# Ollama Configuration for Local Development
# This configuration uses local Ollama models to save costs during development

# Model configuration - using Ollama's qwen2.5:1.5b
model: "ollama/qwen2.5:1.5b"
api_base: "http://localhost:11434"

# Generation parameters
temperature: 0.0  # Deterministic for testing
max_tokens: 512
top_p: 1.0
top_k: 0
repetition_penalty: 1.0

# Batch processing settings
batch_size: 10  # Can be higher for local inference
max_retries: 3
retry_delay: 1.0

# Timeout settings
timeout: 120  # Ollama can be slower for first inference

# Output settings
output_dir: "results"
save_results: true

# Evaluation settings
evaluation_batch_size: 20
evaluation_timeout: 120

# Development mode flags
debug: true
verbose: true

# Prompt format (same as default.yaml)
prompt_format: "zero_shot"  # Options: zero_shot, few_shot

# Task-specific overrides (if needed)
task_overrides:
  # Example: For tasks needing more tokens
  finqa:
    max_tokens: 1024
  convfinqa:
    max_tokens: 1024