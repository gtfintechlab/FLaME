# Core settings
models:
  inference: "together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
dataset: fomc
mode: inference
log_level: "ERROR"
litellm_log_level: "ERROR"
file_name: null

# Model parameters
max_tokens: 10
temperature: 0.0
top_p: 0.9
top_k: 50
repetition_penalty: 1.0
batch_size: 100

# Dataset parameters
dataset_org: "gtfintechlab"
sample_size: 1
method: null
seeds: null
splits: null

# Other parameters
prompt_format: "superflue"