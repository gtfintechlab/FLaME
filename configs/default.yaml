# Core settings
dataset: fomc
mode: inference  # inference or evaluate
file_name: null  # Required for evaluate mode
# eg: results/finbench/finbench_together_ai/meta-llama/Llama-3-8b-chat-hf_10_12_2024.csv

# Model configuration
models:
  inference: "meta-llama/Llama-2-70b"  # Model for generating responses
  extraction: "meta-llama/Llama-2-7b"  # Model for extracting/evaluating responses

# Model parameters
max_tokens: 128
temperature: 0.0
top_p: 0.9
top_k: null
repetition_penalty: 1.0
batch_size: 10

# Dataset parameters
sample_size: 10
method: random
seeds: [42, 123, 456]
splits: [5768, 78516, 944601]

# Other parameters
prompt_format: "superflue"