{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "946c77d4-a5a6-4d2c-a3dc-f0671d6e73a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find llama2_sft.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mglennmatlin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================== HUGGINGFACE ======================\n",
    "HF_AUTH = \"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\"\n",
    "# ====================== WEIGHTS AND BIASES ======================\n",
    "import os\n",
    "import wandb\n",
    "WANDB_PROJECT = f\"llama2_sft_fomc\"\n",
    "# Set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"] = WANDB_PROJECT\n",
    "# Turn off save your trained model checkpoint to wandb (our models are too large)\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
    "# Turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"fa69ffc6a97578da0410b553042cbb8b3bf5fcaf\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = f\"llama2_sft\"\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a81af90-910b-4b1d-ae45-23f627bc0594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================== IMPORTS ======================\n",
    "# Standard Libraries\n",
    "import os\n",
    "import gc\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import NamedTuple, List, Type\n",
    "from IPython.display import display\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import nltk\n",
    "\n",
    "# PyTorch and HuggingFace Libraries\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import logging as hf_logging\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    TrainingArguments,\n",
    "    # DataCollatorForLanguageModeling,\n",
    "    # LlamaConfig,\n",
    "    # LlamaForCausalLM,\n",
    "    # LlamaModel,\n",
    "    # LlamaTokenizer,\n",
    "    # TextGenerationPipeline,\n",
    "    # Trainer,\n",
    "    # pipeline,\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    AutoPeftModelForCausalLM,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b898c011-1db3-456d-a6b0-e0c8e12e2200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================== HUGGINGFACE ======================\n",
    "organization = \"gtfintechlab\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# ====================== TASK PARAMETERS ======================\n",
    "task_name = \"fomc_communication\"\n",
    "# seeds = (5768, 78516, 944601)\n",
    "# seed = seeds[0]\n",
    "seed = 42\n",
    "\n",
    "# ====================== LOGGING PARAMETERS ======================\n",
    "report_to = \"tensorboard\"\n",
    "logging_dir = Path.home() / \"tensorboard\" / \"logs\"\n",
    "\n",
    "# ====================== MODEL PARAMETERS ======================\n",
    "model_parameters = \"7b\"\n",
    "model_id = f\"meta-llama/Llama-2-{model_parameters}-chat-hf\"\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "\n",
    "# ====================== PROMPT PARAMETERS ======================\n",
    "system_prompt = f\"\"\"Discard all previous instructions.\n",
    "Below is an instruction that describes a task.\n",
    "Write a response that appropriately completes the request.\n",
    "\"\"\"\n",
    "\n",
    "instruction_prompt = f\"\"\"Behave like you are an expert sentence classifier.\n",
    "Classify the following sentence from FOMC into 'HAWKISH', 'DOVISH', or 'NEUTRAL' class.\n",
    "Label 'HAWKISH' if it is corresponding to tightening of the monetary policy.\n",
    "Label 'DOVISH' if it is corresponding to easing of the monetary policy.\n",
    "Label 'NEUTRAL' if the stance is neutral.\n",
    "Provide a single label from the choices 'HAWKISH', 'DOVISH', or 'NEUTRAL' then stop generating text.\n",
    "\n",
    "The sentence:\n",
    "\"\"\"\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "BOS, EOS = \"<s>\", \"</s>\"\n",
    "\n",
    "repo_name = f\"{organization}/{model_name}_{task_name}\"\n",
    "\n",
    "# ====================== QLORA PARAMETERS ======================\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# ====================== SFT PARAMETERS ======================\n",
    "# Default maximum sequence length to use\n",
    "max_seq_length = 4096\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "neftune_noise_alpha = 5\n",
    "\n",
    "# ====================== CUDA PARAMETERS ======================\n",
    "# Enable fp16/bf16 training\n",
    "compute_dtype = torch.bfloat16\n",
    "fp16, bf16 = False, True\n",
    "\n",
    "cuda_n_gpus, cuda_max_memory = None, None # Determined dynamically at runtime\n",
    "\n",
    "device_map = \"auto\"  # Automatically determine the device map\n",
    "\n",
    "save_safetensors = True\n",
    "\n",
    "# ====================== BITSANDBYTES PARAMETERS ======================\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate 8-bit precision base model loading\n",
    "load_in_8bit = False\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_compute_dtype = compute_dtype\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_use_double_quant = False\n",
    "\n",
    "\n",
    "def configure_bnb(args):\n",
    "    \"\"\"\n",
    "    Configures BitsAndBytes based on the arguments provided.\n",
    "    \"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=args.load_in_4bit,\n",
    "        load_in_8bit=args.load_in_8bit,\n",
    "        bnb_4bit_use_double_quant=args.bnb_use_double_quant,\n",
    "        bnb_8bit_use_double_quant=args.bnb_use_double_quant,\n",
    "        bnb_4bit_quant_type=args.bnb_quant_type,\n",
    "        bnb_8bit_quant_type=args.bnb_quant_type,\n",
    "        bnb_4bit_compute_dtype=args.bnb_compute_dtype,\n",
    "        bnb_8bit_compute_dtype=args.bnb_compute_dtype,\n",
    "    )\n",
    "    return bnb_config\n",
    "\n",
    "\n",
    "# ====================== TRAININGARGUMENTS PARAMETERS ======================\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "args_output_dir = \"/fintech_3/20231018/results\"\n",
    "output_dir = Path(args_output_dir) / f\"{model_name}_{task_name}\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 12\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 8\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 8\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = False\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 3e-3\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"adamw_bnb_8bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 100\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 100\n",
    "\n",
    "load_best_model_at_end = True\n",
    "\n",
    "strategy = \"steps\"\n",
    "save_strategy = strategy\n",
    "logging_strategy = strategy\n",
    "evaluation_strategy = strategy\n",
    "\n",
    "disable_tqdm = True\n",
    "predict_with_generate = False # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0308c828-13a0-4c7a-ace3-444aeba11bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================== LOGGING SETUP ======================\n",
    "def setup_logging():\n",
    "    logger = logging.getLogger(\"llama2_finetune\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    hf_logging.set_verbosity(hf_logging.DEBUG)\n",
    "\n",
    "    # Create handlers\n",
    "    c_handler = logging.StreamHandler()\n",
    "    f_handler = logging.FileHandler(\"llama2_finetune.log\")\n",
    "    c_handler.setLevel(logging.DEBUG)\n",
    "    f_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Create formatters and add it to handlers\n",
    "    format = \"%(name)s - %(levelname)s - %(message)s\"\n",
    "    c_format = logging.Formatter(format)\n",
    "    f_format = logging.Formatter(format)\n",
    "    c_handler.setFormatter(c_format)\n",
    "    f_handler.setFormatter(f_format)\n",
    "\n",
    "    # Add handlers to the logger\n",
    "    logger.addHandler(c_handler)\n",
    "    logger.addHandler(f_handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ffbf4e-10d6-4985-8139-67e6cb1f8ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd53ccee-5ffc-4b0b-9a10-10968b769f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================== ARGUMENTS SETUP ======================\n",
    "\n",
    "# TODO: MOVE OUR DEFAULT VALUES INTO OUR DATA CLASS(ES)\n",
    "@dataclass\n",
    "class Args():\n",
    "    repo_name: str\n",
    "    task_name: str\n",
    "    system_prompt: str\n",
    "    instruction_prompt: str\n",
    "    seed: int\n",
    "    model_id: str\n",
    "    model_name: str\n",
    "    organization: str\n",
    "    lora_r: float\n",
    "    lora_alpha: float\n",
    "    lora_dropout: float\n",
    "    max_seq_length: int\n",
    "    packing: bool\n",
    "    device_map: str\n",
    "    load_in_4bit: bool\n",
    "    load_in_8bit: bool\n",
    "    bnb_compute_dtype: bool\n",
    "    bnb_use_double_quant: bool\n",
    "    bnb_quant_type: str\n",
    "    output_dir: str\n",
    "    num_train_epochs: int\n",
    "    fp16: bool\n",
    "    bf16: bool\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    gradient_checkpointing: bool\n",
    "    max_grad_norm: float\n",
    "    learning_rate: float\n",
    "    weight_decay: float\n",
    "    optim: str\n",
    "    lr_scheduler_type: str\n",
    "    max_steps: int\n",
    "    warmup_ratio: float\n",
    "    group_by_length: bool\n",
    "    save_steps: int\n",
    "    save_strategy: str\n",
    "    logging_strategy: str\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    neftune_noise_alpha: float\n",
    "    save_safetensors: bool\n",
    "    load_best_model_at_end: bool\n",
    "    disable_tqdm: bool\n",
    "    B_INST: str\n",
    "    E_INST: str\n",
    "    B_SYS: str\n",
    "    E_SYS: str\n",
    "    BOS: str\n",
    "    EOS: str\n",
    "    report_to: str\n",
    "    logging_dir: str\n",
    "    predict_with_generate: bool\n",
    "    cuda_n_gpus: int \n",
    "    cuda_max_memory: str\n",
    "\n",
    "\n",
    "def setup_args() -> Args:\n",
    "    args = Args(\n",
    "        repo_name=repo_name,\n",
    "        task_name=task_name,\n",
    "        system_prompt=system_prompt,\n",
    "        instruction_prompt=instruction_prompt,\n",
    "        seed=seed,\n",
    "        model_id=model_id,\n",
    "        model_name=model_id.split(\"/\")[-1],\n",
    "        organization=organization,\n",
    "        lora_r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=packing,\n",
    "        device_map=device_map,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "        bnb_compute_dtype=bnb_compute_dtype,\n",
    "        bnb_use_double_quant=bnb_use_double_quant,\n",
    "        bnb_quant_type=bnb_quant_type,\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        fp16=fp16,\n",
    "        bf16=bf16,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        gradient_checkpointing=gradient_checkpointing,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        optim=optim,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        max_steps=max_steps,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        group_by_length=group_by_length,\n",
    "        save_steps=save_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        logging_strategy=logging_strategy,\n",
    "        logging_steps=logging_steps,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        neftune_noise_alpha=neftune_noise_alpha,\n",
    "        save_safetensors=save_safetensors,\n",
    "        load_best_model_at_end=load_best_model_at_end,\n",
    "        disable_tqdm=disable_tqdm,\n",
    "        B_INST=B_INST,\n",
    "        E_INST=E_INST,\n",
    "        B_SYS=B_SYS,\n",
    "        E_SYS=E_SYS,\n",
    "        BOS=BOS,\n",
    "        EOS=EOS,\n",
    "        report_to=report_to,\n",
    "        logging_dir=logging_dir,\n",
    "        predict_with_generate=predict_with_generate,\n",
    "        cuda_n_gpus=cuda_n_gpus,\n",
    "        cuda_max_memory=cuda_max_memory\n",
    "    )\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ccfef69-80df-4336-a1eb-ea6eecebec2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============== SFT LOGGING FUNCTIONS ==================\n",
    "def log_trainable_parameters(model, logger):\n",
    "    \"\"\"\n",
    "    Logs the number of trainable parameters in the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model : torch.nn.Module - The model to log.\n",
    "    - logger : logging.Logger - Logger to use for logging the info.\n",
    "    \"\"\"\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    logger.info(\n",
    "        f\"Trainable params: {trainable_params} || \"\n",
    "        f\"All params: {total_params} || \"\n",
    "        f\"Trainable%: {100 * trainable_params / total_params}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def log_dtypes(model, logger):\n",
    "    \"\"\"\n",
    "    Logs the data types of the model parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - model : torch.nn.Module - The model to log.\n",
    "    - logger : logging.Logger - Logger to use for logging the info.\n",
    "    \"\"\"\n",
    "    dtypes = {}\n",
    "\n",
    "    for p in model.parameters():\n",
    "        dtype = p.dtype\n",
    "        dtypes[dtype] = dtypes.get(dtype, 0) + p.numel()\n",
    "\n",
    "    total = sum(dtypes.values())\n",
    "\n",
    "    for dtype, count in dtypes.items():\n",
    "        logger.info(f\"{dtype}: {count} ({100 * count / total:.2f}%)\")\n",
    "\n",
    "\n",
    "def log_and_save_info(model, logger, args):\n",
    "    \"\"\"\n",
    "    Log information and save it for further analysis.\n",
    "    \"\"\"\n",
    "    info_data = []\n",
    "\n",
    "    logger.debug(\"Getting the model's memory footprint...\")\n",
    "    memory_footprint = model.get_memory_footprint()\n",
    "    info_data.append([\"Memory Footprint\", memory_footprint])\n",
    "\n",
    "    logger.debug(f\"Model Dtypes before preparing for kbit training ...\")\n",
    "    dtypes_after = log_dtypes(\n",
    "        model, logger\n",
    "    )  # Assuming log_dtypes returns relevant data\n",
    "    info_data.append([\"Dtypes Before KBit Prep\", dtypes_after])\n",
    "\n",
    "    logger.debug(\"Using the prepare_model_for_kbit_training method from PEFT...\")\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model, use_gradient_checkpointing=args.gradient_checkpointing\n",
    "    )\n",
    "\n",
    "    logger.debug(f\"Model Dtypes after preparing for kbit training ...\")\n",
    "    dtypes_after = log_dtypes(\n",
    "        model, logger\n",
    "    )  # Assuming log_dtypes returns relevant data\n",
    "    info_data.append([\"Dtypes After KBit Prep\", dtypes_after])\n",
    "\n",
    "    logger.debug(\"Get module names for the linear layers where we add LORA adapters...\")\n",
    "    layers_for_adapters = find_all_linear_names(model, 4)\n",
    "    logger.debug(f\"Layers for Adapters: {layers_for_adapters}\")\n",
    "    info_data.append([\"Layers for Adapters\", layers_for_adapters])\n",
    "\n",
    "    logger.info(\"Create PEFT config for these modules and wrap the model to PEFT...\")\n",
    "    peft_config = create_peft_config(args, layers_for_adapters)\n",
    "\n",
    "    logger.info(f\"Model Dtypes before applying PEFT config ...\")\n",
    "    dtypes_before = log_dtypes(model, logger)\n",
    "    info_data.append([\"Dtypes Before PEFT Config\", dtypes_before])\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    logger.info(f\"Model Dtypes after applying PEFT config ...\")\n",
    "    dtypes_after_peft = log_dtypes(model, logger)\n",
    "    info_data.append([\"Dtypes After PEFT Config\", dtypes_after_peft])\n",
    "\n",
    "    logger.info(\"Information about the percentage of trainable parameters...\")\n",
    "    trainable_parameters = log_trainable_parameters(model, logger)\n",
    "    info_data.append([\"Trainable Parameters\", trainable_parameters])\n",
    "\n",
    "    # Convert the info_data list into a pandas DataFrame and save it\n",
    "    df = pd.DataFrame(info_data, columns=[\"Info\", \"Value\"])\n",
    "    df.to_csv(\"model_info.csv\", index=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def merge_evaluation_results(\n",
    "    baseline_results: dict, final_results: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge evaluation results for comparison.\n",
    "\n",
    "    Parameters:\n",
    "    baseline_results : dict - The baseline evaluation results.\n",
    "    final_results : dict - The fine-tuned evaluation results.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame - A DataFrame containing merged results.\n",
    "    \"\"\"\n",
    "    all_metrics = set(baseline_results.keys()).union(final_results.keys())\n",
    "    data = {\n",
    "        \"Metric\": list(all_metrics),\n",
    "        \"Baseline\": [baseline_results.get(metric, None) for metric in all_metrics],\n",
    "        \"After Fine-tuning\": [\n",
    "            final_results.get(metric, None) for metric in all_metrics\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "802a3450-247c-4d9c-b51c-793065beca49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== DATA SET PROCESSING FUNCTIONS ==========\n",
    "FOMC_COMMUNICATION_MAPPING = {\n",
    "    0: \"dovish\",\n",
    "    1: \"hawkish\",\n",
    "    2: \"neutral\"\n",
    "}\n",
    "\n",
    "# Function to decode the labels\n",
    "def decode_label(label_number):\n",
    "    return FOMC_COMMUNICATION_MAPPING.get(label_number, \"undefined\").upper()\n",
    "\n",
    "# Function to encode the labels\n",
    "def encode_label(label_name):\n",
    "    reversed_mapping = {v: k for k, v in FOMC_COMMUNICATION_MAPPING.items()}\n",
    "    return reversed_mapping.get(label_name.lower(), -1)\n",
    "\n",
    "def get_max_length(model: Type[torch.nn.Module]) -> int:\n",
    "    \"\"\"\n",
    "    Get the maximum length of position embeddings in the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model : torch.nn.Module - The model to inspect\n",
    "\n",
    "    Returns:\n",
    "    - int - Maximum length of position embeddings\n",
    "    \"\"\"\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "\n",
    "    # Checking various attributes to determine max length\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(conf, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max length: {max_length}\")\n",
    "            break\n",
    "\n",
    "    # Defaulting to 1024 if no length attribute is found\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(\n",
    "    batch,\n",
    "    args: Args,\n",
    "    tokenizer,\n",
    "    max_seq_length,\n",
    "    context_field=\"sentence\",\n",
    "    response_field=\"label_decoded\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates formatted prompts and tokenizes in batch mode.\n",
    "\n",
    "    Parameters:\n",
    "    - batch: dict - Batch containing columns as lists.\n",
    "    - args: Args - Arguments needed for formatting.\n",
    "    - tokenizer: AutoTokenizer - Tokenizer for the model.\n",
    "    - max_seq_length: int - Maximum sequence length for tokenization.\n",
    "    - context_field: str - The key for context text in the batch.\n",
    "    - response_field: str - The key for response text in the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    instruction_prompt = args.instruction_prompt\n",
    "    system_prompt = args.system_prompt\n",
    "\n",
    "    # Validating the necessary components\n",
    "    if not instruction_prompt.strip() or not system_prompt.strip():\n",
    "        raise ValueError(\"Instruction and system prompts must be non-empty strings.\")\n",
    "\n",
    "    # Check each element of the context_field and response_field\n",
    "    if not all(item.strip() for item in batch[context_field]) or not all(\n",
    "        item.strip() for item in batch[response_field]\n",
    "    ):\n",
    "        raise ValueError(\"Fields must be non-empty strings.\")\n",
    "\n",
    "    # Creating the formatted prompt for each sample in the batch\n",
    "    batch[\"text\"] = [\n",
    "        args.B_INST\n",
    "        + args.B_SYS\n",
    "        + system_prompt\n",
    "        + args.E_SYS\n",
    "        + instruction_prompt\n",
    "        + context\n",
    "        + args.E_INST\n",
    "        for context in batch[context_field]\n",
    "    ]\n",
    "\n",
    "    # Tokenizing the batch\n",
    "    return tokenizer(batch[\"text\"], max_length=max_seq_length, truncation=True)\n",
    "\n",
    "\n",
    "def preprocess_dataset(\n",
    "    args: Args, tokenizer: AutoTokenizer, max_seq_length: int, dataset: Dataset\n",
    "):\n",
    "    \"\"\"\n",
    "    Format & tokenize the dataset for training.\n",
    "\n",
    "    Parameters:\n",
    "    - args: Args - Arguments needed for formatting.\n",
    "    - tokenizer: AutoTokenizer - Tokenizer for the model.\n",
    "    - max_seq_length: int - Maximum sequence length for tokenization.\n",
    "    - dataset: Dataset - Dataset to preprocess.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply the decode function to the 'label_encoded' column\n",
    "    dataset = dataset.rename_column('label', 'label_encoded')    \n",
    "    dataset = dataset.map(lambda examples: {'label_decoded': decode_label(examples['label_encoded'])})\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        partial(\n",
    "            preprocess_batch,\n",
    "            args=args,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "        ),\n",
    "        batched=True,\n",
    "    )\n",
    "\n",
    "    # Further processing steps if necessary (e.g., filtering, shuffling)\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_seq_length)\n",
    "    # dataset = dataset.filter(lambda sample: \n",
    "    #                     sample[\"input_ids\"] is not None and \n",
    "    #                     max_seq_length is not None and \n",
    "    #                     len(sample[\"input_ids\"]) < max_seq_length)\n",
    "\n",
    "    \n",
    "    dataset = dataset.shuffle(seed=args.seed)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_and_preprocess_dataset(args, logger, tokenizer, max_seq_length, split: str):\n",
    "    \"\"\"\n",
    "    Load and preprocess datasets based on the split specified (train/test).\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading {split} dataset...\")\n",
    "    dataset = load_dataset(f\"{args.organization}/{args.task_name}\")[\n",
    "        split\n",
    "    ]\n",
    "\n",
    "    logger.info(f\"Preprocessing {split} dataset...\")\n",
    "    preprocessed_dataset = preprocess_dataset(\n",
    "        args=args,\n",
    "        dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length,\n",
    "    )\n",
    "\n",
    "    return preprocessed_dataset\n",
    "\n",
    "\n",
    "def split_dataset(train_dataset, train_ratio=0.7, seed=42):\n",
    "    \"\"\"\n",
    "    Split a Hugging Face dataset into training and validation sets with a given ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - train_dataset: Hugging Face dataset to split\n",
    "    - train_ratio: Ratio of data to keep in the training set\n",
    "    - seed: Seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - train_set: Training dataset\n",
    "    - val_set: Validation dataset\n",
    "    \"\"\"\n",
    "    # Ensuring the ratios are valid\n",
    "    if train_ratio <= 0 or train_ratio >= 1:\n",
    "        raise ValueError(\"Train ratio must be between 0 and 1\")\n",
    "\n",
    "    val_ratio = 1 - train_ratio\n",
    "\n",
    "    # Splitting the dataset\n",
    "    datasets = train_dataset.train_test_split(test_size=val_ratio, seed=seed)\n",
    "    train_set = datasets[\"train\"]\n",
    "    val_set = datasets[\"test\"] # TODO: can I name this eval instead?\n",
    "\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8108a0ff-8e38-4b8f-b969-559885e5e597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======= PEFT HELPER FUNCTIONS ===========\n",
    "class PeftSavingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback to save the PEFT adapters during the model training.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        checkpoint_path = os.path.join(\n",
    "            args.output_dir, f\"checkpoint-{state.global_step}\"\n",
    "        )\n",
    "        kwargs[\"model\"].save_pretrained(checkpoint_path)\n",
    "\n",
    "        if \"pytorch_model.bin\" in os.listdir(checkpoint_path):\n",
    "            os.remove(os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
    "\n",
    "\n",
    "def find_all_linear_names(model: Type[torch.nn.Module], bits: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find names of all linear layers in the model based on the number of bits specified.\n",
    "\n",
    "    Parameters:\n",
    "    - model : torch.nn.Module - The model to inspect\n",
    "    - bits : int - The number of bits to select the appropriate linear layer class\n",
    "\n",
    "    Returns:\n",
    "    - List[str] - List of linear layer names\n",
    "    \"\"\"\n",
    "\n",
    "    # Selecting the appropriate class based on the number of bits\n",
    "    if bits == 4:\n",
    "        cls = bnb.nn.Linear4bit\n",
    "    elif bits == 8:\n",
    "        cls = bnb.nn.Linear8bitLt\n",
    "    else:\n",
    "        cls = torch.nn.Linear\n",
    "\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    # Removing 'lm_head' if exists (specific to 16-bit scenarios)\n",
    "    lora_module_names.discard(\"lm_head\")\n",
    "\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "def create_peft_config(args: Args, modules: List[str]) -> LoraConfig:\n",
    "    \"\"\"\n",
    "    Create PEFT configuration for LoRA.\n",
    "\n",
    "    Parameters:\n",
    "    - args : Args - The arguments containing LoRA parameters\n",
    "    - modules : List[str] - List of module names\n",
    "\n",
    "    Returns:\n",
    "    - LoraConfig - Configuration object for PEFT\n",
    "    \"\"\"\n",
    "    return LoraConfig(\n",
    "        target_modules=modules,\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48750a21-2937-4620-880f-e693e35a4984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== TRAINING FUNCTIONS ===============\n",
    "def configure_tokenizer(args):\n",
    "    \"\"\"\n",
    "    Configures the tokenizer based on the provided arguments.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=False)\n",
    "    tokenizer.pad_token = args.EOS\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def configure_model(args, logger):\n",
    "    \"\"\"\n",
    "    Applies further configurations to the model based on the arguments provided.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Creating BitsAndBytesConfig ...\")\n",
    "    bnb_config = configure_bnb(args)\n",
    "\n",
    "    logger.debug(\"Creating ModelforCausalLM ...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        load_in_4bit=args.load_in_4bit,\n",
    "        load_in_8bit=args.load_in_8bit,\n",
    "        device_map=args.device_map,\n",
    "        max_memory=args.cuda_max_memory,\n",
    "        torch_dtype=compute_dtype,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=False,\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    model = log_and_save_info(model, logger, args)\n",
    "    return model\n",
    "\n",
    "\n",
    "def setup_training_arguments(args):\n",
    "    \"\"\"\n",
    "    Configures and returns the TrainingArguments based on the provided arguments.\n",
    "    \"\"\"\n",
    "    # Directory setup for outputs\n",
    "    output_dir = setup_output_directory(\n",
    "        args.output_dir\n",
    "    )\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        fp16=args.fp16,\n",
    "        bf16=args.bf16,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        weight_decay=args.weight_decay,\n",
    "        optim=args.optim,\n",
    "        learning_rate=args.learning_rate,\n",
    "        lr_scheduler_type=args.lr_scheduler_type,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        max_steps=args.max_steps,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        save_safetensors=args.save_safetensors,\n",
    "        load_best_model_at_end=args.load_best_model_at_end,\n",
    "        push_to_hub=False,\n",
    "        evaluation_strategy=args.evaluation_strategy,\n",
    "        logging_dir=logging_dir,\n",
    "        report_to=args.report_to,\n",
    "        save_strategy=args.save_strategy,\n",
    "        save_steps=args.save_steps,\n",
    "        logging_strategy=args.logging_strategy,\n",
    "        logging_steps=args.logging_steps,\n",
    "        group_by_length=args.group_by_length,\n",
    "    )\n",
    "    return training_arguments\n",
    "\n",
    "\n",
    "def setup_trainer(\n",
    "    args, model, tokenizer, peft_config, train_dataset, eval_dataset, training_arguments\n",
    "):\n",
    "    \"\"\"\n",
    "    Configures and returns the trainer based on the provided arguments and datasets.\n",
    "    \"\"\"\n",
    "    callbacks = [PeftSavingCallback()]\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_arguments,\n",
    "        packing=args.packing,\n",
    "        max_seq_length=max_seq_length,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        peft_config=peft_config,\n",
    "        callbacks=callbacks,\n",
    "        dataset_text_field=\"text\",\n",
    "        neftune_noise_alpha=args.neftune_noise_alpha,\n",
    "        tokenizer=tokenizer,\n",
    "        # compute_metrics=compute_metrics, # TODO: remove comment\n",
    "    )\n",
    "\n",
    "    trainer.predict_with_generate = args.predict_with_generate\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def execute_training_and_evaluation(trainer, args, logger):\n",
    "    \"\"\"\n",
    "    Executes the training and evaluation process based on the configured trainer and arguments.\n",
    "    \"\"\"\n",
    "    logger.debug(\n",
    "        \"Evaluating the baseline performance of the model before fine-tuning...\"\n",
    "    )\n",
    "    baseline_results = trainer.evaluate()\n",
    "    logger.info(f\"Baseline evaluation results: {baseline_results}\")\n",
    "\n",
    "    # TODO: improve the try/except blocks\n",
    "    logger.info(\"Running trainer.train() ...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        raise Exception(e)\n",
    "    finally:\n",
    "        memory_cleanup()\n",
    "\n",
    "    if args.report_to == \"wandb\":\n",
    "        wandb.finish()\n",
    "\n",
    "    logger.info(\"trainer.evaluate() ...\")\n",
    "    try:\n",
    "        metrics = trainer.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "    except Exception as e:\n",
    "        logger.info(\"metrics block failed\")\n",
    "        logger.error(e)\n",
    "        raise e\n",
    "\n",
    "    final_results = trainer.evaluate()\n",
    "    trainer.save_state()\n",
    "    logger.info(f\"Final evaluation results: {final_results}\")\n",
    "    results_df = merge_evaluation_results(baseline_results, final_results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def train(args, logger):\n",
    "    logger.info(\"Starting Supervised Fine Tuning...\")\n",
    "\n",
    "    # Tokenizer setup and configuration\n",
    "    logger.debug(\"Creating the Tokenizer...\")\n",
    "    tokenizer = configure_tokenizer(args)\n",
    "\n",
    "    # TODO: remove these comments\n",
    "    # Metrics setup and configuration\n",
    "    # logger.debug(\"Creating Metrics...\")\n",
    "    # compute_metrics_function = metric_computer(tokenizer)\n",
    "    # logger.info(compute_metrics_function)\n",
    "\n",
    "    model = configure_model(args, logger)\n",
    "    max_seq_length = get_max_length(\n",
    "        model\n",
    "    )\n",
    "\n",
    "    # Loading and preprocessing datasets\n",
    "    logger.debug(\"Loading and preprocessing train dataset...\")\n",
    "    train_dataset = load_and_preprocess_dataset(\n",
    "        args, logger, tokenizer, max_seq_length, \"train\"\n",
    "    )\n",
    "    train_set, val_set = split_dataset(train_dataset, train_ratio=0.7, seed=args.seed)\n",
    "\n",
    "    # TrainingArguments setup\n",
    "    logger.info(\"Creating TrainingArguments ...\")\n",
    "    training_arguments = setup_training_arguments(args)\n",
    "\n",
    "    logger.info(\"Creating PEFT config ...\")\n",
    "    layers_for_adapters = find_all_linear_names(model, 4)\n",
    "    logger.debug(f\"Layers for Adapters: {layers_for_adapters}\")\n",
    "    peft_config = create_peft_config(args, layers_for_adapters)\n",
    "\n",
    "    # Trainer setup\n",
    "    logger.info(\"Creating SFTTrainer ...\")\n",
    "    trainer = setup_trainer(args, model, tokenizer, peft_config, train_set, val_set, training_arguments)\n",
    "\n",
    "    # Training and Evaluation\n",
    "    results_df = execute_training_and_evaluation(trainer, args, logger)\n",
    "    display(results_df)\n",
    "\n",
    "    # Saving final model and tokenizer states\n",
    "    model = trainer.model\n",
    "    save_model_and_tokenizer(model, tokenizer, output_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f96427a-f68a-4d58-93b9-838049c6f46e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ============== METRICS FUNCTIONS =================\n",
    "def decode_predictions(predictions, labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Decode predictions and labels from token IDs to strings.\n",
    "    \"\"\"\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    return decoded_preds, decoded_labels\n",
    "    \n",
    "def compute_metrics(eval_pred, tokenizer, metric):\n",
    "    \"\"\"\n",
    "    Compute custom metrics like ROUGE for the given predictions and labels.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds, decoded_labels = decode_predictions(predictions, labels, tokenizer)\n",
    "\n",
    "    # TODO: REVIEW THIS CODE ... SHOULD I EVEN USE NLTK??\n",
    "    decoded_preds = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds\n",
    "    ]\n",
    "    decoded_labels = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
    "    ]\n",
    "\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "        use_aggregator=True,\n",
    "    )\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "def metric_computer(tokenizer):\n",
    "    \"\"\"\n",
    "    Load and compute custom metrics like BLEU and ROUGE.\n",
    "    \"\"\"\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "    def compute(p):\n",
    "        predictions, references = p\n",
    "        pred_ids = np.argmax(p.predictions, axis=2)\n",
    "\n",
    "        pred_texts = [\n",
    "            tokenizer.decode(ids, skip_special_tokens=True) for ids in pred_ids\n",
    "        ]\n",
    "        label_texts = [\n",
    "            tokenizer.decode(ids, skip_special_tokens=True) for ids in p.label_ids\n",
    "        ]\n",
    "\n",
    "        bleu_score = bleu_metric.compute(predictions=pred_texts, references=label_texts)\n",
    "        rouge_score = rouge_metric.compute(\n",
    "            predictions=pred_texts, references=label_texts\n",
    "        )\n",
    "\n",
    "        return {\"bleu\": bleu_score, \"rouge\": rouge_score}\n",
    "\n",
    "    return compute"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6860cda2-fb76-494a-a8ca-f066a1657ee1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ======== EVALUATION FUNCTIONS ===============\n",
    "def generate(model=None, tokenizer=None, dataset=None):\n",
    "    temperature = 0.0  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "    do_sample = False\n",
    "    max_new_tokens = 256\n",
    "    top_k = 10\n",
    "    top_p = 0.92\n",
    "    repetition_penalty = 1.0  # 1.0 means no penalty\n",
    "    num_return_sequences = 1  # Only generate one response\n",
    "    num_beams = 1\n",
    "\n",
    "    input_ids = tokenizer(dataset[\"text\"])\n",
    "\n",
    "    # Ensure that input_ids is a PyTorch tensor\n",
    "    # input_ids = torch.tensor(input_ids).long()\n",
    "\n",
    "    # Move the tensor to the GPU\n",
    "    input_ids = input_ids.cuda()\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_beams,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "        ),\n",
    "    )\n",
    "    seq = generation_output.sequences\n",
    "    output = tokenizer.decode(seq[0])\n",
    "    return output.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "\n",
    "# TODO: INCORPORATE EXTRACT LABEL\n",
    "def extract_label(text_output, E_INST=\"[/INST]\"):\n",
    "    # Find the 'end of instruction' token and remove text before it\n",
    "    response_pos = text_output.find(E_INST)\n",
    "    generated_text = text_output[response_pos + len(E_INST) :].strip()\n",
    "    # Convert the string to lowercase for case-insensitive search\n",
    "    text = text_output.lower()\n",
    "\n",
    "    # Define the substring options\n",
    "    substrings = [\"label: positive\", \"label: negative\", \"label: neutral\"]\n",
    "\n",
    "    # Iterate over the substrings and find the matching label\n",
    "    for i, substring in enumerate(substrings):\n",
    "        if substring in text:\n",
    "            return i\n",
    "\n",
    "    # If none of the substrings are found, return -1\n",
    "    return -1\n",
    "\n",
    "\n",
    "# TODO: INCORPORATE COMPUTE METRICS\n",
    "def compute_metrics(files, outputs_directory):\n",
    "    acc_list = []\n",
    "    f1_list = []\n",
    "    missing_perc_list = []\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv(outputs_directory / file)\n",
    "\n",
    "        # Make sure the 'Label:' was provided in all generated text\n",
    "        if all(df[\"text_output\"].str.contains(\"Label:\")):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"not all responses contain the substring 'Label:'\")\n",
    "\n",
    "        # Decode the predicted label\n",
    "        df[\"generated_label\"] = df[\"text_output\"].apply(extract_label)\n",
    "\n",
    "        # Calculate metrics\n",
    "        acc_list.append(accuracy_score(df[\"true_label\"], df[\"generated_label\"]))\n",
    "        f1_list.append(\n",
    "            f1_score(df[\"true_label\"], df[\"generated_label\"], average=\"weighted\")\n",
    "        )\n",
    "        missing_perc_list.append(\n",
    "            (len(df[df[\"generated_label\"] == -1]) / df.shape[0]) * 100.0\n",
    "        )\n",
    "\n",
    "    return acc_list, f1_list, missing_perc_list\n",
    "\n",
    "\n",
    "def evaluate_results():\n",
    "    # TODO: RESULTS CODE BELOW NEEDS TO BE INCORPORATED\n",
    "    # results = {}\n",
    "    # for model_name in model_names:\n",
    "    #     results[model_name] = {}\n",
    "    #     for quantization in quantizations:\n",
    "    #         # Define output directory\n",
    "    #         LLM_OUTPUTS_DIRECTORY = (\n",
    "    #             ROOT_DIRECTORY\n",
    "    #             / \"data\"\n",
    "    #             / task_name\n",
    "    #             / \"llm_prompt_outputs\"\n",
    "    #             / quantization\n",
    "    #         )\n",
    "    #         # Filter out relevant files\n",
    "    #         files = [\n",
    "    #             f.stem\n",
    "    #             for f in LLM_OUTPUTS_DIRECTORY.iterdir()\n",
    "    #             if model_name in f.name and f.suffix == \".csv\"\n",
    "    #         ]\n",
    "    #         results[model_name][quantization] = files\n",
    "    # acc_list, f1_list, missing_perc_list = compute_metrics(files, LLM_OUTPUTS_DIRECTORY)\n",
    "    #\n",
    "    # # Print results\n",
    "    # print(\"f1 score mean: \", format(np.mean(f1_list), \".4f\"))\n",
    "    # print(\"f1 score std: \", format(np.std(f1_list), \".4f\"))\n",
    "    # print(\n",
    "    #     \"Percentage of cases when didn't follow instruction: \",\n",
    "    #     format(np.mean(missing_perc_list), \".4f\"),\n",
    "    #     \"\\n\",\n",
    "    # )\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159739de-7ee4-43d4-9ea0-bcd52c89d8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====== UTILS =======\n",
    "def memory_cleanup():\n",
    "    # Empty VRAM\n",
    "    if \"trainer\" in locals() or \"trainer\" in globals():\n",
    "        del trainer\n",
    "    if \"model\" in locals() or \"model\" in globals():\n",
    "        del model\n",
    "    if \"pipe\" in locals() or \"pipe\" in globals():\n",
    "        del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "\n",
    "def configure_cuda_args(args, logger):\n",
    "    if args.cuda_n_gpus is None:\n",
    "        args.cuda_n_gpus = torch.cuda.device_count()\n",
    "        logger.debug(f\"args.cuda_n_gpus now defined: {args.cuda_n_gpus}\")\n",
    "    else:\n",
    "        logger.debug(\"args.cuda_n_gpus already defined.\")\n",
    "        \n",
    "    if args.cuda_max_memory is None:\n",
    "        CUDA_MAX_MEMORY = f\"{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB\"\n",
    "        args.cuda_max_memory = {i: CUDA_MAX_MEMORY for i in range(args.cuda_n_gpus)}\n",
    "        logger.debug(f\"args.cuda_max_memory now defined: {args.cuda_max_memory}\")\n",
    "    else:\n",
    "        logger.debug(\"args.cuda_max_memory already defined.\")\n",
    "        \n",
    "    return args\n",
    "\n",
    "def load_models(args, logger):\n",
    "    # Load the foundation model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        device_map=args.device_map,\n",
    "        max_memory=args.cuda_max_memory,\n",
    "        torch_dtype=args.bnb_compute_dtype,\n",
    "    )\n",
    "    log_dtypes(base_model, logger)\n",
    "\n",
    "    # Load the fine-tuned model\n",
    "    logger.debug(\"Creating BitsAndBytesConfig ...\")\n",
    "    bnb_config = configure_bnb(args)\n",
    "    new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        args.output_dir / \"final_checkpoint\",\n",
    "        device_map=args.device_map,\n",
    "        max_memory=args.cuda_max_memory,\n",
    "        torch_dtype=args.bnb_compute_dtype,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "\n",
    "    log_dtypes(new_model, logger)\n",
    "\n",
    "    return base_model, new_model\n",
    "\n",
    "\n",
    "def merge_models(base_model, new_model, logger):\n",
    "    # Merge the LoRa layers into the base model for standalone use\n",
    "    peft_model = PeftModel.from_pretrained(base_model, new_model)\n",
    "    peft_model.merge_and_unload()\n",
    "    log_dtypes(peft_model, logger)\n",
    "\n",
    "    return peft_model\n",
    "\n",
    "\n",
    "def save_and_push(args, peft_model):\n",
    "    # Save inference\n",
    "    merged_checkpoint_dir = args.output_dir / \"final_merged_checkpoint\"\n",
    "    peft_model.save_pretrained(merged_checkpoint_dir, safe_serialization=True)\n",
    "\n",
    "    # Load and save tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.output_dir / \"final_checkpoint\", pad_token=EOS\n",
    "    )\n",
    "    tokenizer.save_pretrained(merged_checkpoint_dir)\n",
    "\n",
    "    # Push model and tokenizer to hub\n",
    "    peft_model.push_to_hub(args.repo_name, private=True, use_temp_dir=True)\n",
    "    tokenizer.push_to_hub(args.repo_name, private=True, use_temp_dir=True)\n",
    "\n",
    "def save_model_and_tokenizer(model, tokenizer, model_dir):\n",
    "    \"\"\"\n",
    "    Save the model and tokenizer in the trainer to the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - trainer\n",
    "        The trainer object containing the model.\n",
    "    - tokenizer : PreTrainedTokenizer\n",
    "        The tokenizer to be saved.\n",
    "    - model_dir : str\n",
    "        The directory where the model and tokenizer will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Save model\n",
    "        model.save_pretrained(model_dir)\n",
    "        print(f\"Model saved to {model_dir}\")\n",
    "\n",
    "        # Save tokenizer\n",
    "        tokenizer.save_pretrained(model_dir)\n",
    "        print(f\"Tokenizer saved to {model_dir}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the model and tokenizer: {e}\")\n",
    "\n",
    "def setup_output_directory(output_dir_path):\n",
    "    \"\"\"\n",
    "    Sets up the output directory for saving model checkpoints and other outputs.\n",
    "    \"\"\"\n",
    "    output_dir = output_dir_path / \"final_checkpoint\"\n",
    "    output_dir.mkdir(mode=0o777, parents=True, exist_ok=True)\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dc32935-1e70-4d25-ad2e-4ac0dd7eb2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= main ===========\n",
    "def main():\n",
    "    args = setup_args()\n",
    "    logger = setup_logging()\n",
    "    args = configure_cuda_args(args, logger)\n",
    "    logger.info(f\"Using k={args.cuda_n_gpus} CUDA GPUs with max memory {args.cuda_max_memory}\")\n",
    "\n",
    "    # if notebook: get_ipython().run_line_magic('tensorboard', '--logdir logs')\n",
    "\n",
    "    try:\n",
    "        train(args, logger)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        raise Exception(e)\n",
    "    finally:\n",
    "        memory_cleanup()\n",
    "\n",
    "    base_model, new_model = load_models(args, logger)\n",
    "    peft_model = merge_models(base_model, new_model, logger)\n",
    "    save_and_push(args, peft_model)\n",
    "    max_seq_length = get_max_length(peft_model)\n",
    "\n",
    "    logger.info(\"Loading and preprocessing test dataset...\")\n",
    "    logger.debug(\"Creating Tokenizer...\")\n",
    "    tokenizer = configure_tokenizer(args)\n",
    "    logger.debug(\"Creating Test Dataset...\")\n",
    "    test_set = load_and_preprocess_dataset(\n",
    "        args, logger, tokenizer, max_seq_length, \"test\"\n",
    "    )\n",
    "\n",
    "#     # TODO: holdout evaluation\n",
    "#     output_list = []\n",
    "#     for i in range(len(test_set)):\n",
    "#         output_list.append(\n",
    "#             generate(model=peft_model, tokenizer=tokenizer, dataset=test_set)\n",
    "#         )\n",
    "#     output_list.replace(\"</s>\", \"\")\n",
    "#     return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b89e6650-0c01-485b-969f-52c6ef085c16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=50'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cde0c8-7b10-4505-83fc-4c4155f8b394",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a205994c-de1f-4c89-b81b-e2e1710361a3",
   "metadata": {},
   "source": [
    "# SANDBOX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54a854-dcbc-4f6c-adcd-cef8f4f6b91a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1231ae-b861-4a1c-bc1b-64110b3003b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad083d8d-e666-4ac4-a458-ca8c7048730e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eeafa6-5f0a-4786-8e7a-ef60dc562cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - DEBUG - args.cuda_n_gpus now defined: 2\n",
      "llama2_finetune - DEBUG - args.cuda_max_memory now defined: {0: '41GB', 1: '41GB'}\n",
      "llama2_finetune - INFO - Using k=2 CUDA GPUs with max memory {0: '41GB', 1: '41GB'}\n",
      "llama2_finetune - INFO - Starting Supervised Fine Tuning...\n",
      "llama2_finetune - DEBUG - Creating the Tokenizer...\n",
      "loading file tokenizer.model from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/tokenizer.model\n",
      "loading file tokenizer.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/tokenizer_config.json\n",
      "llama2_finetune - DEBUG - Creating BitsAndBytesConfig ...\n",
      "llama2_finetune - DEBUG - Creating ModelforCausalLM ...\n",
      "loading configuration file config.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Detected PIL version 10.0.1\n",
      "loading weights file model.safetensors from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Detected 4-bit loading: activating 4-bit loading for this model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ceafd6079c448a19aa9d4820726c000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "llama2_finetune - DEBUG - Getting the model's memory footprint...\n",
      "llama2_finetune - DEBUG - Model Dtypes before preparing for kbit training ...\n",
      "llama2_finetune - INFO - torch.bfloat16: 262410240 (7.50%)\n",
      "llama2_finetune - INFO - torch.uint8: 3238002688 (92.50%)\n",
      "llama2_finetune - DEBUG - Using the prepare_model_for_kbit_training method from PEFT...\n",
      "llama2_finetune - DEBUG - Model Dtypes after preparing for kbit training ...\n",
      "llama2_finetune - INFO - torch.float32: 262410240 (7.50%)\n",
      "llama2_finetune - INFO - torch.uint8: 3238002688 (92.50%)\n",
      "llama2_finetune - DEBUG - Get module names for the linear layers where we add LORA adapters...\n",
      "llama2_finetune - DEBUG - Layers for Adapters: ['v_proj', 'down_proj', 'o_proj', 'up_proj', 'q_proj', 'k_proj', 'gate_proj']\n",
      "llama2_finetune - INFO - Create PEFT config for these modules and wrap the model to PEFT...\n",
      "llama2_finetune - INFO - Model Dtypes before applying PEFT config ...\n",
      "llama2_finetune - INFO - torch.float32: 262410240 (7.50%)\n",
      "llama2_finetune - INFO - torch.uint8: 3238002688 (92.50%)\n",
      "llama2_finetune - INFO - Model Dtypes after applying PEFT config ...\n",
      "llama2_finetune - INFO - torch.float32: 422318080 (11.54%)\n",
      "llama2_finetune - INFO - torch.uint8: 3238002688 (88.46%)\n",
      "llama2_finetune - INFO - Information about the percentage of trainable parameters...\n",
      "llama2_finetune - INFO - Trainable params: 159907840 || All params: 3660320768 || Trainable%: 4.368683788535114\n",
      "llama2_finetune - DEBUG - Loading and preprocessing train dataset...\n",
      "llama2_finetune - INFO - Loading train dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max length: 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - Preprocessing train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddd66ec793744319f92ff6fe2e53e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - Creating TrainingArguments ...\n",
      "using `logging_steps` to initialize `eval_steps` to 100\n",
      "PyTorch: setting up devices\n",
      "llama2_finetune - INFO - Creating PEFT config ...\n",
      "llama2_finetune - DEBUG - Layers for Adapters: ['v_proj', 'down_proj', 'o_proj', 'up_proj', 'q_proj', 'k_proj', 'gate_proj']\n",
      "llama2_finetune - INFO - Creating SFTTrainer ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd177cbf32143588900938034736b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959d14d9eace4c9bb6d62825f69d7c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "llama2_finetune - DEBUG - Evaluating the baseline performance of the model before fine-tuning...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 8\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 04:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - Baseline evaluation results: {'eval_loss': 2.9575114250183105, 'eval_runtime': 63.7511, 'eval_samples_per_second': 9.349, 'eval_steps_per_second': 1.176}\n",
      "llama2_finetune - INFO - Running trainer.train() ...\n",
      "Currently training with a batch size of: 8\n",
      "***** Running training *****\n",
      "  Num examples = 1,388\n",
      "  Num Epochs = 12\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2,088\n",
      "  Number of trainable parameters = 159,907,840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='558' max='2088' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 558/2088 20:46 < 57:11, 0.45 it/s, Epoch 3.20/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.315300</td>\n",
       "      <td>7.335701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.379700</td>\n",
       "      <td>5.010995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.266900</td>\n",
       "      <td>5.377721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.185600</td>\n",
       "      <td>5.130898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.100700</td>\n",
       "      <td>5.406809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-100\n",
      "tokenizer config file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-100/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-200\n",
      "tokenizer config file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-200/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-300\n",
      "tokenizer config file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-300/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-400\n",
      "tokenizer config file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-400/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 596\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-500\n",
      "tokenizer config file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/final_checkpoint/checkpoint-500/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22914840-482d-4fd9-b21a-96c0452b8bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
