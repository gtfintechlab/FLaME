{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75897156-1b29-4b0d-b3fe-86a54336691c",
   "metadata": {},
   "source": [
    "***TODO?***: see if `offload_folder=\"tmp\"` can help when loading models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "946c77d4-a5a6-4d2c-a3dc-f0671d6e73a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/AD/gmatlin3/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find llama2_sft.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================== HUGGINGFACE ======================\n",
    "import os\n",
    "import huggingface_hub\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/home/AD/gmatlin3/.cache/huggingface/\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/home/AD/gmatlin3/.cache/huggingface/hub/\"\n",
    "HF_AUTH = \"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\"\n",
    "huggingface_hub.login(HF_AUTH)\n",
    "# ====================== WEIGHTS AND BIASES ======================\n",
    "import wandb\n",
    "\n",
    "WANDB_PROJECT = f\"llama2_sft_fomc\"\n",
    "os.environ[\n",
    "    \"WANDB_PROJECT\"\n",
    "] = WANDB_PROJECT  # Set the wandb project where this run will be logged\n",
    "os.environ[\n",
    "    \"WANDB_LOG_MODEL\"\n",
    "] = \"false\"  # Turn off save your trained model checkpoint to wandb (our models are too large)\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\"  # Turn off watch to log faster\n",
    "os.environ[\"WANDB_API_KEY\"] = \"fa69ffc6a97578da0410b553042cbb8b3bf5fcaf\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = f\"llama2_sft\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a81af90-910b-4b1d-ae45-23f627bc0594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================== IMPORTS ======================\n",
    "# Standard Libraries\n",
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "import fire\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import NamedTuple, List, Type\n",
    "from IPython.display import display\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch and HuggingFace Libraries\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import logging as hf_logging\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers import set_seed as transformers_set_seed\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    AutoPeftModelForCausalLM,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f8e4ff-187e-48e1-966c-559a4ac47a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_uid(id_length=8, dt_format=\"%y%m%d\"):\n",
    "    date_str = datetime.now().strftime(dt_format)\n",
    "\n",
    "    # Generate a short UUID\n",
    "    uid = str(uuid.uuid4())[:id_length]\n",
    "\n",
    "    # Combine\n",
    "    uid = f\"{uid}_{date_str}\"\n",
    "\n",
    "    return uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b898c011-1db3-456d-a6b0-e0c8e12e2200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================== HUGGINGFACE ======================\n",
    "organization = \"gtfintechlab\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# ====================== TASK PARAMETERS ======================\n",
    "task_name = \"fomc_communication\"\n",
    "seeds = (5768, 78516, 944601)\n",
    "seed = 5768\n",
    "\n",
    "# ====================== MODEL PARAMETERS ======================\n",
    "model_parameters = \"7b\"\n",
    "model_id = f\"meta-llama/Llama-2-{model_parameters}-chat-hf\"\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "\n",
    "# ====================== LOGGING PARAMETERS ======================\n",
    "report_to = \"tensorboard\"\n",
    "logging_dir = Path.home() / \"tensorboard\" / \"logs\"\n",
    "\n",
    "# ====================== DIRECTORY PARAMETERS ======================\n",
    "output_dir = Path(\"/fintech_3\") / \"glenn\" / \"results\" / f\"{task_name}\" / f\"{model_name}\" / f\"{generate_uid()}\"\n",
    "checkpoint_dir = output_dir / \"final_checkpoint\"\n",
    "mergepoint_dir = output_dir / \"final_merged_checkpoint\"\n",
    "\n",
    "# ====================== PROMPT PARAMETERS ======================\n",
    "# system_prompt = f\"Discard all previous instructions. Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "system_prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "# instruction_prompt = f\"Discard all the previous instructions. Behave like you are an expert sentence classifier. Classify the following sentence from FOMC into 'HAWKISH', 'DOVISH', or 'NEUTRAL' class. Label 'HAWKISH' if it is corresponding to tightening of the monetary policy, 'DOVISH' if it is corresponding to easing of the monetary policy, or 'NEUTRAL' if the stance is neutral. Provide the label 'HAWKISH', 'DOVISH', or 'NEUTRAL'. The sentence: \",\n",
    "# instruction_prompt = f\"Behave like you are an expert sentence classifier. Classify the following sentence from the Federal Open Market Committee into 'HAWKISH', 'DOVISH', or 'NEUTRAL' class. Label 'HAWKISH' if it is corresponding to tightening of the monetary policy. Label 'DOVISH' if it is corresponding to easing of the monetary policy. Label 'NEUTRAL' if the stance is neutral. Provide a single label from the choices 'HAWKISH', 'DOVISH', or 'NEUTRAL' then stop generating text. The sentence: \"\n",
    "instruction_prompt = \"Discard all the previous instructions. Behave like you are an expert sentence classifier. Classify the following sentence from FOMC into 'HAWKISH', 'DOVISH', or 'NEUTRAL' class. Label 'HAWKISH' if it is corresponding to tightening of the monetary policy, 'DOVISH' if it is corresponding to easing of the monetary policy, or 'NEUTRAL' if the stance is neutral. Provide the label in the first line and provide a short explanation in the second line. The sentence: \"\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "BOS, EOS = \"<s>\", \"</s>\"\n",
    "\n",
    "repo_name = f\"{organization}/{model_name}_{task_name}\"\n",
    "\n",
    "# ====================== QLORA PARAMETERS ======================\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# ====================== SFT PARAMETERS ======================\n",
    "# Default maximum sequence length to use\n",
    "max_seq_length = 4096\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "neftune_noise_alpha = 5\n",
    "\n",
    "# ====================== CUDA PARAMETERS ======================\n",
    "# Default compute type\n",
    "compute_dtype = torch.bfloat16\n",
    "# Enable fp16/bf16 training\n",
    "fp16, bf16 = False, True\n",
    "\n",
    "cuda_n_gpus, cuda_max_memory = None, None  # Determined dynamically at runtime\n",
    "\n",
    "# Automatically determine the device map\n",
    "device_map = \"auto\"\n",
    "device = \"cuda:0\" # if single device is needed...\n",
    "\n",
    "save_safetensors = True\n",
    "\n",
    "# ====================== BITSANDBYTES PARAMETERS ======================\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate 8-bit precision base model loading\n",
    "load_in_8bit = False\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_compute_dtype = compute_dtype\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_use_double_quant = False\n",
    "\n",
    "# ====================== TRAININGARGUMENTS PARAMETERS ======================\n",
    "# Number of training epochs\n",
    "num_train_epochs = 12\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 8\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 8\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = False\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 3e-3\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"adamw_bnb_8bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0.1\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 0.1\n",
    "\n",
    "load_best_model_at_end = True\n",
    "\n",
    "strategy = \"epoch\"\n",
    "save_strategy = strategy\n",
    "logging_strategy = strategy\n",
    "evaluation_strategy = strategy\n",
    "\n",
    "disable_tqdm = False\n",
    "predict_with_generate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0308c828-13a0-4c7a-ace3-444aeba11bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================== LOGGING SETUP ======================\n",
    "def create_logger(name=\"llama2_finetune\", level=logging.DEBUG):\n",
    "    logger = logging.getLogger(name)\n",
    "    if not logger.hasHandlers():\n",
    "        logger.setLevel(level)\n",
    "        hf_logging.set_verbosity(level)\n",
    "\n",
    "        # Create handlers\n",
    "        c_handler = logging.StreamHandler()\n",
    "        f_handler = logging.FileHandler(\"llama2_finetune.log\")\n",
    "        c_handler.setLevel(level)\n",
    "        f_handler.setLevel(level)\n",
    "\n",
    "        # Create formatters and add it to handlers\n",
    "        format = \"%(name)s - %(levelname)s - %(message)s\"\n",
    "        c_handler.setFormatter(logging.Formatter(format))\n",
    "        f_handler.setFormatter(logging.Formatter(format))\n",
    "\n",
    "        # Add handlers to the logger\n",
    "        logger.addHandler(c_handler)\n",
    "        logger.addHandler(f_handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5492697-2396-413a-9c11-e8a811485c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============== LABEL FUNCTIONS =================\n",
    "# TODO: double-check the fomc mapping\n",
    "FOMC_COMMUNICATION_MAPPING = {0: \"DOVISH\", 1: \"HAWKISH\", 2: \"NEUTRAL\"}\n",
    "\n",
    "\n",
    "# Function to decode the labels\n",
    "def decode_label(label_number):\n",
    "    return FOMC_COMMUNICATION_MAPPING.get(label_number, \"undefined\").upper()\n",
    "\n",
    "\n",
    "# Function to encode the labels\n",
    "def encode_label(label_name):\n",
    "    reversed_mapping = {v: k for k, v in FOMC_COMMUNICATION_MAPPING.items()}\n",
    "    return reversed_mapping.get(label_name.lower(), -1)\n",
    "\n",
    "\n",
    "# TODO: have extract_lavel use our encoding/mapping\n",
    "def extract_label(text_output, label_list=[\"DOVISH\", \"HAWKISH\", \"NEUTRAL\"], E_INST=\"[/INST]\"):\n",
    "    \"\"\"\n",
    "    Extracts the label from the text output from a large language model\n",
    "    \"\"\"\n",
    "    # Find the 'end of instruction' token and remove text before it\n",
    "    response_pos = text_output.find(E_INST)\n",
    "    # Convert the string to uppercase for case-insensitive search\n",
    "    generated_text = text_output[response_pos + len(E_INST) :].strip().upper()\n",
    "    # Define the substring options\n",
    "    label_list = [\"DOVISH\", \"HAWKISH\", \"NEUTRAL\"]\n",
    "    # Iterate over the substrings and find the matching label\n",
    "    for i, label in enumerate(label_list):\n",
    "        if label in generated_text:\n",
    "            return i\n",
    "    # If none of the substrings are found, return -1\n",
    "    return -1\n",
    "\n",
    "# ============== METRICS FUNCTIONS =================\n",
    "def compute_metrics(eval_pred, tokenizer, label_list=[\"DOVISH\", \"HAWKISH\", \"NEUTRAL\"]):\n",
    "    predictions, true_labels = eval_pred\n",
    "    # Decode the predictions to text\n",
    "    # decoded_preds = [\n",
    "    #     tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions\n",
    "    # ]\n",
    "    decoded_preds = tokenizer.batch_decode(sequences=pred, skip_special_tokens=True)\n",
    "    \n",
    "    predicted_labels = [\n",
    "        extract_label(decoded_preds[i]) for i in range(len(decoded_preds))\n",
    "    ]\n",
    "    accuracy_perc, f1_score_perc, missing_perc = evaluate_predictions(\n",
    "        true_labels, predicted_labels\n",
    "    )\n",
    "    return {\n",
    "        \"accuracy\": accuracy_perc,\n",
    "        \"f1_score\": f1_score_perc,\n",
    "        \"missing\": missing_perc\n",
    "    }\n",
    "\n",
    "def evaluate_predictions(true_labels, predicted_labels):\n",
    "    accuracy_perc = accuracy_score(true_labels, predicted_labels)\n",
    "    f1_score_perc = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
    "    missing_perc = (predicted_labels.count(-1) / len(predicted_labels)) * 100.0\n",
    "    return accuracy_perc, f1_score_perc, missing_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd53ccee-5ffc-4b0b-9a10-10968b769f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================== ARGUMENTS SETUP ======================\n",
    "# TODO: MOVE OUR DEFAULT VALUES INTO OUR DATA CLASS(ES)\n",
    "@dataclass\n",
    "class Args:\n",
    "    repo_name: str\n",
    "    task_name: str\n",
    "    system_prompt: str\n",
    "    instruction_prompt: str\n",
    "    seed: int\n",
    "    model_id: str\n",
    "    model_name: str\n",
    "    organization: str\n",
    "    lora_r: float\n",
    "    lora_alpha: float\n",
    "    lora_dropout: float\n",
    "    max_seq_length: int\n",
    "    packing: bool\n",
    "    device_map: str\n",
    "    load_in_4bit: bool\n",
    "    load_in_8bit: bool\n",
    "    bnb_compute_dtype: bool\n",
    "    bnb_use_double_quant: bool\n",
    "    bnb_quant_type: str\n",
    "    output_dir: str\n",
    "    checkpoint_dir: str\n",
    "    mergepoint_dir: str\n",
    "    logging_dir: str\n",
    "    num_train_epochs: int\n",
    "    fp16: bool\n",
    "    bf16: bool\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    gradient_checkpointing: bool\n",
    "    max_grad_norm: float\n",
    "    learning_rate: float\n",
    "    weight_decay: float\n",
    "    optim: str\n",
    "    lr_scheduler_type: str\n",
    "    max_steps: int\n",
    "    warmup_ratio: float\n",
    "    group_by_length: bool\n",
    "    save_steps: int\n",
    "    save_strategy: str\n",
    "    logging_strategy: str\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    neftune_noise_alpha: float\n",
    "    save_safetensors: bool\n",
    "    load_best_model_at_end: bool\n",
    "    disable_tqdm: bool\n",
    "    B_INST: str\n",
    "    E_INST: str\n",
    "    B_SYS: str\n",
    "    E_SYS: str\n",
    "    BOS: str\n",
    "    EOS: str\n",
    "    report_to: str\n",
    "    predict_with_generate: bool\n",
    "    cuda_n_gpus: int\n",
    "    cuda_max_memory: str\n",
    "\n",
    "\n",
    "def setup_args() -> Args:\n",
    "    args = Args(\n",
    "        repo_name=repo_name,\n",
    "        task_name=task_name,\n",
    "        system_prompt=system_prompt,\n",
    "        instruction_prompt=instruction_prompt,\n",
    "        seed=seed,\n",
    "        model_id=model_id,\n",
    "        model_name=model_id.split(\"/\")[-1],\n",
    "        organization=organization,\n",
    "        lora_r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=packing,\n",
    "        device_map=device_map,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "        bnb_compute_dtype=bnb_compute_dtype,\n",
    "        bnb_use_double_quant=bnb_use_double_quant,\n",
    "        bnb_quant_type=bnb_quant_type,\n",
    "        output_dir=output_dir,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        mergepoint_dir=mergepoint_dir,\n",
    "        logging_dir=logging_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        fp16=fp16,\n",
    "        bf16=bf16,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        gradient_checkpointing=gradient_checkpointing,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        optim=optim,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        max_steps=max_steps,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        group_by_length=group_by_length,\n",
    "        save_steps=save_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        logging_strategy=logging_strategy,\n",
    "        logging_steps=logging_steps,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        neftune_noise_alpha=neftune_noise_alpha,\n",
    "        save_safetensors=save_safetensors,\n",
    "        load_best_model_at_end=load_best_model_at_end,\n",
    "        disable_tqdm=disable_tqdm,\n",
    "        B_INST=B_INST,\n",
    "        E_INST=E_INST,\n",
    "        B_SYS=B_SYS,\n",
    "        E_SYS=E_SYS,\n",
    "        BOS=BOS,\n",
    "        EOS=EOS,\n",
    "        report_to=report_to,\n",
    "        predict_with_generate=predict_with_generate,\n",
    "        cuda_n_gpus=cuda_n_gpus,\n",
    "        cuda_max_memory=cuda_max_memory,\n",
    "    )\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8670f60a-5a69-4812-b1eb-474ac2070d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============== SFT LOGGING FUNCTIONS ==================\n",
    "def log_trainable_parameters(model, logger: logging.Logger):\n",
    "    \"\"\"\n",
    "    Logs the number of trainable parameters in the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model : torch.nn.Module - The model to log.\n",
    "    - logger : logging.Logger - Logger to use for logging the info.\n",
    "    \"\"\"\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    logger.info(\n",
    "        f\"Trainable params: {trainable_params} || \"\n",
    "        f\"All params: {total_params} || \"\n",
    "        f\"Trainable%: {100 * trainable_params / total_params}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def log_dtypes(model, logger: logging.Logger):\n",
    "    \"\"\"\n",
    "    Logs the data types of the model parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - model : torch.nn.Module - The model to log.\n",
    "    - logger : logging.Logger - Logger to use for logging the info.\n",
    "    \"\"\"\n",
    "    dtypes = {}\n",
    "\n",
    "    for p in model.parameters():\n",
    "        dtype = p.dtype\n",
    "        dtypes[dtype] = dtypes.get(dtype, 0) + p.numel()\n",
    "\n",
    "    total = sum(dtypes.values())\n",
    "\n",
    "    for dtype, count in dtypes.items():\n",
    "        logger.info(f\"{dtype}: {count} ({100 * count / total:.2f}%)\")\n",
    "\n",
    "\n",
    "def merge_evaluation_results(\n",
    "    baseline_results: dict, final_results: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge evaluation results for comparison.\n",
    "    \"\"\"\n",
    "    all_metrics = set(baseline_results.keys()).union(final_results.keys())\n",
    "    data = {\"Metric\": [], \"Baseline\": [], \"After Fine-tuning\": []}\n",
    "\n",
    "    for metric in all_metrics:\n",
    "        data[\"Metric\"].append(metric)\n",
    "        data[\"Baseline\"].append(baseline_results.get(metric, \"N/A\"))\n",
    "        data[\"After Fine-tuning\"].append(final_results.get(metric, \"N/A\"))\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e95ac99a-2d79-4e54-9071-8d785dd27a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== DATA SET PROCESSING FUNCTIONS ==========\n",
    "def load_dataset_split(args, logger, split: str):\n",
    "    \"\"\"\n",
    "    Load a dataset split\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Loading {split} dataset...\")\n",
    "    dataset_split = load_dataset(f\"{args.organization}/{args.task_name}\")[split]\n",
    "\n",
    "    return dataset_split\n",
    "\n",
    "\n",
    "def split_dataset(train_dataset, train_ratio=0.7, seed=42):\n",
    "    \"\"\"\n",
    "    Split a Hugging Face dataset into training and validation sets with a given ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - train_dataset: Hugging Face dataset to split\n",
    "    - train_ratio: Ratio of data to keep in the training set\n",
    "    - seed: Seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - train_set: Training dataset\n",
    "    - val_set: Validation dataset\n",
    "    \"\"\"\n",
    "    # Ensuring the ratios are valid\n",
    "    if train_ratio <= 0 or train_ratio >= 1:\n",
    "        raise ValueError(\"Train ratio must be between 0 and 1\")\n",
    "\n",
    "    val_ratio = 1 - train_ratio\n",
    "\n",
    "    # Splitting the dataset\n",
    "    datasets = train_dataset.train_test_split(test_size=val_ratio, seed=seed)\n",
    "    train_set = datasets[\"train\"]\n",
    "    val_set = datasets[\"test\"]  # TODO: can I name this eval instead?\n",
    "\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8dc125-b086-4ee3-801b-6eabb43f855c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _preprocess_batch_(\n",
    "    batch,\n",
    "    args: Args,\n",
    "    logger: logging.Logger,\n",
    "    tokenizer: AutoTokenizer,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates formatted prompts and tokenizes in batch mode.\n",
    "\n",
    "    Parameters:\n",
    "    - batch: dict - Batch containing columns as lists.\n",
    "    - args: Args - Arguments needed for formatting.\n",
    "    - tokenizer: AutoTokenizer - Tokenizer for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Fields should be in args\n",
    "    context_field = \"sentence\"\n",
    "    label_field = \"label\"\n",
    "    encoded_label_field = f\"{label_field}_encoded\"\n",
    "    response_field = \"label_decoded\"\n",
    "    text_field = \"input_texts\"\n",
    "    id_field = \"input_ids\"\n",
    "    truncation_field = True\n",
    "    padding_field = True\n",
    "\n",
    "    # Rename a column\n",
    "    batch[encoded_label_field] = batch[label_field]\n",
    "    # Decode the label\n",
    "    batch[response_field] = [\n",
    "        decode_label(label) for label in batch[encoded_label_field]\n",
    "    ]\n",
    "    # Validate the prompts\n",
    "    if not args.instruction_prompt.strip() or not args.system_prompt.strip():\n",
    "        raise ValueError(\"All prompts (instruction, system) must be non-empty strings.\")\n",
    "    # Validate the fields\n",
    "    if not all(item.strip() for item in batch[context_field]) or not all(\n",
    "        item.strip() for item in batch[response_field]\n",
    "    ):\n",
    "        raise ValueError(\"All fields (context, response) must be non-empty strings.\")\n",
    "    # Formatt the input text for the batch\n",
    "    batch[text_field] = [\n",
    "        args.B_INST\n",
    "        + args.B_SYS\n",
    "        + args.system_prompt\n",
    "        + args.E_SYS\n",
    "        + args.instruction_prompt\n",
    "        + context\n",
    "        + args.E_INST\n",
    "        for context in batch[context_field]\n",
    "    ]\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        batch[text_field],\n",
    "        max_length=args.max_seq_length,\n",
    "        truncation=truncation_field,\n",
    "        padding=padding_field,\n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = tokenized_inputs[\"input_ids\"]\n",
    "    batch[\"attention_mask\"] = tokenized_inputs[\"attention_mask\"]\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def preprocess_dataset(\n",
    "    args: Args, logger: logging.Logger, tokenizer: AutoTokenizer, dataset: Dataset\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for supervised fine-tuning.\n",
    "\n",
    "    Parameters:\n",
    "    - args: Args - Arguments needed for formatting.\n",
    "    - tokenizer: AutoTokenizer - Tokenizer for the model.\n",
    "    - dataset: Dataset - Dataset to preprocess.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Fields should be in args\n",
    "    context_field = \"sentence\"\n",
    "    label_field = \"label\"\n",
    "    encoded_label_field = f\"{label_field}_encoded\"\n",
    "    response_field = \"label_decoded\"\n",
    "    text_field = \"input_texts\"\n",
    "    id_field = \"input_ids\"\n",
    "    truncation_field = True\n",
    "    padding_field = True\n",
    "\n",
    "    logger.debug(f\"Preprocessing dataset...\")\n",
    "\n",
    "    # We have to preprocess in batch because datasets dont allow for easy assignment of new fields\n",
    "    dataset = dataset.map(\n",
    "        partial(\n",
    "            _preprocess_batch_,\n",
    "            args=args,\n",
    "            logger=logger,\n",
    "            tokenizer=tokenizer,\n",
    "        ),\n",
    "        batched=True,\n",
    "    )\n",
    "\n",
    "    logger.debug(\"Filtering dataset to ensure we are below the maximum sequence length\")\n",
    "    dataset = dataset.filter(\n",
    "        lambda sample: len(sample[\"input_ids\"]) <= args.max_seq_length\n",
    "    )\n",
    "    logger.debug(\"Shuffling the data using our seed value\")\n",
    "    dataset = dataset.shuffle(seed=args.seed)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8108a0ff-8e38-4b8f-b969-559885e5e597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======= PEFT HELPER FUNCTIONS ===========\n",
    "def create_bnb_config(args):\n",
    "    \"\"\"\n",
    "    Configures BitsAndBytes based on the arguments provided.\n",
    "    \"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=args.load_in_4bit,\n",
    "        load_in_8bit=args.load_in_8bit,\n",
    "        bnb_4bit_use_double_quant=args.bnb_use_double_quant,\n",
    "        bnb_8bit_use_double_quant=args.bnb_use_double_quant,\n",
    "        bnb_4bit_quant_type=args.bnb_quant_type,\n",
    "        bnb_8bit_quant_type=args.bnb_quant_type,\n",
    "        bnb_4bit_compute_dtype=args.bnb_compute_dtype,\n",
    "        bnb_8bit_compute_dtype=args.bnb_compute_dtype,\n",
    "    )\n",
    "    return bnb_config\n",
    "\n",
    "def create_peft_config(args: Args, modules: List[str]) -> LoraConfig:\n",
    "    \"\"\"\n",
    "    Create PEFT configuration for LoRA.\n",
    "\n",
    "    Parameters:\n",
    "    - args : Args - The arguments containing LoRA parameters\n",
    "    - modules : List[str] - List of module names\n",
    "\n",
    "    Returns:\n",
    "    - LoraConfig - Configuration object for PEFT\n",
    "    \"\"\"\n",
    "    return LoraConfig(\n",
    "        target_modules=modules,\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "\n",
    "def find_all_linear_names(model: Type[torch.nn.Module], bits: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find names of all linear layers in the model based on the number of bits specified.\n",
    "\n",
    "    Parameters:\n",
    "    - model : torch.nn.Module - The model to inspect\n",
    "    - bits : int - The number of bits to select the appropriate linear layer class\n",
    "\n",
    "    Returns:\n",
    "    - List[str] - List of linear layer names\n",
    "    \"\"\"\n",
    "\n",
    "    # Selecting the appropriate class based on the number of bits\n",
    "    if bits == 4:\n",
    "        cls = bnb.nn.Linear4bit\n",
    "    elif bits == 8:\n",
    "        cls = bnb.nn.Linear8bitLt\n",
    "    else:\n",
    "        cls = torch.nn.Linear\n",
    "\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    # Removing 'lm_head' if exists (specific to 16-bit scenarios)\n",
    "    lora_module_names.discard(\"lm_head\")\n",
    "\n",
    "    return list(lora_module_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "892c9c70-ad58-48cb-a197-b586911aa006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== MODEL AND TOKENIZER FUNCTIONS ===============\n",
    "def create_tokenizer(args, logger):\n",
    "    \"\"\"\n",
    "    Configures the tokenizer based on the provided arguments.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=False)\n",
    "    tokenizer.pad_token = args.EOS\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def create_model(args, logger, bnb_mode=True, peft_mode=True):\n",
    "    \"\"\"\n",
    "    Applies further configurations to the model based on the arguments provided.\n",
    "    \"\"\"\n",
    "\n",
    "    if not bnb_mode:\n",
    "        logger.debug(\"Creating ModelforCausalLM ...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.model_id,\n",
    "            device_map=args.device_map,\n",
    "            max_memory=args.cuda_max_memory,\n",
    "            torch_dtype=args.bnb_compute_dtype,\n",
    "            trust_remote_code=False,\n",
    "        )\n",
    "    else:\n",
    "        logger.debug(\"Creating BitsAndBytesConfig ...\")\n",
    "        bnb_config = create_bnb_config(args)\n",
    "\n",
    "        logger.debug(\"Creating ModelforCausalLM ...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.model_id,\n",
    "            load_in_4bit=args.load_in_4bit,\n",
    "            load_in_8bit=args.load_in_8bit,\n",
    "            device_map=args.device_map,\n",
    "            max_memory=args.cuda_max_memory,\n",
    "            torch_dtype=args.bnb_compute_dtype,\n",
    "            quantization_config=bnb_config,\n",
    "            trust_remote_code=False,\n",
    "        )\n",
    "\n",
    "        model.config.use_cache = False\n",
    "        model.config.pretraining_tp = 1\n",
    "        info_data = []\n",
    "\n",
    "        logger.debug(\"Logging the model's memory footprint ...\")\n",
    "        memory_footprint = model.get_memory_footprint()\n",
    "        info_data.append([\"Memory Footprint\", memory_footprint])\n",
    "\n",
    "        logger.debug(f\"Logging the model's Dtypes ...\")\n",
    "        dtypes_loaded = log_dtypes(model, logger)\n",
    "        info_data.append([\"Dtypes init\", dtypes_loaded])\n",
    "\n",
    "    if peft_mode:\n",
    "        logger.debug(\"Using the prepare_model_for_kbit_training method from PEFT...\")\n",
    "        model = prepare_model_for_kbit_training(\n",
    "            model, use_gradient_checkpointing=args.gradient_checkpointing\n",
    "        )\n",
    "\n",
    "        logger.debug(f\"Model Dtypes after preparing for kbit training ...\")\n",
    "        dtypes_after = log_dtypes(model, logger)\n",
    "        info_data.append([\"Dtypes After KBit Prep\", dtypes_after])\n",
    "\n",
    "        logger.debug(\n",
    "            \"Get module names for the linear layers where we add LORA adapters...\"\n",
    "        )\n",
    "        layers_for_adapters = find_all_linear_names(model, 4)\n",
    "        logger.debug(f\"Layers for Adapters: {layers_for_adapters}\")\n",
    "        info_data.append([\"Layers for Adapters\", layers_for_adapters])\n",
    "\n",
    "        logger.debug(\n",
    "            \"Create PEFT config for these modules and wrap the model to PEFT...\"\n",
    "        )\n",
    "        peft_config = create_peft_config(args, layers_for_adapters)\n",
    "\n",
    "        logger.debug(f\"Model Dtypes before applying PEFT config ...\")\n",
    "        dtypes_before = log_dtypes(model, logger)\n",
    "        info_data.append([\"Dtypes Before PEFT Config\", dtypes_before])\n",
    "\n",
    "        model = get_peft_model(model, peft_config)\n",
    "\n",
    "        logger.debug(f\"Model Dtypes after applying PEFT config ...\")\n",
    "        dtypes_after_peft = log_dtypes(model, logger)\n",
    "        info_data.append([\"Dtypes After PEFT Config\", dtypes_after_peft])\n",
    "\n",
    "        logger.debug(\"Information about the percentage of trainable parameters...\")\n",
    "        trainable_parameters = log_trainable_parameters(model, logger)\n",
    "        info_data.append([\"Trainable Parameters\", trainable_parameters])\n",
    "\n",
    "    if bnb_mode or peft_mode:\n",
    "        logger.debug(\n",
    "            \"Converting the info_data list into a pandas DataFrame and saving it...\"\n",
    "        )\n",
    "        df = pd.DataFrame(info_data, columns=[\"Info\", \"Value\"])\n",
    "        logger.debug(\"\\n%s\", df.to_string(index=False))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "730e6738-6832-4cd4-9e6b-e2525da53eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== TRAINER FUNCTIONS ===============\n",
    "def setup_training_arguments(args):\n",
    "    \"\"\"\n",
    "    Configures and returns the TrainingArguments based on the provided arguments.\n",
    "    \"\"\"\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        fp16=args.fp16,\n",
    "        bf16=args.bf16,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        weight_decay=args.weight_decay,\n",
    "        optim=args.optim,\n",
    "        learning_rate=args.learning_rate,\n",
    "        lr_scheduler_type=args.lr_scheduler_type,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        max_steps=args.max_steps,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        save_safetensors=args.save_safetensors,\n",
    "        load_best_model_at_end=args.load_best_model_at_end,\n",
    "        push_to_hub=False,\n",
    "        evaluation_strategy=args.evaluation_strategy,\n",
    "        logging_dir=logging_dir,\n",
    "        report_to=args.report_to,\n",
    "        save_strategy=args.save_strategy,\n",
    "        save_steps=args.save_steps,\n",
    "        logging_strategy=args.logging_strategy,\n",
    "        logging_steps=args.logging_steps,\n",
    "        group_by_length=args.group_by_length,\n",
    "    )\n",
    "    return training_arguments\n",
    "\n",
    "class PeftSavingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback to save the PEFT adapters during the model training.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        checkpoint_path = os.path.join(\n",
    "            args.output_dir, f\"checkpoint-{state.global_step}\"\n",
    "        )\n",
    "        kwargs[\"model\"].save_pretrained(checkpoint_path)\n",
    "\n",
    "        if \"pytorch_model.bin\" in os.listdir(checkpoint_path):\n",
    "            os.remove(os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
    "\n",
    "def setup_trainer(\n",
    "    args, model, tokenizer, peft_config, train_dataset, eval_dataset, training_arguments\n",
    "):\n",
    "    \n",
    "    text_field = \"input_texts\"\n",
    "    \"\"\"\n",
    "    Configures and returns the trainer based on the provided arguments and datasets.\n",
    "    \"\"\"\n",
    "    callbacks = [PeftSavingCallback()]\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_arguments,\n",
    "        packing=args.packing,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        peft_config=peft_config,\n",
    "        callbacks=callbacks,\n",
    "        dataset_text_field=text_field,\n",
    "        neftune_noise_alpha=args.neftune_noise_alpha,\n",
    "        tokenizer=tokenizer,\n",
    "        # compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.predict_with_generate = args.predict_with_generate\n",
    "    return trainer\n",
    "\n",
    "def __training__(trainer, args, logger):\n",
    "    \"\"\"\n",
    "    Executes the training and evaluation process based on the configured trainer and arguments.\n",
    "    \"\"\"\n",
    "    logger.info(\n",
    "        \"Evaluating the baseline performance of the model before fine-tuning ...\"\n",
    "    )\n",
    "\n",
    "    # baseline_train_results, baseline_eval_results = baseline_results(trainer, logger)\n",
    "    logger.info(\"Running Supervised Fine Tuning ...\")\n",
    "    logger.debug(\"Trying trainer.train() ...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        raise Exception(e)\n",
    "    finally:\n",
    "        memory_cleanup()\n",
    "\n",
    "    if args.report_to == \"wandb\":\n",
    "        wandb.finish()\n",
    "\n",
    "    logger.debug(\"Trying the final trainer.evaluate() ...\")\n",
    "    try:\n",
    "        final_eval_results = trainer.evaluate()\n",
    "        for key, value in final_eval_results.items():\n",
    "            logger.info(f\" {key}: {value}\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"The final trainer.evaluate() failed !!!\")\n",
    "        logger.error(e)\n",
    "        raise e\n",
    "    trainer.save_state()\n",
    "    logger.debug(\"Creating results dataframe ...\")\n",
    "    # results_df = merge_evaluation_results(baseline_eval_results, final_eval_results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def train(args, logger):\n",
    "    logger.info(\"Starting Supervised Fine Tuning...\")\n",
    "    logger.debug(\"Creating the Tokenizer...\")\n",
    "    tokenizer = create_tokenizer(args, logger)\n",
    "    logger.debug(\"Creating the Model...\")\n",
    "    model = create_model(args, logger, bnb_mode=True, peft_mode=True)\n",
    "\n",
    "    # TODO: have the args do this dynamically, or add more info here\n",
    "    assert args.max_seq_length == get_max_seq_length(model)\n",
    "\n",
    "    logger.debug(\"Loading and preprocessing train dataset...\")\n",
    "    train_dataset = load_dataset_split(args=args, logger=logger, split=\"train\")\n",
    "    train_dataset = preprocess_dataset(\n",
    "        args=args, logger=logger, tokenizer=tokenizer, dataset=train_dataset\n",
    "    )\n",
    "    train_set, val_set = split_dataset(train_dataset, train_ratio=0.7, seed=args.seed)\n",
    "    logger.debug(\"Creating TrainingArguments ...\")\n",
    "    training_arguments = setup_training_arguments(args)\n",
    "    logger.debug(\"Creating PEFT config ...\")\n",
    "    layers_for_adapters = find_all_linear_names(model, 4)\n",
    "    logger.debug(f\"Layers for Adapters: {layers_for_adapters}\")\n",
    "    peft_config = create_peft_config(args, layers_for_adapters)\n",
    "    logger.debug(\"Creating SFTTrainer ...\")\n",
    "    trainer = setup_trainer(\n",
    "        args, model, tokenizer, peft_config, train_set, val_set, training_arguments\n",
    "    )\n",
    "    logger.debug(\"Executing the SFTTrainer pipeline\")\n",
    "    results_df = __training__(trainer, args, logger)\n",
    "    display(results_df)\n",
    "    logger.debug(\"Saving final model and tokenizer states\")\n",
    "    model = trainer.model\n",
    "    # TODO: see about incorporating the arguments save_adapter=True, save_config=True when saving PEFT model\n",
    "    save_model_and_tokenizer(\n",
    "        model_dir=args.checkpoint_dir, logger=logger, model=model, tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dcd96a3-ba70-4d57-bcde-0029b269e060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====== UTILS =======\n",
    "def memory_cleanup():\n",
    "    \"\"\"\n",
    "    Empty VRAM\n",
    "    \"\"\"\n",
    "    if \"trainer\" in locals() or \"trainer\" in globals():\n",
    "        del trainer\n",
    "    if \"model\" in locals() or \"model\" in globals():\n",
    "        del model\n",
    "    if \"pipe\" in locals() or \"pipe\" in globals():\n",
    "        del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def configure_cuda_args(args, logger):\n",
    "    \"\"\"\n",
    "    Configure the parameter arguments using the system's CUDA information\n",
    "    \"\"\"\n",
    "    if args.cuda_n_gpus is None:\n",
    "        args.cuda_n_gpus = torch.cuda.device_count()\n",
    "        logger.debug(f\"args.cuda_n_gpus now defined: {args.cuda_n_gpus}\")\n",
    "    else:\n",
    "        logger.debug(\"args.cuda_n_gpus already defined.\")\n",
    "\n",
    "    if args.cuda_max_memory is None:\n",
    "        CUDA_MAX_MEMORY = f\"{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB\"\n",
    "        args.cuda_max_memory = {i: CUDA_MAX_MEMORY for i in range(args.cuda_n_gpus)}\n",
    "        logger.debug(f\"args.cuda_max_memory now defined: {args.cuda_max_memory}\")\n",
    "    else:\n",
    "        logger.debug(\"args.cuda_max_memory already defined.\")\n",
    "\n",
    "    return args\n",
    "\n",
    "def get_max_seq_length(model: Type[torch.nn.Module]) -> int:\n",
    "    \"\"\"\n",
    "    Get the maximum length of position embeddings in the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model : torch.nn.Module - The model to inspect\n",
    "\n",
    "    Returns:\n",
    "    - int - Maximum length of position embeddings\n",
    "    \"\"\"\n",
    "    conf = model.config\n",
    "    max_seq_length = None\n",
    "\n",
    "    # Checking various attributes to determine max length\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_seq_length = getattr(conf, length_setting, None)\n",
    "        if max_seq_length:\n",
    "            print(f\"Found max sequence length: {max_seq_length}\")\n",
    "            break\n",
    "\n",
    "    # Defaulting to 4096 if no length attribute is found\n",
    "    if not max_seq_length:\n",
    "        max_seq_length = 4096\n",
    "        print(f\"Using default max sequence length: {max_seq_length}\")\n",
    "\n",
    "    return max_seq_length\n",
    "\n",
    "def save_model_and_tokenizer(logger, model, tokenizer, model_dir):\n",
    "    \"\"\"\n",
    "    Save the model and tokenizer in the trainer to the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - model\n",
    "        model object\n",
    "    - tokenizer : PreTrainedTokenizer\n",
    "        The tokenizer to be saved.\n",
    "    - model_dir : str\n",
    "        The directory where the model and tokenizer will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.debug(f\"Model saving to {model_dir} ...\")\n",
    "        model.save_pretrained(model_dir, safe_serialization=True)\n",
    "        logger.debug(f\"Tokenizer saving to {model_dir} ..\")\n",
    "        tokenizer.save_pretrained(model_dir)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while saving the model and tokenizer: {e}\")\n",
    "        raise Exception(e)\n",
    "\n",
    "\n",
    "def setup_output_directory(args):\n",
    "    \"\"\"\n",
    "    Sets up the output directory for saving model checkpoints and other outputs.\n",
    "    \"\"\"\n",
    "    checkpoint_path = args.checkpoint_dir\n",
    "    checkpoint_path.mkdir(mode=0o777, parents=True, exist_ok=True)\n",
    "    mergepoint_path = args.mergepoint_dir\n",
    "    mergepoint_path.mkdir(mode=0o777, parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "712b7b0c-c779-465b-a18f-d958181e80d7",
   "metadata": {},
   "source": [
    "# def load_models(args, logger):\n",
    "#     \"\"\"\n",
    "#     Load the base foundation model and the new finetuned model\n",
    "#     \"\"\"\n",
    "#     # TODO: load_models() should be decomposed or functionalized e.g. base, finetune, etc.?\n",
    "#     base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#         args.model_id,\n",
    "#         device_map=args.device_map,\n",
    "#         max_memory=args.cuda_max_memory,\n",
    "#         torch_dtype=args.bnb_compute_dtype,\n",
    "#     )\n",
    "#     log_dtypes(base_model, logger)\n",
    "#     # Load the fine-tuned model\n",
    "#     logger.debug(\"Creating BitsAndBytesConfig ...\")\n",
    "#     bnb_config = create_bnb_config(args)\n",
    "#     new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#         args.output_dir / \"final_checkpoint\",\n",
    "#         device_map=args.device_map,\n",
    "#         max_memory=args.cuda_max_memory,\n",
    "#         torch_dtype=args.bnb_compute_dtype,\n",
    "#         quantization_config=bnb_config,\n",
    "#     )\n",
    "#     log_dtypes(new_model, logger)\n",
    "#     return base_model, new_model\n",
    "\n",
    "# def save_and_push(args, logger, peft_model, tokenizer):\n",
    "#     \"\"\"\n",
    "#     Save peft model and tokenizer, then push them to the hub\n",
    "#     \"\"\"\n",
    "#     merged_checkpoint_dir = args.output_dir / \"final_merged_checkpoint\"\n",
    "#     logger.debug(f\"Saving the final checkpoint from PEFT model to {merged_checkpoint_dir}\")\n",
    "#     peft_model.save_pretrained(merged_checkpoint_dir, safe_serialization=True)\n",
    "#     logger.debug(f\"Saving the tokenizer to {merged_checkpoint_dir}\")\n",
    "#     tokenizer.save_pretrained(merged_checkpoint_dir)\n",
    "#     logger.debug(f\"Pushing the PEFT'd model and tokenizer to hub repo {args.repo_name}\")\n",
    "#     peft_model.push_to_hub(args.repo_name, private=True, use_temp_dir=True)\n",
    "#     tokenizer.push_to_hub(args.repo_name, private=True, use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0ada57f-a856-4043-a906-062ecfe9c9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= MAIN FUNCTION ===========\n",
    "def main(seed: int = None):\n",
    "    args = setup_args()\n",
    "    if \"logger\" in locals():\n",
    "        pass\n",
    "    else:\n",
    "        logger = create_logger()\n",
    "    args = configure_cuda_args(args, logger)\n",
    "    \n",
    "    if seed > 0:\n",
    "        args.seed = seed\n",
    "        logger.debug(f\"Seed value overriden to '{args.seed}'\")\n",
    "    else:\n",
    "        logger.debug(f\"Seed value is '{args.seed}'\")\n",
    "        \n",
    "    logger.info(\n",
    "        f\"Using k={args.cuda_n_gpus} CUDA GPUs with max memory {args.cuda_max_memory}\"\n",
    "    )\n",
    "\n",
    "    logger.debug(f\"Setting reproducibility seed: '{args.seed}'\")\n",
    "    transformers_set_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    setup_output_directory(args)\n",
    "\n",
    "    try:\n",
    "        train(args, logger)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        raise Exception(e)\n",
    "    finally:\n",
    "        memory_cleanup()\n",
    "\n",
    "    base_model = create_model(args=args, logger=logger, bnb_mode=False, peft_mode=False)\n",
    "    log_dtypes(logger=logger, model=base_model)\n",
    "\n",
    "    logger.debug(\"Creating BitsAndBytesConfig ...\")\n",
    "    bnb_config = create_bnb_config(args)\n",
    "    logger.debug(\"Loading the final checkpoint for the PEFT Model ...\")\n",
    "    new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        args.checkpoint_dir,\n",
    "        device_map=args.device_map,\n",
    "        max_memory=args.cuda_max_memory,\n",
    "        torch_dtype=args.bnb_compute_dtype,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "    log_dtypes(new_model, logger)\n",
    "\n",
    "    logger.debug(f\"Creating the final PEFT Model ...\")\n",
    "    peft_model = PeftModel.from_pretrained(base_model, new_model)\n",
    "    peft_model.merge_and_unload()\n",
    "    log_dtypes(peft_model, logger)\n",
    "\n",
    "    save_model_and_tokenizer(\n",
    "        model_dir=args.mergepoint_dir,\n",
    "        logger=logger,\n",
    "        model=peft_model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # logger.debug(f\"Pushing the PEFT'd model and tokenizer to hub repo {args.repo_name}\")\n",
    "    # peft_model.push_to_hub(args.repo_name, private=True, use_temp_dir=True)\n",
    "    # tokenizer.push_to_hub(args.repo_name, private=True, use_temp_dir=True)\n",
    "    \n",
    "    return peft_model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785ee18-ca54-4296-a825-cca181179d9b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# MAIN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e79c6be-ad2e-4648-a522-f936f2228b49",
   "metadata": {
    "tags": []
   },
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=100'\n",
    "\n",
    "peft_model, tokenizer = main(seed=5768)\n",
    "\n",
    "# if name = \"__main__\" :\n",
    "   # fire.Fire(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97c729-94e6-46d8-be4b-aaf080eeb67c",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# GATHERING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617448c-5b9a-409f-883e-cb4b61ad4ec1",
   "metadata": {
    "tags": []
   },
   "source": [
    "FOMC classification performance; Single A20 NVIDIA GPU, Batch Size of 8:\n",
    "| Model Size | Seed | Tuned | Decoding | Accuracy % | F1 Score % | Missing % | Wall Clock |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 7B | 42 | False | Greedy | 42.54 | 42.81 | 0 | 254s |\n",
    "| 7B | 5768 | False | Greedy | 42.14 | 42.39 | 0 | 254s |\n",
    "| 7B | 78516 | False | Greedy | 41.53 | 41.66 | 0 | 255s |\n",
    "| 7B | 944601 | False | Greedy | 42.54 | 42.76 | 0 | ?m ?s |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1508b-08cc-4f73-80c9-9e8055ebe5b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "With `seed==42`, 1 GPU, batch 8:\n",
    "| Model Size | Tuned | BNB | Decoding | Accuracy | F1 Score | Missing | Inference Time |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 7B | False | False | Greedy | 0.4254032258064516 | 0.42806914082433745 | 0.0 | ? |\n",
    "| 13B | False | False | Greedy | 0.5020161290322581 | 0.4370082141973838 | 0.0 | 7m 30s |\n",
    "| 13B | False | uint8 / 4 bit? | Greedy | 0.4939516129032258 | 0.42175117172580323 | 0.20161290322580644 | 25m |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c063bb0-5236-4bcb-8cdd-d259e5a0dc8c",
   "metadata": {},
   "source": [
    "comparing model sizes: \n",
    "| Model Name | Parameter Count (in Milllions) | Weights Size (in Gigabytes) |\n",
    "| --- | --- | --- |\n",
    "| RoBERTa-base | 125M | 0.5GB |\n",
    "| RoBERTa-large | 355M | 1.5GB |\n",
    "| Llama2-chat | 7,000M | 13GB |\n",
    "| Llama2-chat | 13,000M | 25GB |\n",
    "| Llama2-chat | 70,000M | 120GB |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7476d21-d433-4316-aff2-1324449cf654",
   "metadata": {
    "tags": []
   },
   "source": [
    "fomc data:\n",
    "\n",
    "| metric | total | test | train | fine-tuning | validation |\n",
    "|---|---|---|---|---|---|\n",
    "| pct_total | 100% | 20% | 80% | 56% | 24% |\n",
    "| n_total | 2,476 | 496 | 1,980 | 1,386 | 594 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccacfb3f-61c6-4c5f-8a21-316a531847eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "parameter breakdowns for adapters\n",
    "| Model ID | Weights+Biases | Adapters |\n",
    "|---|---|---|\n",
    "| Llama2 7B | 6_738_415_616 (97.68%) | 15_9907_840 (2.32%) |\n",
    "\n",
    "keep in mind the final model version the adapters values are merged in so there's no added params during final inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50ee9c-2126-4219-bbce-06d64cc1f701",
   "metadata": {},
   "source": [
    "PEFT Ratios\n",
    "\n",
    "`['o_proj', 'up_proj', 'k_proj', 'v_proj', 'gate_proj', 'down_proj', 'q_proj']`\n",
    "\n",
    "| Trainable params | All params | Trainable % |\n",
    "|---|---|---|\n",
    "| 160 M ? 260 M | 7,000 M | 2.29 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f4d284-5114-40ff-8746-f753f5959f10",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# SANDBOX?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80419881-8889-4aba-8277-9f460cb46efc",
   "metadata": {
    "tags": []
   },
   "source": [
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34e7e96b-7c1c-4c9e-89bb-d7d7fdd1000a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = setup_args()\n",
    "args.device, args.cuda_n_gpus, args.cuda_max_memory = \"cuda:0\", 1, {0: \"41GB\"}\n",
    "logger = create_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71b98918-c336-4b99-9a47-b2132750a327",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/tokenizer.model\n",
      "loading file tokenizer.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
    "tokenizer.pad_token = args.EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "befd2593-af6c-4459-9742-b85d30148328",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Detected PIL version 10.0.1\n",
      "loading weights file model.safetensors from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2231fadaf52e4bc39061b5b51748f744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        device_map=args.device,\n",
    "        max_memory=args.cuda_max_memory,\n",
    "        torch_dtype=args.bnb_compute_dtype,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a918f5d-07bb-451d-a9ef-e2af631ad415",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - torch.bfloat16: 6738415616 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "log_dtypes(base_model, logger)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "953ca3e4-857a-40e1-ac0b-99c784c607b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        '/fintech_3/glenn/v_foo/checkpoint-2088',\n",
    "        device_map=args.device,\n",
    "        max_memory=args.cuda_max_memory,\n",
    "        torch_dtype=args.bnb_compute_dtype,\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd1d04ab-ee97-43fa-bd10-5ecb40817986",
   "metadata": {
    "tags": []
   },
   "source": [
    "log_dtypes(new_model, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d1fed27-b7e9-4f85-9f52-1466f951da21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "916d5037-08bb-48a4-b1b9-41b34775bdb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_model = PeftModel.from_pretrained(base_model, '/fintech_3/glenn/v_foo/checkpoint-2088')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "245e335d-653b-40ed-913f-4d1254e10aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1537952-3ac9-4dd0-a229-ec39b3643ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - torch.bfloat16: 6738415616 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "log_dtypes(merged_model, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1e9e516-a905-446e-ad87-feb4b0ef323b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /fintech_3/glenn/v_foo/checkpoint-2088/final_merged_checkpoint/config.json\n",
      "Configuration saved in /fintech_3/glenn/v_foo/checkpoint-2088/final_merged_checkpoint/generation_config.json\n",
      "The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /fintech_3/glenn/v_foo/checkpoint-2088/final_merged_checkpoint/pytorch_model.bin.index.json.\n",
      "tokenizer config file saved in /fintech_3/glenn/results/fomc_communication/Llama-2-7b-chat-hf/0ddeb50b_231029/final_merged_checkpoint/tokenizer_config.json\n",
      "Special tokens file saved in /fintech_3/glenn/results/fomc_communication/Llama-2-7b-chat-hf/0ddeb50b_231029/final_merged_checkpoint/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/fintech_3/glenn/results/fomc_communication/Llama-2-7b-chat-hf/0ddeb50b_231029/final_merged_checkpoint/tokenizer_config.json',\n",
       " '/fintech_3/glenn/results/fomc_communication/Llama-2-7b-chat-hf/0ddeb50b_231029/final_merged_checkpoint/special_tokens_map.json',\n",
       " '/fintech_3/glenn/results/fomc_communication/Llama-2-7b-chat-hf/0ddeb50b_231029/final_merged_checkpoint/tokenizer.model',\n",
       " '/fintech_3/glenn/results/fomc_communication/Llama-2-7b-chat-hf/0ddeb50b_231029/final_merged_checkpoint/added_tokens.json',\n",
       " '/fintech_3/glenn/results/fomc_communication/Llama-2-7b-chat-hf/0ddeb50b_231029/final_merged_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.save_pretrained('/fintech_3/glenn/v_foo/checkpoint-2088/final_merged_checkpoint') #, safe_serialization=True)\n",
    "tokenizer.save_pretrained(args.mergepoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd696b-4c6c-4288-9358-9bd89875050a",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# TEXT GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "676dabc1-be75-4509-a5d3-1820a79c0b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= GENERATION PARAMETERS ===========\n",
    "@dataclass\n",
    "class GenerationParams:\n",
    "    \"\"\"\n",
    "    ##### TODO: Convert my generation params into a config for the project\n",
    "    # from transformers import GenerationConfig\n",
    "    # generation_config = GenerationConfig( args )\n",
    "    ### Tip: add `push_to_hub=True` to push to the Hub\n",
    "    # generation_config.save_pretrained(\"/tmp\", \"translation_generation_config.json\")\n",
    "    ### You could then use the named generation config file to parameterize generation\n",
    "    # generation_config = GenerationConfig.from_pretrained(\"/tmp\", \"translation_generation_config.json\")\n",
    "    # outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    # tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \"\"\"\n",
    "\n",
    "    ## Strategies [https://huggingface.co/docs/transformers/generation_strategies]\n",
    "    ### Greedy Sampling: beams==1; sample==False\n",
    "    ### Multinomial Sampling: beams==1; sample==True\n",
    "    ### Beam Search: beams>1; sample==False\n",
    "    ### Beam Search + Multinomial Sampling: beams>1; sample==True\n",
    "    num_beams: int = 1  # Beams used in non-greedy search\n",
    "    do_sample: bool = (\n",
    "        False  # Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "    )\n",
    "\n",
    "    ## Contrastive Search Parameters; penalty_alpha, top_k [https://huggingface.co/blog/introducing-csearch]\n",
    "    penalty_alpha: float = None  # float = 0.6 #\n",
    "    # top_k: int = None # 4 # none for greedy\n",
    "\n",
    "    # TODO: should these be here? or in other args?\n",
    "    # eos_token_id=model.config.eos_token_id,\n",
    "    # pad_token=model.config.pad_token_id,\n",
    "\n",
    "    # TODO: how to unset a parameter in the pre-loaded config?\n",
    "    # /home/AD/gmatlin3/.conda/envs/conference/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
    "\n",
    "    ## Other Parameters\n",
    "    max_new_tokens: int = 100  # The maximum numbers of tokens to generate\n",
    "    min_length: int = None  # The minimum length of the sequence to be generated, input prompt + min_new_tokens\n",
    "    use_cache: bool = True  # Whether or not the model should use the past last key/values attentions Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
    "    top_p: float = 1.0  # If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "    temperature: float = (\n",
    "        None  # The value used to modulate the next token probabilities.\n",
    "    )\n",
    "    repetition_penalty: float = (\n",
    "        1.0  # Parameter for repetition penalty. 1.0 means no penalty.\n",
    "    )\n",
    "    length_penalty: int = (\n",
    "        1  # Exponential penalty to the length that is used with beam-based generation.\n",
    "    )\n",
    "    max_padding_length: int = (\n",
    "        4096  # Max padding length to be used with tokenizer padding the prompts.\n",
    "    )\n",
    "    num_return_sequences: int = 1\n",
    "    early_stopping: bool = True\n",
    "    return_dict_in_generate: bool = False\n",
    "    output_scores: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77cbaa7d-1a54-4660-89b3-9b587c313543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= TEXT GENERATION ===========\n",
    "def __text_generation__(generation_params: GenerationParams, model_path: str = None, seed: int = None, n_limit: int = -1):\n",
    "    print(f\"Getting logger ...\")\n",
    "    if \"logger\" in locals():\n",
    "        pass\n",
    "    else:\n",
    "        logger = create_logger()\n",
    "\n",
    "    logger.debug(\"Setting up argument parameters ...\")\n",
    "    args = setup_args()\n",
    "    \n",
    "    if model_path:\n",
    "        args.mergepoint_dir = model_path\n",
    "    \n",
    "    if seed>0:\n",
    "        args.seed = seed\n",
    "        logger.debug(f\"Seed value overriden to '{args.seed}'\")\n",
    "    \n",
    "    logger.debug(f\"Model ID: '{args.model_id}'\")\n",
    "    # TODO: Fields should be in args.X\n",
    "    args_bnb_mode = False\n",
    "    context_field = \"sentence\"\n",
    "    label_field = \"label\"\n",
    "    encoded_label_field = f\"{label_field}_encoded\"\n",
    "    response_field = \"label_decoded\"\n",
    "    text_field = \"input_texts\"\n",
    "    id_field = \"input_ids\"\n",
    "    truncation_field = True\n",
    "    padding_field = True\n",
    "\n",
    "    logger.debug(\"Configuring CUDA ...\")\n",
    "    # args = configure_cuda_args(args, logger)\n",
    "    args.device, args.cuda_n_gpus, args.cuda_max_memory = \"cuda:0\", 1, {0: \"41GB\"}\n",
    "    \n",
    "    logger.info(\n",
    "        f\"Using k={args.cuda_n_gpus} CUDA GPUs with max memory {args.cuda_max_memory}\"\n",
    "    )\n",
    "\n",
    "    logger.debug(f\"Setting reproducibility seed: '{args.seed}'\")\n",
    "    transformers_set_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    logger.debug(\"Creating the Tokenizer ...\")\n",
    "    tokenizer = create_tokenizer(args=args, logger=logger)\n",
    "\n",
    "    # logger.debug(\"Creating the Model ...\")\n",
    "    # model = create_model(\n",
    "    #     args=args, logger=logger, bnb_mode=args_bnb_mode, peft_mode=False\n",
    "    # )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.mergepoint_dir,\n",
    "        device_map=args.device,\n",
    "        max_memory=args.cuda_max_memory,\n",
    "        torch_dtype=args.bnb_compute_dtype,\n",
    "    )\n",
    "    \n",
    "    logger.debug(\n",
    "        \"Specifing some generation configs when loading the model, doesnt work if doing it inside of .generate()!!!\"\n",
    "    )\n",
    "    model.generation_config.do_sample = generation_params.do_sample\n",
    "    model.generation_config.temperature = generation_params.temperature\n",
    "\n",
    "    test_dataset = load_dataset_split(args, logger, \"test\")\n",
    "\n",
    "    if n_limit >= 1:\n",
    "        test_dataset = test_dataset.select(range(n_limit))\n",
    "\n",
    "    test_dataset = preprocess_dataset(args, logger, tokenizer, test_dataset)\n",
    "    logger.debug(\n",
    "        f\"Creating the Test DataLoader with batch size == {args.per_device_eval_batch_size} ...\"\n",
    "    )\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=args.per_device_eval_batch_size\n",
    "    )\n",
    "    logger.debug(f\"Sending the model to device '{args.device}'\")\n",
    "    model.eval()  # TODO: double check I need to use model.eval() here?\n",
    "\n",
    "    if not args_bnb_mode:\n",
    "        model.to(args.device)\n",
    "\n",
    "    logger.info(\"Generating text ...\")\n",
    "    test_responses = []\n",
    "    start = time.perf_counter()\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        inputs = tokenizer(\n",
    "            batch[text_field],\n",
    "            padding=padding_field,\n",
    "            truncation=truncation_field,\n",
    "            max_length=generation_params.max_padding_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # input_ids = torch.tensor(input_ids).long() ## TODO: extra code to ensure that input_ids is a PyTorch tensor ... is unneeded\n",
    "        inputs.to(args.device)\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=generation_params.max_new_tokens,\n",
    "                    do_sample=generation_params.do_sample,\n",
    "                    temperature=generation_params.temperature,\n",
    "                    top_p=generation_params.top_p,\n",
    "                    # top_k=generation_params.top_k,\n",
    "                    min_length=generation_params.min_length,\n",
    "                    use_cache=generation_params.use_cache,\n",
    "                    repetition_penalty=generation_params.repetition_penalty,\n",
    "                    length_penalty=generation_params.length_penalty,\n",
    "                    num_return_sequences=generation_params.num_return_sequences,\n",
    "                )\n",
    "            except TypeError as e:\n",
    "                logger.error(f\"An error occurred during generation: {e}\")\n",
    "                raise TypeError(e)\n",
    "        generated_texts = [\n",
    "            tokenizer.decode(gen_id, skip_special_tokens=True)\n",
    "            for gen_id in generated_ids\n",
    "        ]\n",
    "        # generated_texts = tokenizer.batch_decode(sequences=gen_id, skip_special_tokens=True)\n",
    "\n",
    "        test_responses.extend(generated_texts)\n",
    "\n",
    "    e2e_inference_time = (time.perf_counter() - start) * 1000\n",
    "    logger.debug(f\"the inference time is {e2e_inference_time} ms\")\n",
    "\n",
    "    predicted_labels = [\n",
    "        extract_label(test_responses[i]) for i in range(len(test_responses))\n",
    "    ]\n",
    "    logger.debug(\n",
    "        f\"Predicted label_encoded counts:\\n {pd.Series(predicted_labels).value_counts().to_string()}\"\n",
    "    )\n",
    "    true_labels = test_dataset[\"label_encoded\"]\n",
    "    logger.debug(\n",
    "        f\"Ground truth label_encoded counts:\\n {pd.Series(true_labels).value_counts().to_string()}\"\n",
    "    )\n",
    "\n",
    "    logger.debug(\"Evaluating prediction metrics ...\")\n",
    "    accuracy_perc, f1_score_perc, missing_perc = evaluate_predictions(\n",
    "        true_labels, predicted_labels\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Accuracy: {accuracy_perc}\")\n",
    "    logger.info(f\"F1 Score: {f1_score_perc}\")\n",
    "    logger.info(f\"Missing Percent: {missing_perc}\")\n",
    "\n",
    "    return test_dataset, generated_texts, true_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f524ed-f83a-42f5-9c07-abcd7fe3e707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - DEBUG - Setting up argument parameters ...\n",
      "llama2_finetune - DEBUG - Seed value overriden to '5768'\n",
      "llama2_finetune - DEBUG - Model ID: 'meta-llama/Llama-2-7b-chat-hf'\n",
      "llama2_finetune - DEBUG - Configuring CUDA ...\n",
      "llama2_finetune - INFO - Using k=1 CUDA GPUs with max memory {0: '41GB'}\n",
      "llama2_finetune - DEBUG - Setting reproducibility seed: '5768'\n",
      "llama2_finetune - DEBUG - Creating the Tokenizer ...\n",
      "loading file tokenizer.model from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/tokenizer.model\n",
      "loading file tokenizer.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/94b07a6e30c3292b8265ed32ffdeccfdadf434a8/tokenizer_config.json\n",
      "loading configuration file /fintech_3/glenn/v_foo/checkpoint-2088/final_merged_checkpoint/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/fintech_3/glenn/v_foo/checkpoint-2088/final_merged_checkpoint\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /fintech_3/glenn/v_foo/checkpoint-2088/final_merged_checkpoint/pytorch_model.bin.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting logger ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f61ca7b2c749b7917a672e4a18cb29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at /fintech_3/glenn/v_foo/checkpoint-2088/final_merged_checkpoint.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file /fintech_3/glenn/v_foo/checkpoint-2088/final_merged_checkpoint/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "llama2_finetune - DEBUG - Specifing some generation configs when loading the model, doesnt work if doing it inside of .generate()!!!\n",
      "llama2_finetune - DEBUG - Loading test dataset...\n",
      "llama2_finetune - DEBUG - Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9418b57ac84e97a6ef9773e70bee15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - DEBUG - Filtering dataset to ensure we are below the maximum sequence length\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54127ba53637406b9f42f618d15a8690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - DEBUG - Shuffling the data using our seed value\n",
      "llama2_finetune - DEBUG - Creating the Test DataLoader with batch size == 8 ...\n",
      "llama2_finetune - DEBUG - Sending the model to device 'cuda:0'\n",
      "llama2_finetune - INFO - Generating text ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4776ed231e2d4e43abd01f2f526d87b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AD/gmatlin3/.conda/envs/conference/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_dataset, generated_texts, true_labels, predicted_labels = __text_generation__(\n",
    "    GenerationParams(), seed=5768, model_path='/fintech_3/glenn/v_foo/checkpoint-2088/final_merged_checkpoint'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1231ae-b861-4a1c-bc1b-64110b3003b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad083d8d-e666-4ac4-a458-ca8c7048730e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531f6ad-c5ee-434c-b995-7dd1227822d3",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
