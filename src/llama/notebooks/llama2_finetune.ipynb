{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7b7872-f90b-4f28-85dc-800e391cd279",
   "metadata": {},
   "source": [
    "# NOT FOR PUBLIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ec6ed-8ee9-49e2-8b67-b9acbb0b9618",
   "metadata": {},
   "source": [
    "## HF/WNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "946c77d4-a5a6-4d2c-a3dc-f0671d6e73a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/AD/gmatlin3/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find llama2_sft.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mglennmatlin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import huggingface_hub\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/home/AD/gmatlin3/.cache/huggingface/\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/home/AD/gmatlin3/.cache/huggingface/hub/\"\n",
    "HF_AUTH = \"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\"\n",
    "huggingface_hub.login(HF_AUTH)\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Set the wandb project where this run will be logged\n",
    "WANDB_PROJECT = f\"llama2_sft_fomc\"\n",
    "os.environ[\n",
    "    \"WANDB_PROJECT\"\n",
    "] = WANDB_PROJECT\n",
    "# Turn off save your trained model checkpoint to wandb (our models are too large)\n",
    "os.environ[\n",
    "    \"WANDB_LOG_MODEL\"\n",
    "] = \"false\"\n",
    "\n",
    "# Turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"fa69ffc6a97578da0410b553042cbb8b3bf5fcaf\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = f\"llama2_sft\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c2001-f079-478b-84b2-617f6c89f92d",
   "metadata": {},
   "source": [
    "# TODOS\n",
    "\n",
    "- See if `offload_folder=\"tmp\"` can help when loading models?\n",
    "\n",
    "- `input_ids = torch.tensor(input_ids).long()` # TODO: extra code to ensure that input_ids is a PyTorch tensor ... is unneeded\n",
    "\n",
    "- How to unset a parameter in the pre-loaded config?\n",
    "> /home/AD/gmatlin3/.conda/envs/conference/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d0a623-6c5a-42b2-9d13-c8d7972e94d1",
   "metadata": {},
   "source": [
    "# [PUBLIC]<br>Supervised Fine-Tuning of Llama2 on FOMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b51727-06dc-4063-8dee-b83846f9a67b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714bce9-27b2-4244-af13-276cf6b83d1f",
   "metadata": {},
   "source": [
    "### Standard Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff718620-a88b-4332-96ca-9a2ea5f414b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "import fire\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import NamedTuple, List, Type\n",
    "from IPython.display import display\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f452c-c384-49e9-bf05-9efa60fdcbe6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Third-Party Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d987f0-200a-49cc-a275-12bb14c66e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371d5d97-89b2-4d7d-9b36-29752d08598b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PyTorch and HuggingFace Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b66f6e0-88be-4bc3-a64e-3cb92495dbf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import logging as hf_logging\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers import set_seed as transformers_set_seed\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    AutoPeftModelForCausalLM,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from dataclasses import dataclass, fields, make_dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001f60d-383a-4589-8045-b5ff122221a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4418ec62-b4f0-4fd8-8685-e414f3202385",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47450a50-3e47-4957-90a1-68ed9781ef73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_uid(id_length=8, dt_format=\"%y%m%d\"):\n",
    "    date_str = datetime.now().strftime(dt_format)\n",
    "    uid = str(uuid.uuid4())[:id_length]\n",
    "    uid = f\"{uid}_{date_str}\"\n",
    "    return uid\n",
    "\n",
    "def create_logger(name=\"llama2_finetune\", level=logging.DEBUG):\n",
    "    logger = logging.getLogger(name)\n",
    "    if not logger.hasHandlers():\n",
    "        logger.setLevel(level)\n",
    "        hf_logging.set_verbosity(level)\n",
    "\n",
    "        # Create handlers\n",
    "        c_handler = logging.StreamHandler()\n",
    "        f_handler = logging.FileHandler(\"llama2_finetune.log\")\n",
    "        c_handler.setLevel(level)\n",
    "        f_handler.setLevel(level)\n",
    "\n",
    "        # Create formatters and add it to handlers\n",
    "        format = \"%(name)s - %(levelname)s - %(message)s\"\n",
    "        c_handler.setFormatter(logging.Formatter(format))\n",
    "        f_handler.setFormatter(logging.Formatter(format))\n",
    "\n",
    "        # Add handlers to the logger\n",
    "        logger.addHandler(c_handler)\n",
    "        logger.addHandler(f_handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f52690-4bdc-4e18-9ab0-05695e42ce59",
   "metadata": {},
   "source": [
    "### Label Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81061777-0ac9-4263-931d-6f2d11f2f311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOMC_COMMUNICATION_MAPPING = {0: \"DOVISH\", 1: \"HAWKISH\", 2: \"NEUTRAL\"}\n",
    "\n",
    "# Function to decode the labels\n",
    "def decode_label(label_number):\n",
    "    return FOMC_COMMUNICATION_MAPPING.get(label_number, \"undefined\").upper()\n",
    "\n",
    "\n",
    "# Function to encode the labels\n",
    "def encode_label(label_name):\n",
    "    reversed_mapping = {v: k for k, v in FOMC_COMMUNICATION_MAPPING.items()}\n",
    "    return reversed_mapping.get(label_name.lower(), -1)\n",
    "\n",
    "\n",
    "# TODO: have extract_lavel use our encoding/mapping\n",
    "def extract_label(text_output, label_list=[\"DOVISH\", \"HAWKISH\", \"NEUTRAL\"], E_INST=\"[/INST]\"):\n",
    "    \"\"\"\n",
    "    Extracts the label from the text output from a large language model\n",
    "    \"\"\"\n",
    "    # Find the 'end of instruction' token and remove text before it\n",
    "    response_pos = text_output.find(E_INST)\n",
    "    # Convert the string to uppercase for case-insensitive search\n",
    "    generated_text = text_output[response_pos + len(E_INST) :].strip().upper()\n",
    "    # Define the substring options\n",
    "    label_list = [\"DOVISH\", \"HAWKISH\", \"NEUTRAL\"]\n",
    "    # Iterate over the substrings and find the matching label\n",
    "    for i, label in enumerate(label_list):\n",
    "        if label in generated_text:\n",
    "            return i\n",
    "    # If none of the substrings are found, return -1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4a492-e3dd-4503-8bef-18a59ba6b78b",
   "metadata": {},
   "source": [
    "### Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9011278a-51cf-482f-bc67-ee6585702271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred, tokenizer, label_list=[\"DOVISH\", \"HAWKISH\", \"NEUTRAL\"]):\n",
    "    predictions, true_labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(sequences=pred, skip_special_tokens=True)\n",
    "    # Decode the predictions to text\n",
    "    # decoded_preds = [\n",
    "    #     tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions\n",
    "    # ]    \n",
    "    predicted_labels = [\n",
    "        extract_label(decoded_preds[i]) for i in range(len(decoded_preds))\n",
    "    ]\n",
    "    accuracy_perc, f1_score_perc, missing_perc = evaluate_predictions(\n",
    "        true_labels, predicted_labels\n",
    "    )\n",
    "    return {\n",
    "        \"accuracy\": accuracy_perc,\n",
    "        \"f1_score\": f1_score_perc,\n",
    "        \"missing\": missing_perc\n",
    "    }\n",
    "\n",
    "def evaluate_predictions(true_labels, predicted_labels):\n",
    "    accuracy_perc = accuracy_score(true_labels, predicted_labels)\n",
    "    f1_score_perc = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
    "    missing_perc = (predicted_labels.count(-1) / len(predicted_labels)) * 100.0\n",
    "    return accuracy_perc, f1_score_perc, missing_perc\n",
    "\n",
    "def log_trainable_parameters(model, logger: logging.Logger):\n",
    "    \"\"\"\n",
    "    Logs the number of trainable parameters in the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model : torch.nn.Module - The model to log.\n",
    "    - logger : logging.Logger - Logger to use for logging the info.\n",
    "    \"\"\"\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    logger.info(\n",
    "        f\"Trainable params: {trainable_params} || \"\n",
    "        f\"All params: {total_params} || \"\n",
    "        f\"Trainable%: {100 * trainable_params / total_params}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def log_dtypes(model, logger: logging.Logger):\n",
    "    \"\"\"\n",
    "    Logs the data types of the model parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - model : torch.nn.Module - The model to log.\n",
    "    - logger : logging.Logger - Logger to use for logging the info.\n",
    "    \"\"\"\n",
    "    dtypes = {}\n",
    "\n",
    "    for p in model.parameters():\n",
    "        dtype = p.dtype\n",
    "        dtypes[dtype] = dtypes.get(dtype, 0) + p.numel()\n",
    "\n",
    "    total = sum(dtypes.values())\n",
    "\n",
    "    for dtype, count in dtypes.items():\n",
    "        logger.info(f\"{dtype}: {count} ({100 * count / total:.2f}%)\")\n",
    "\n",
    "\n",
    "def merge_evaluation_results(\n",
    "    baseline_results: dict, final_results: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge evaluation results for comparison.\n",
    "    \"\"\"\n",
    "    all_metrics = set(baseline_results.keys()).union(final_results.keys())\n",
    "    data = {\"Metric\": [], \"Baseline\": [], \"After Fine-tuning\": []}\n",
    "\n",
    "    for metric in all_metrics:\n",
    "        data[\"Metric\"].append(metric)\n",
    "        data[\"Baseline\"].append(baseline_results.get(metric, \"N/A\"))\n",
    "        data[\"After Fine-tuning\"].append(final_results.get(metric, \"N/A\"))\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66037cb6-eb5f-4521-8615-a82be23ebe9a",
   "metadata": {},
   "source": [
    "### Dataset Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47ab2efd-4634-4fce-a255-6305bd091b69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset_split(args, logger, split: str):\n",
    "    \"\"\"\n",
    "    Load a dataset split\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Loading {split} dataset...\")\n",
    "    dataset_split = load_dataset(f\"{args.organization}/{args.task_name}\")[split]\n",
    "\n",
    "    return dataset_split\n",
    "\n",
    "\n",
    "def split_dataset(train_dataset, train_ratio=0.7, seed=42):\n",
    "    \"\"\"\n",
    "    Split a Hugging Face dataset into training and validation sets with a given ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - train_dataset: Hugging Face dataset to split\n",
    "    - train_ratio: Ratio of data to keep in the training set\n",
    "    - seed: Seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - train_set: Training dataset\n",
    "    - val_set: Validation dataset\n",
    "    \"\"\"\n",
    "    # Ensuring the ratios are valid\n",
    "    if train_ratio <= 0 or train_ratio >= 1:\n",
    "        raise ValueError(\"Train ratio must be between 0 and 1\")\n",
    "\n",
    "    val_ratio = 1 - train_ratio\n",
    "\n",
    "    # Splitting the dataset\n",
    "    datasets = train_dataset.train_test_split(test_size=val_ratio, seed=seed)\n",
    "    train_set = datasets[\"train\"]\n",
    "    val_set = datasets[\"test\"]  # TODO: can I name this eval instead?\n",
    "\n",
    "    return train_set, val_set\n",
    "\n",
    "def _preprocess_dataset_batch_(\n",
    "    batch,\n",
    "    args,\n",
    "    logger: logging.Logger,\n",
    "    tokenizer: AutoTokenizer,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates formatted prompts and tokenizes in batch mode.\n",
    "\n",
    "    Parameters:\n",
    "    - batch: dict - Batch containing columns as lists.\n",
    "    - args: Args - Arguments needed for formatting.\n",
    "    - tokenizer: AutoTokenizer - Tokenizer for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Rename a column\n",
    "    batch[args.encoded_label_field] = batch[args.label_field]\n",
    "    # Decode the label\n",
    "    batch[args.response_field] = [\n",
    "        decode_label(label) for label in batch[args.encoded_label_field]\n",
    "    ]\n",
    "    # Validate the prompts\n",
    "    if not args.instruction_prompt.strip() or not args.system_prompt.strip():\n",
    "        raise ValueError(\"All prompts (instruction, system) must be non-empty strings.\")\n",
    "    # Validate the fields\n",
    "    if not all(item.strip() for item in batch[args.context_field]) or not all(\n",
    "        item.strip() for item in batch[args.response_field]\n",
    "    ):\n",
    "        raise ValueError(\"All fields (context, response) must be non-empty strings.\")\n",
    "    # Formatt the input text for the batch\n",
    "    batch[args.text_field] = [\n",
    "        args.B_INST\n",
    "        + args.B_SYS\n",
    "        + args.system_prompt\n",
    "        + args.E_SYS\n",
    "        + args.instruction_prompt\n",
    "        + context\n",
    "        + args.E_INST\n",
    "        for context in batch[args.context_field]\n",
    "    ]\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        batch[args.text_field],\n",
    "        max_length=args.max_seq_length,\n",
    "        truncation=args.truncation,\n",
    "        padding=args.padding,\n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = tokenized_inputs[\"input_ids\"]\n",
    "    batch[\"attention_mask\"] = tokenized_inputs[\"attention_mask\"]\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def preprocess_dataset(\n",
    "    args, logger: logging.Logger, tokenizer: AutoTokenizer, dataset: Dataset\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for supervised fine-tuning.\n",
    "\n",
    "    Parameters:\n",
    "    - args: Args - Arguments needed for formatting.\n",
    "    - tokenizer: AutoTokenizer - Tokenizer for the model.\n",
    "    - dataset: Dataset - Dataset to preprocess.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.debug(f\"Preprocessing dataset...\")\n",
    "\n",
    "    # We have to preprocess in batch because datasets dont allow for easy assignment of new fields\n",
    "    dataset = dataset.map(\n",
    "        partial(\n",
    "            _preprocess_dataset_batch_,\n",
    "            args=args,\n",
    "            logger=logger,\n",
    "            tokenizer=tokenizer,\n",
    "        ),\n",
    "        batched=True,\n",
    "    )\n",
    "\n",
    "    logger.debug(\"Filtering dataset to ensure we are below the maximum sequence length\")\n",
    "    dataset = dataset.filter(\n",
    "        lambda sample: len(sample[\"input_ids\"]) <= args.max_seq_length\n",
    "    )\n",
    "    logger.debug(\"Shuffling the data using our seed value\")\n",
    "    dataset = dataset.shuffle(seed=args.seed)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744f64b-5aa7-4d51-b3c1-b2609568c78f",
   "metadata": {},
   "source": [
    "### Model & Tokenizer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "892c9c70-ad58-48cb-a197-b586911aa006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(args, logger):\n",
    "    \"\"\"\n",
    "    Configures the tokenizer based on the provided arguments.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, use_fast=False)\n",
    "    tokenizer.pad_token = args.EOS\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def create_model(args, logger, bnb_config=None, peft_config=None):\n",
    "    \"\"\"\n",
    "    Applies further configurations to the model based on the arguments provided.\n",
    "    \"\"\"\n",
    "    info_data = []\n",
    "\n",
    "    if bnb_config is not None:\n",
    "        logger.debug(\"Creating ModelforCausalLM using BitsAndBytes ...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.model_id,\n",
    "            load_in_4bit=args.load_in_4bit,\n",
    "            load_in_8bit=args.load_in_8bit,\n",
    "            device_map=args.device_map,\n",
    "            max_memory=args.cuda_max_memory,\n",
    "            torch_dtype=args.bnb_compute_dtype,\n",
    "            quantization_config=bnb_config,\n",
    "        )\n",
    "    else:\n",
    "        logger.debug(\"Creating ModelforCausalLM ...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.model_id,\n",
    "            device_map=args.device_map,\n",
    "            max_memory=args.cuda_max_memory,\n",
    "            torch_dtype=args.bnb_compute_dtype,\n",
    "        )\n",
    "\n",
    "    logger.debug(\"Logging the model's memory footprint ...\")\n",
    "    memory_footprint = model.get_memory_footprint()\n",
    "    info_data.append([\"Memory Footprint\", memory_footprint])\n",
    "    logger.debug(f\"Logging the model's Dtypes ...\")\n",
    "    dtypes_loaded = log_dtypes(model, logger)\n",
    "    info_data.append([\"Dtypes init\", dtypes_loaded])\n",
    "    \n",
    "    if peft_config is not None:\n",
    "        logger.debug(f\"Model Dtypes before applying PEFT config ...\")\n",
    "        dtypes_before = log_dtypes(model, logger)\n",
    "        info_data.append([\"Dtypes Before PEFT Config\", dtypes_before])\n",
    "\n",
    "        model = get_peft_model(model, peft_config)\n",
    "\n",
    "        logger.debug(f\"Model Dtypes after applying PEFT config ...\")\n",
    "        dtypes_after_peft = log_dtypes(model, logger)\n",
    "        info_data.append([\"Dtypes After PEFT Config\", dtypes_after_peft])\n",
    "\n",
    "        logger.debug(\"Information about the percentage of trainable parameters...\")\n",
    "        trainable_parameters = log_trainable_parameters(model, logger)\n",
    "        info_data.append([\"Trainable Parameters\", trainable_parameters])\n",
    "\n",
    "    if bnb_config or peft_config:\n",
    "        logger.debug(\n",
    "            \"Converting the info_data list into a pandas DataFrame and saving it...\"\n",
    "        )\n",
    "        df = pd.DataFrame(info_data, columns=[\"Info\", \"Value\"])\n",
    "        logger.debug(\"\\n%s\", df.to_string(index=False))\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93a49a-8a6c-402f-b14f-91a6116d471e",
   "metadata": {},
   "source": [
    "### Configuration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8108a0ff-8e38-4b8f-b969-559885e5e597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bnb_config(args):\n",
    "    \"\"\"\n",
    "    Configures BitsAndBytes based on the arguments provided.\n",
    "    \"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=args.load_in_4bit,\n",
    "        load_in_8bit=args.load_in_8bit,\n",
    "        bnb_4bit_use_double_quant=args.bnb_use_double_quant,\n",
    "        bnb_8bit_use_double_quant=args.bnb_use_double_quant,\n",
    "        bnb_4bit_quant_type=args.bnb_quant_type,\n",
    "        bnb_8bit_quant_type=args.bnb_quant_type,\n",
    "        bnb_4bit_compute_dtype=args.bnb_compute_dtype,\n",
    "        bnb_8bit_compute_dtype=args.bnb_compute_dtype,\n",
    "    )\n",
    "    return bnb_config\n",
    "\n",
    "def create_peft_config(args, modules: List[str]) -> LoraConfig:\n",
    "    \"\"\"\n",
    "    Create PEFT configuration for LoRA.\n",
    "\n",
    "    Parameters:\n",
    "    - args : Args - The arguments containing LoRA parameters\n",
    "    - modules : List[str] - List of module names\n",
    "\n",
    "    Returns:\n",
    "    - LoraConfig - Configuration object for PEFT\n",
    "    \"\"\"\n",
    "    return LoraConfig(\n",
    "        target_modules=modules,\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "\n",
    "def create_generation_config(args) -> GenerationConfig:\n",
    "    generation_config = GenerationConfig(\n",
    "                    # force_words_ids=force_words_ids,\n",
    "                    max_new_tokens=args.max_new_tokens,\n",
    "                    do_sample=args.do_sample,\n",
    "                    num_beams=args.num_beams,\n",
    "                    temperature=args.temperature,\n",
    "                    top_p=args.top_p,\n",
    "                    top_k=args.top_k,\n",
    "                    penalty_alpha=args.penalty_alpha,\n",
    "                    min_length=args.min_length,\n",
    "                    use_cache=args.use_cache,\n",
    "                    repetition_penalty=args.repetition_penalty,\n",
    "                    length_penalty=args.length_penalty,\n",
    "                    num_return_sequences=args.num_return_sequences,\n",
    "                    early_stopping=args.early_stopping,\n",
    "                    remove_invalid_values=args.remove_invalid_values,\n",
    "                    no_repeat_ngram_size=args.no_repeat_ngram_size,\n",
    "                    # push_to_hub=args.push_to_hub\n",
    "                )\n",
    "    return generation_config\n",
    "# generation_config.save_pretrained(args.output_dir, \"generation_config.json\")\n",
    "# ## You could then use the named generation config file to parameterize generation\n",
    "# generation_config = GenerationConfig.from_pretrained(args.output_dir, \"generation_config.json\")\n",
    "# outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "# tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "def create_training_arguments(args) -> TrainingArguments:\n",
    "    \"\"\"\n",
    "    Configures and returns the TrainingArguments based on the provided arguments.\n",
    "    \"\"\"\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        fp16=args.fp16,\n",
    "        bf16=args.bf16,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        weight_decay=args.weight_decay,\n",
    "        optim=args.optim,\n",
    "        learning_rate=args.learning_rate,\n",
    "        lr_scheduler_type=args.lr_scheduler_type,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        max_steps=args.max_steps,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        save_safetensors=args.save_safetensors,\n",
    "        load_best_model_at_end=args.load_best_model_at_end,\n",
    "        evaluation_strategy=args.evaluation_strategy,\n",
    "        logging_dir=args.logging_dir,\n",
    "        report_to=args.report_to,\n",
    "        save_strategy=args.save_strategy,\n",
    "        save_steps=args.save_steps,\n",
    "        logging_strategy=args.logging_strategy,\n",
    "        logging_steps=args.logging_steps,\n",
    "        push_to_hub=False,\n",
    "        # group_by_length=args.group_by_length,\n",
    "        # torch_compile=args.torch_compile,\n",
    "    )\n",
    "    return training_arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68489c51-da55-4856-8a28-ad24cf2ca7c6",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dcd96a3-ba70-4d57-bcde-0029b269e060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def memory_cleanup():\n",
    "    \"\"\"\n",
    "    Empty VRAM\n",
    "    \"\"\"\n",
    "    if \"trainer\" in locals() or \"trainer\" in globals():\n",
    "        del trainer\n",
    "    if \"model\" in locals() or \"model\" in globals():\n",
    "        del model\n",
    "    if \"pipe\" in locals() or \"pipe\" in globals():\n",
    "        del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "\n",
    "def set_seeds(args, logger):\n",
    "    logger.debug(f\"Setting reproducibility seed: '{args.seed}'\")\n",
    "    transformers_set_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "def configure_cuda_args(args, logger):\n",
    "    \"\"\"\n",
    "    Configure the parameter arguments using the system's CUDA information\n",
    "    \"\"\"\n",
    "    if args.cuda_n_gpus is None:\n",
    "        args.cuda_n_gpus = torch.cuda.device_count()\n",
    "        logger.debug(f\"args.cuda_n_gpus now defined: {args.cuda_n_gpus}\")\n",
    "    else:\n",
    "        logger.debug(\"args.cuda_n_gpus already defined.\")\n",
    "\n",
    "    if args.cuda_max_memory is None:\n",
    "        CUDA_MAX_MEMORY = f\"{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB\"\n",
    "        args.cuda_max_memory = {i: CUDA_MAX_MEMORY for i in range(args.cuda_n_gpus)}\n",
    "        logger.debug(f\"args.cuda_max_memory now defined: {args.cuda_max_memory}\")\n",
    "    else:\n",
    "        logger.debug(\"args.cuda_max_memory already defined.\")\n",
    "\n",
    "    return args\n",
    "\n",
    "def get_max_seq_length(model: Type[torch.nn.Module]) -> int:\n",
    "    \"\"\"\n",
    "    Get the maximum length of position embeddings in the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model : torch.nn.Module - The model to inspect\n",
    "\n",
    "    Returns:\n",
    "    - int - Maximum length of position embeddings\n",
    "    \"\"\"\n",
    "    conf = model.config\n",
    "    max_seq_length = None\n",
    "\n",
    "    # Checking various attributes to determine max length\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_seq_length = getattr(conf, length_setting, None)\n",
    "        if max_seq_length:\n",
    "            print(f\"Found max sequence length: {max_seq_length}\")\n",
    "            break\n",
    "\n",
    "    # Defaulting to 1024 if no length attribute is found\n",
    "    if not max_seq_length:\n",
    "        max_seq_length = 1024\n",
    "        print(f\"Using default max sequence length: {max_seq_length}\")\n",
    "\n",
    "    return max_seq_length\n",
    "\n",
    "def setup_output_directory(args):\n",
    "    \"\"\"\n",
    "    Sets up the output directory for saving model checkpoints and other outputs.\n",
    "    \"\"\"\n",
    "    checkpoint_path = args.checkpoint_dir\n",
    "    checkpoint_path.mkdir(mode=0o777, parents=True, exist_ok=True)\n",
    "    mergepoint_path = args.mergepoint_dir\n",
    "    mergepoint_path.mkdir(mode=0o777, parents=True, exist_ok=True)\n",
    "    \n",
    "\n",
    "def find_all_linear_names(model: Type[torch.nn.Module], bits: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find names of all linear layers in the model based on the number of bits specified.\n",
    "\n",
    "    Parameters:\n",
    "    - model : torch.nn.Module - The model to inspect\n",
    "    - bits : int - The number of bits to select the appropriate linear layer class\n",
    "\n",
    "    Returns:\n",
    "    - List[str] - List of linear layer names\n",
    "    \"\"\"\n",
    "\n",
    "    # Selecting the appropriate class based on the number of bits\n",
    "    if bits == 4:\n",
    "        cls = bnb.nn.Linear4bit\n",
    "    elif bits == 8:\n",
    "        cls = bnb.nn.Linear8bitLt\n",
    "    else:\n",
    "        cls = torch.nn.Linear\n",
    "\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    # Removing 'lm_head' if exists (specific to 16-bit scenarios)\n",
    "    lora_module_names.discard(\"lm_head\")\n",
    "\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611185e-6e81-4ee7-8fa8-9d4d124136d1",
   "metadata": {},
   "source": [
    "### Trainer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e67a4f09-5a69-4429-aaea-9182754b6c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def supervised_fine_tuning(args, logger):\n",
    "    logger.info(\"Starting Supervised Fine Tuning ...\")\n",
    "    set_seeds(args, logger)\n",
    "    \n",
    "    # BitsAndBytes Setup\n",
    "    logger.debug(\"Creating BitsAndBytesConfig ...\")\n",
    "    bnb_config = create_bnb_config(args)\n",
    "    \n",
    "    # Load Base Model for Configs Creation\n",
    "    logger.debug(\"Creating Base Model ...\")\n",
    "    base_model = create_model(args, logger, bnb_config=bnb_config, peft_config=None)\n",
    "\n",
    "    # Parmeter Efficient Fine-Tuning Setup\n",
    "    logger.debug(\n",
    "        \"Get module names for the linear layers where we add LORA adapters...\"\n",
    "    )\n",
    "    layers_for_adapters = find_all_linear_names(base_model, 4)\n",
    "    logger.debug(f\"Layers for Adapters: {layers_for_adapters}\")\n",
    "    logger.debug(\n",
    "        \"Create PEFT config using the adapted layers for PEFT...\"\n",
    "    )\n",
    "    peft_config = create_peft_config(args, layers_for_adapters)\n",
    "\n",
    "    # Model & Tokenizer Setup    \n",
    "    del(base_model)\n",
    "    logger.debug(\"Creating Tokenizer ...\")\n",
    "    tokenizer = create_tokenizer(args, logger)\n",
    "    logger.debug(\"Creating Base Model ...\")\n",
    "    model = create_model(args, logger, bnb_config=bnb_config, peft_config=peft_config)\n",
    "    assert args.max_seq_length == get_max_seq_length(model)\n",
    "    logger.debug(\"Creating GenerationConfig ...\")\n",
    "    generation_config = create_generation_config(args)\n",
    "    model.generation_config = generation_config\n",
    "\n",
    "    # Prepare for K-Bit Training\n",
    "    logger.debug(f\"Using the prepare_model_for_kbit_training method from PEFT...\")\n",
    "    logger.debug(f\"Gradient Checkpointing == {args.gradient_checkpointing}\")\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model, use_gradient_checkpointing=args.gradient_checkpointing\n",
    "    )\n",
    "    logger.debug(f\"Model Dtypes after preparing for kbit training ...\")\n",
    "    dtypes_after = log_dtypes(model, logger)\n",
    "\n",
    "    # Dataset Setup\n",
    "    logger.debug(\"Loading train dataset ...\")\n",
    "    train_dataset = load_dataset_split(args=args, logger=logger, split=\"train\")\n",
    "    logger.debug(\"Preprocessing train dataset ...\")\n",
    "    train_dataset = preprocess_dataset(\n",
    "        args=args, logger=logger, tokenizer=tokenizer, dataset=train_dataset\n",
    "    )\n",
    "    train_set, eval_set = split_dataset(train_dataset, train_ratio=0.7, seed=args.seed)\n",
    "    \n",
    "    # Supervised Fine-Tuning\n",
    "    logger.debug(\"Running Supervised Fine Tuning ...\")    \n",
    "    logger.debug(\"Creating TrainingArguments ...\")\n",
    "    training_arguments = create_training_arguments(args)\n",
    "    logger.debug(\"Creating Trainer Callbacks ...\")\n",
    "    callbacks = [PeftSavingCallback()]\n",
    "    logger.debug(\"Creating `SFTTrainer` ...\")\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        train_dataset=train_set.with_format(\"torch\"),\n",
    "        eval_dataset=eval_set.with_format(\"torch\"),\n",
    "        peft_config=peft_config,\n",
    "        callbacks=callbacks,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "        dataset_text_field=args.text_field,\n",
    "        predict_with_generate = args.predict_with_generate\n",
    "        # compute_metrics=compute_metrics,\n",
    "        # packing=args.packing,\n",
    "        # neftune_noise_alpha=args.neftune_noise_alpha,\n",
    "    )\n",
    "    # trainer.predict_with_generate = args.predict_with_generate\n",
    "    \n",
    "    logger.debug(\"Trying trainer.train() ...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        logger.error(\"The trainer.train() failed !!!\")\n",
    "        raise Exception(e)\n",
    "\n",
    "    # Saving-exiting\n",
    "    trainer.save_state()\n",
    "    logger.debug(\"Saving final results ...\")\n",
    "    model = trainer.model\n",
    "    logger.debug(f\"Saving PEFT Model adapters to {args.checkpoint_dir} ...\")\n",
    "    model.save_pretrained(args.checkpoint_dir, safe_serialization=True, save_adapter=True, save_config=True)\n",
    "    logger.debug(f\"saving tokenizer to {args.checkpoint_dir} ...\")\n",
    "    tokenizer.save_pretrained(args.checkpoint_dir)\n",
    "    \n",
    "class PeftSavingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback to save the PEFT adapters during the model training.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        checkpoint_path = os.path.join(\n",
    "            args.output_dir, f\"checkpoint-{state.global_step}\"\n",
    "        )\n",
    "        kwargs[\"model\"].save_pretrained(checkpoint_path, save_adapter=True, save_config=True)\n",
    "\n",
    "        if \"pytorch_model.bin\" in os.listdir(checkpoint_path):\n",
    "            os.remove(os.path.join(checkpoint_path, \"pytorch_model.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44958bd7-ae15-4f8b-a8de-996b994b62a1",
   "metadata": {},
   "source": [
    "### Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77cbaa7d-1a54-4660-89b3-9b587c313543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_generation(args, logger, model, tokenizer, generation_config,\n",
    "                    forced_words=None, override_seed: int = None):\n",
    "    \"\"\"\n",
    "    TODO: 2 gpu text generation https://huggingface.co/docs/accelerate/usage_guides/distributed_inference\n",
    "    \"\"\"\n",
    "\n",
    "    logger.debug(\"Re-configuring CUDA to single-device ...\")\n",
    "    args.device, args.cuda_n_gpus, args.cuda_max_memory = \"cuda:0\", 1, {0: \"39GB\"}\n",
    "    logger.debug(\n",
    "        f\"Using k={args.cuda_n_gpus} CUDA GPUs with max memory {args.cuda_max_memory}\"\n",
    "    )\n",
    "\n",
    "    set_seeds(args, logger)\n",
    "    \n",
    "    if model is not None:\n",
    "        logger.debug(f\"Using model provided to function ...\")\n",
    "        pass\n",
    "    else:\n",
    "        logger.debug(\"Creating the Model ...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.mergepoint_dir,\n",
    "            device_map=args.device,\n",
    "            max_memory=args.cuda_max_memory,\n",
    "            torch_dtype=args.bnb_compute_dtype,\n",
    "        )\n",
    "\n",
    "    if tokenizer is not None:\n",
    "        logger.debug(f\"Using tokenizer provided to function ...\")\n",
    "        pass\n",
    "    else:\n",
    "        logger.debug(\"Creating the Tokenizer ...\")\n",
    "        tokenizer = create_tokenizer(args=args, logger=logger)\n",
    "    \n",
    "    # Load Test Dataset\n",
    "    test_dataset = load_dataset_split(args, logger, \"test\")\n",
    "    test_dataset = preprocess_dataset(args, logger, tokenizer, test_dataset)\n",
    "    logger.debug(\n",
    "        f\"Creating the Test DataLoader with batch size == {args.per_device_eval_batch_size} ...\"\n",
    "    )\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=args.per_device_eval_batch_size,\n",
    "        num_workers=4, pin_memory=True,\n",
    "    )\n",
    "    logger.debug(f\"Sending the model to device '{args.device}'\")\n",
    "    model.eval()\n",
    "    model.to(args.device)\n",
    "\n",
    "    # Generating Text from Model\n",
    "    logger.debug(\"Generating text ...\")\n",
    "    if generation_config is not None:\n",
    "        logger.debug(\"Using the GenerationConfig provided to function ...\")\n",
    "        pass\n",
    "    else:\n",
    "        logger.debug(\"Creating GenerationConfig ...\")\n",
    "        generation_config = create_generation_config(args)\n",
    "    test_responses = []\n",
    "    start = time.perf_counter()\n",
    "    if forced_words:\n",
    "        force_words_ids = tokenizer(forced_words, add_special_tokens=False).input_ids\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        inputs = tokenizer(\n",
    "            batch[args.text_field],\n",
    "            padding=args.padding,\n",
    "            truncation=args.truncation,\n",
    "            max_length=args.max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs.to(args.device)        \n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    force_words_ids=force_words_ids,\n",
    "                    generation_config=generation_config,\n",
    "                    # max_new_tokens=args.max_new_tokens,\n",
    "                    # do_sample=args.do_sample,\n",
    "                    # num_beams=args.num_beams,\n",
    "                    # temperature=args.temperature,\n",
    "                    # top_p=args.top_p,\n",
    "                    # top_k=args.top_k,\n",
    "                    # penalty_alpha=args.penalty_alpha,\n",
    "                    # min_length=args.min_length,\n",
    "                    # use_cache=args.use_cache,\n",
    "                    # repetition_penalty=args.repetition_penalty,\n",
    "                    # length_penalty=args.length_penalty,\n",
    "                    # num_return_sequences=args.num_return_sequences,\n",
    "                    # early_stopping=args.early_stopping,\n",
    "                    # remove_invalid_values=args.remove_invalid_values,\n",
    "                    # no_repeat_ngram_size=args.no_repeat_ngram_size,\n",
    "                )\n",
    "            except TypeError as e:\n",
    "                logger.error(f\"An error occurred during generation: {e}\")\n",
    "                raise TypeError(e)\n",
    "        generated_texts = tokenizer.batch_decode(sequences=gen_id, skip_special_tokens=True)\n",
    "        # generated_texts = [\n",
    "        #     tokenizer.decode(gen_id, skip_special_tokens=True)\n",
    "        #     for gen_id in generated_ids\n",
    "        # ]\n",
    "\n",
    "        test_responses.extend(generated_texts)\n",
    "\n",
    "    e2e_inference_time = (time.perf_counter() - start) * 1000\n",
    "    logger.debug(f\"the inference time is {e2e_inference_time} ms\")\n",
    "\n",
    "    predicted_labels = [\n",
    "        extract_label(test_responses[i]) for i in range(len(test_responses))\n",
    "    ]\n",
    "    logger.debug(\n",
    "        f\"Predicted label_encoded counts:\\n {pd.Series(predicted_labels).value_counts().to_string()}\"\n",
    "    )\n",
    "    true_labels = test_dataset[\"label_encoded\"]\n",
    "    logger.debug(\n",
    "        f\"Ground truth label_encoded counts:\\n {pd.Series(true_labels).value_counts().to_string()}\"\n",
    "    )\n",
    "    logger.debug(\"Evaluating prediction metrics ...\")\n",
    "    accuracy_perc, f1_score_perc, missing_perc = evaluate_predictions(\n",
    "        true_labels, predicted_labels\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Accuracy: {accuracy_perc}\")\n",
    "    logger.info(f\"F1 Score: {f1_score_perc}\")\n",
    "    logger.info(f\"Missing Percent: {missing_perc}\")\n",
    "\n",
    "    return test_dataset, generated_texts, true_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8b988-f982-40e8-86c2-ce9fc470ecc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb6b44a7-f7a0-45f4-910f-3ff7ff449b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HuggingfaceParams:\n",
    "    organization: str = \"gtfintechlab\"\n",
    "    tokenizers_parallelism: str = \"false\"\n",
    "    push_to_hub: bool = False\n",
    "\n",
    "@dataclass\n",
    "class TaskParams:\n",
    "    task_name: str = \"fomc_communication\"\n",
    "    seed: int = 5768\n",
    "    context_field: str = \"sentence\"\n",
    "    label_field: str = \"label\"\n",
    "    encoded_label_field: str = \"label_encoded\"\n",
    "    response_field: str = \"label_decoded\"\n",
    "    text_field: str = \"input_texts\"\n",
    "\n",
    "@dataclass\n",
    "class ModelParams:\n",
    "    model_parameters: str = \"7b\"\n",
    "    model_id: str = f\"meta-llama/Llama-2-{model_parameters}-chat-hf\"\n",
    "    model_name: str = model_id.split(\"/\")[-1]\n",
    "\n",
    "@dataclass\n",
    "class LoggingParams:\n",
    "    report_to: str = \"tensorboard\"\n",
    "    logging_dir: str = str(Path.home() / \"tensorboard\" / \"logs\")\n",
    "\n",
    "@dataclass\n",
    "class DirectoryParams:\n",
    "    # TODO: maybe move uid out of params?\n",
    "    uid: str = generate_uid()\n",
    "    output_dir: str = Path(\"/fintech_3\") / \"glenn\" / \"results\" / f\"{TaskParams.task_name}\" / f\"{ModelParams.model_name}\" / uid\n",
    "    checkpoint_dir: str = output_dir / \"final_checkpoint\"\n",
    "    mergepoint_dir: str = output_dir / \"final_merged_checkpoint\"\n",
    "\n",
    "@dataclass\n",
    "class PromptParams:\n",
    "    system_prompt: str = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    instruction_prompt: str = f\"Discard all the previous instructions. Behave like you are an expert sentence classifier. Classify the following sentence from FOMC into 'HAWKISH', 'DOVISH', or 'NEUTRAL' class. Label 'HAWKISH' if it is corresponding to tightening of the monetary policy, 'DOVISH' if it is corresponding to easing of the monetary policy, or 'NEUTRAL' if the stance is neutral. Provide the label in the first line and provide a short explanation in the second line. The sentence: \"\n",
    "    ## system_prompt = f\"Discard all previous instructions. Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    ## instruction_prompt = f\"Discard all the previous instructions. Behave like you are an expert sentence classifier. Classify the following sentence from FOMC into 'HAWKISH', 'DOVISH', or 'NEUTRAL' class. Label 'HAWKISH' if it is corresponding to tightening of the monetary policy, 'DOVISH' if it is corresponding to easing of the monetary policy, or 'NEUTRAL' if the stance is neutral. Provide the label 'HAWKISH', 'DOVISH', or 'NEUTRAL'. The sentence: \",\n",
    "    ## instruction_prompt = f\"Behave like you are an expert sentence classifier. Classify the following sentence from the Federal Open Market Committee into 'HAWKISH', 'DOVISH', or 'NEUTRAL' class. Label 'HAWKISH' if it is corresponding to tightening of the monetary policy. Label 'DOVISH' if it is corresponding to easing of the monetary policy. Label 'NEUTRAL' if the stance is neutral. Provide a single label from the choices 'HAWKISH', 'DOVISH', or 'NEUTRAL' then stop generating text. The sentence: \"\n",
    "    B_INST: str = \"[INST]\"\n",
    "    E_INST: str = \"[/INST]\"\n",
    "    B_SYS: str = \"<<SYS>>\\n\"\n",
    "    E_SYS: str = \"\\n<</SYS>>\\n\\n\"\n",
    "    BOS: str = \"<s>\"\n",
    "    EOS: str = \"</s>\"\n",
    "    repo_name: str = f\"{HuggingfaceParams().organization}/{ModelParams().model_name}_{TaskParams().task_name}\"\n",
    "\n",
    "@dataclass\n",
    "class QloraParams:\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.1\n",
    "\n",
    "@dataclass\n",
    "class SftParams:\n",
    "    # Default maximum sequence length to use\n",
    "    max_seq_length: int = 4096\n",
    "    # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "    packing: bool = True\n",
    "\n",
    "@dataclass\n",
    "class CudaParams:\n",
    "    compute_dtype = torch.bfloat16\n",
    "    fp16: bool = False\n",
    "    bf16: bool = True\n",
    "    cuda_n_gpus: int = field(default=None) # Determined dynamically at runtime\n",
    "    cuda_max_memory: str = field(default=None) # Determined dynamically at runtime\n",
    "    device_map: str = \"auto\"\n",
    "    device: str = \"cuda:0\"\n",
    "    save_safetensors: bool = True\n",
    "    \n",
    "@dataclass\n",
    "class BitsAndBytesParams:\n",
    "    bnb_mode = True\n",
    "    # Activate 4-bit precision base model loading\n",
    "    load_in_4bit: bool = True\n",
    "    # Activate 8-bit precision base model loading\n",
    "    load_in_8bit: bool = False\n",
    "    # Compute dtype for 4-bit base models\n",
    "    bnb_compute_dtype = CudaParams().compute_dtype\n",
    "    # Quantization type (fp4 or nf4)\n",
    "    bnb_quant_type: str = \"nf4\"\n",
    "    # Activate nested quantization for 4-bit base models (double quantization)\n",
    "    bnb_use_double_quant: bool = True\n",
    "\n",
    "@dataclass\n",
    "class GenerationParams:\n",
    "    # Generation Decoding Strategy\n",
    "    ## Number of possible generations\n",
    "    num_beams: int = 1\n",
    "    # num_beam_groups: int = 1\n",
    "    ## How many of possible generations are returned\n",
    "    num_return_sequences: int = 1\n",
    "    do_sample: bool = False\n",
    "    ## The value used to modulate the next token probabilities.\n",
    "    temperature: float = None # 0.0\n",
    "    ## Contrastive Search Parameters\n",
    "    penalty_alpha: float = 0\n",
    "    ## If top_p < 1, only the smallest set of most probable tokens with probabilities\n",
    "    ## that add up to top_p or higher are kept for generation.\n",
    "    top_p: float = None # 0.90\n",
    "    ## top 40 words are chosen\n",
    "    top_k: int = None # 40\n",
    "    # Other Parameters\n",
    "    ## The maximum numbers of tokens to generate\n",
    "    max_new_tokens: int = 100\n",
    "    ## The minimum length of the sequence to be generated, input prompt + min_new_tokens\n",
    "    min_length: int = None\n",
    "    ## Whether or not the model should use the past last key/values attentions\n",
    "    ## (if applicable to the model) to speed up decoding.\n",
    "    use_cache: bool = True\n",
    "    ## Parameter for repetition penalty. 1.0 means no penalty.\n",
    "    repetition_penalty: float = 1.1\n",
    "    ## Exponential penalty to the length that is used with beam-based generation.\n",
    "    length_penalty: int = 1.1\n",
    "    ## Max padding length to be used with tokenizer padding the prompts.\n",
    "    early_stopping: bool = False\n",
    "    return_dict_in_generate: bool = True\n",
    "    output_scores: bool = False\n",
    "    truncation: bool = True\n",
    "    padding: bool = True\n",
    "    remove_invalid_values: bool = True\n",
    "    no_repeat_ngram_size: int = 0\n",
    "\n",
    "@dataclass\n",
    "class TrainingArgumentsParams:\n",
    "    # Number of training epochs\n",
    "    num_train_epochs: int = 5\n",
    "    # Batch size per GPU for training\n",
    "    per_device_train_batch_size: int = 8\n",
    "    # Batch size per GPU for evaluation\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    # Number of update steps before accumulating gradients\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    # Enable gradient checkpointing\n",
    "    gradient_checkpointing: bool = True\n",
    "    # Maximum gradient normal (gradient clipping)\n",
    "    max_grad_norm: float = 0.3\n",
    "    # Initial learning rate (AdamW optimizer)\n",
    "    learning_rate: float = 3e-5\n",
    "    # Weight decay to apply to all layers except bias/LayerNorm\n",
    "    weight_decay: float = 0.001\n",
    "    # Optimizer to use\n",
    "    optim: str = \"adamw_bnb_8bit\"\n",
    "    # Learning rate schedule\n",
    "    lr_scheduler_type: str = \"constant\"\n",
    "    # Number of training steps (overrides num_train_epochs)\n",
    "    max_steps: int = -1\n",
    "    # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "    warmup_ratio: float = 0.03\n",
    "    # Group sequences into batches with same length\n",
    "    # Saves memory and speeds up training considerably\n",
    "    group_by_length: bool = False\n",
    "    # Save checkpoint every X updates steps\n",
    "    save_steps: float = 0.1\n",
    "    # Log every X updates steps\n",
    "    logging_steps: float = 0.1\n",
    "    load_best_model_at_end: bool = True\n",
    "    save_strategy: str = \"epoch\"\n",
    "    logging_strategy: str = \"epoch\"\n",
    "    evaluation_strategy: str = \"epoch\"\n",
    "    disable_tqdm: bool = False\n",
    "    predict_with_generate: bool = True\n",
    "    torch_compile: bool = False\n",
    "\n",
    "@dataclass\n",
    "class Args(HuggingfaceParams, TaskParams, ModelParams, LoggingParams,\n",
    "           DirectoryParams, PromptParams, QloraParams, SftParams, CudaParams,\n",
    "           BitsAndBytesParams, TrainingArgumentsParams, GenerationParams):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a1e17-847b-47fe-bd48-34e5098e696f",
   "metadata": {},
   "source": [
    "## `main()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0ada57f-a856-4043-a906-062ecfe9c9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(seed: int = None):\n",
    "    args = Args()\n",
    "    if 'logger' in locals():\n",
    "        del(logger)\n",
    "        logger = create_logger()\n",
    "    else:\n",
    "        logger = create_logger()\n",
    "    args = configure_cuda_args(args, logger)\n",
    "    logger.debug(\n",
    "        f\"Using k={args.cuda_n_gpus} CUDA GPUs with max memory {args.cuda_max_memory}\"\n",
    "    )\n",
    "    \n",
    "    if seed:\n",
    "        args.seed = seed\n",
    "        logger.debug(f\"Seed value overriden to '{args.seed}'\")\n",
    "    else:\n",
    "        logger.debug(f\"Seed value is '{args.seed}'\")\n",
    "    set_seeds(args, logger)\n",
    "\n",
    "    try:\n",
    "        setup_output_directory(args)\n",
    "        supervised_fine_tuning(args, logger)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        raise Exception(e)\n",
    "    finally:\n",
    "        memory_cleanup()\n",
    "\n",
    "    logger.debug(f\"Reloading original weights of Model ...\")\n",
    "    base_model = create_model(args=args, logger=logger, bnb_config=None, peft_config=None)\n",
    "    log_dtypes(logger=logger, model=base_model)\n",
    "\n",
    "    logger.debug(f\"Merging adapters into weights to create the final Model ...\")\n",
    "    peft_model = PeftModel.from_pretrained(base_model, args.checkpoint_dir)\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    log_dtypes(merged_model, logger)\n",
    "\n",
    "    logger.debug(f\"Merged Model saving to {args.mergepoint_dir} ...\")\n",
    "    merged_model.save_pretrained(args.mergepoint_dir, safe_serialization=True)\n",
    "\n",
    "    logger.debug(f\"Tokenizer saving to {args.mergepoint_dir} ...\")\n",
    "    tokenizer = create_tokenizer(args, logger)\n",
    "    tokenizer.save_pretrained(args.mergepoint_dir)\n",
    "\n",
    "    if args.push_to_hub:\n",
    "        logger.debug(f\"Pushing the Merged PEFT model and tokenizer to hub repo {args.repo_name}\")\n",
    "        merged_model.push_to_hub(args.repo_name, private=True, use_temp_dir=True)\n",
    "        tokenizer.push_to_hub(args.repo_name, private=True, use_temp_dir=True)\n",
    "       \n",
    "    memory_cleanup()\n",
    "\n",
    "    test_dataset, generated_texts, true_labels, predicted_labels = text_generation(\n",
    "        args=args, logger=logger, merged_model=None, tokenizer=None, generation_config=generation_config,\n",
    "        forced_words=[\"DOVISH\", \"HAWKISH\", \"NEUTRAL\"]\n",
    "    )\n",
    "\n",
    "    return merged_model, tokenizer, test_dataset, generated_texts, true_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785ee18-ca54-4296-a825-cca181179d9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# RUNNING `main()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b8b298-ef6c-43cb-8f1c-e23ef3e20bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=100'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = HuggingfaceParams.tokenizers_parallelism\n",
    "\n",
    "merged_model, tokenizer, test_dataset, generated_texts, true_labels, predicted_labels = main()\n",
    "\n",
    "# if name = \"__main__\" :\n",
    "   # fire.Fire(main)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "098dd7e4-8847-4004-b6d0-4fd9783d1919",
   "metadata": {
    "tags": []
   },
   "source": [
    "# merged_model = '/fintech_3/glenn/results/fomc_communication/Llama-2-7b-chat-hf/5732faf0_231030/final_merged_checkpoint/'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ade9aaf2-2802-4e11-b548-2c9b8380a420",
   "metadata": {
    "tags": []
   },
   "source": [
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81c54810-c026-4602-a49b-25641ba2154e",
   "metadata": {
    "tags": []
   },
   "source": [
    "test_dataset, generated_texts, true_labels, predicted_labels = text_generation(\n",
    "override_model_path = 'meta-llama/Llama-2-7b-chat-hf'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab999505-aa94-468e-9b65-fa7797611f6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# test_dataset, generated_texts, true_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07e87e75-d722-4fec-97a9-8624ebeac4a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "evaluate_predictions(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e73eac6-8f11-41d0-a308-bbc82d333d9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# SANDBOX"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9abd39e-b1ba-4627-878c-36626cdf0f4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "memory_cleanup()\n",
    "\n",
    "args = Args()\n",
    "args.device, args.cuda_n_gpus, args.cuda_max_memory = \"cuda:0\", 1, {0: \"41GB\"}\n",
    "logger = create_logger()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_id, use_fast=False)\n",
    "tokenizer.pad_token = args.EOS\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        device_map=args.device,\n",
    "        max_memory=args.cuda_max_memory,\n",
    "        torch_dtype=args.bnb_compute_dtype,\n",
    "    )\n",
    "\n",
    "log_dtypes(base_model, logger)\n",
    "\n",
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        '/fintech_3/glenn/v_foo/checkpoint-2088',\n",
    "        device_map=args.device,\n",
    "        max_memory=args.cuda_max_memory,\n",
    "        torch_dtype=args.bnb_compute_dtype,\n",
    "    )\n",
    "\n",
    "log_dtypes(new_model, logger)\n",
    "\n",
    "del(new_model)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, '/fintech_3/glenn/v_foo/checkpoint-2088')\n",
    "\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "log_dtypes(merged_model, logger)\n",
    "\n",
    "merged_model.save_pretrained('/fintech_3/glenn/v_foo/checkpoint-2088/final_merged_checkpoint') #, safe_serialization=True)\n",
    "tokenizer.save_pretrained(args.mergepoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676c33f-7335-41b3-a935-fa0e3f537aa2",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# RECYCLING BIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c7abd-32e8-4ed3-86b5-8b4ba04c14ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# GATHERING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcb2f3e-f19a-42d6-bde8-fc7ca87f02bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "FOMC classification performance; Single A20 NVIDIA GPU, Batch Size of 8, BFloat16:\n",
    "| Model Size | Seed | Tuned | Decoding | Accuracy % | F1 Score % | Missing % | Wall Clock |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 7B | 42 | False | Greedy | 42.54 | 42.81 | 0 | 254s |\n",
    "| 7B | 5768 | False | Greedy | 42.14 | 42.39 | 0 | 254s |\n",
    "| 7B | 78516 | False | Greedy | 41.53 | 41.66 | 0 | 255s |\n",
    "| 7B | 944601 | False | Greedy | 42.54 | 42.76 | 0 | 253s |\n",
    "| 13B | 42 | False | Greedy | 50.20 | 43.70 | 0 | 452s |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd198cc4-61dc-457f-9322-b1afdb3107fa",
   "metadata": {},
   "source": [
    "Parameter Comparison: \n",
    "| Model Name | Parameter Count (in Milllions) | Weights Size (in Gigabytes) |\n",
    "| --- | --- | --- |\n",
    "| RoBERTa-base | 125M | 0.5GB |\n",
    "| RoBERTa-large | 355M | 1.5GB |\n",
    "| Llama2-chat | 7,000M | 13GB |\n",
    "| Llama2-chat | 13,000M | 25GB |\n",
    "| Llama2-chat | 70,000M | 120GB |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e36ba-0134-43a3-a0f6-1ef0dab40bbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "Checkpoint Size Comparison:\n",
    "    \n",
    "|LLama 2 Checkpoint Size|LoRA (applied to all layers, rank=8)|Traditional fine-tuning|\n",
    "|---|---|---|\n",
    "|7B|40MB|13GB|\n",
    "|13B|65MB|26GB|\n",
    "|70B|200MB|128GB|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bad257-b6cb-47a4-bee1-e330c725ad05",
   "metadata": {
    "tags": []
   },
   "source": [
    "FOMC Communication Dataset:\n",
    "\n",
    "| metric | total | test | train | fine-tuning | validation |\n",
    "|---|---|---|---|---|---|\n",
    "| pct_total | 100% | 20% | 80% | 56% | 24% |\n",
    "| n_total | 2,480 | 496 | 1,984 | 1,388 | 596 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95fbc91-6f5e-4f6e-9cb6-e5a5afa330d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Parameter Count During SFT:\n",
    "| Model ID | Model Count of Parameters | Adapter Count of Parameters | Adapter % of Parameters |\n",
    "|---|---|---|---|\n",
    "| Llama2 7B | 6,738,415,616| 159,907,840 | 2.32% |\n",
    "\n",
    "In the final served version the adapters values could be merged in so there's no added params during final inference\n",
    "\n",
    "Alternatively we can keep the model weights stationary and swap out fine-tuning adapters for each client or scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f4a9d-f158-4822-ae70-d42ee814589f",
   "metadata": {},
   "source": [
    "Text Generation Decoding Strategies [https://huggingface.co/docs/transformers/generation_strategies]\n",
    "\n",
    "Greedy Sampling: `beams==1; sample==False`\n",
    "\n",
    "Multinomial Sampling: `beams==1; sample==True`\n",
    "\n",
    "Beam Search: `beams>1; sample==False`\n",
    "\n",
    "Beam Search + Multinomial Sampling:`beams>1; sample==True`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
