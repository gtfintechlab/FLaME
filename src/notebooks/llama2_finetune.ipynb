{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf37c31a-e659-477b-a0bc-fec3df7b1b6b",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning of Llama2 on Financial Phrasebank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1c34b-f17a-40f1-9dc4-ab7f692ef04e",
   "metadata": {},
   "source": [
    "!python -m ipykernel install --user --name=conference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79f553-61c2-46e9-90fe-306180ac37be",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d3ba20-0627-4f51-9c91-4df4c8783991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SRC_DIRECTORY = Path().cwd().resolve().parent\n",
    "\n",
    "if str(SRC_DIRECTORY) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIRECTORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03982813-c016-4780-8b67-3931384881d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('llama2_finetune')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "consoleHandler.setFormatter(formatter)\n",
    "logger.addHandler(consoleHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb97797-1ecc-498b-ae91-75692518831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4413c42-65b9-404c-9ca7-02e39d1238bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaModel, LlamaConfig, TextGenerationPipeline\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a32f77-63d4-4bcf-b19b-85cc5110ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d880d156-7894-483f-98e9-c6e74ac8d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab82d5d-2a20-4dd5-aaa6-a4bf5d9f000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7668a30-2546-48b0-b2bd-9110c4810db9",
   "metadata": {},
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbcfe766-8f48-499b-aaf4-f0de0af4c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec31578c-71a8-4616-8a8d-98d5eca8bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ce6ed-6ca7-45dd-ac24-fc4589898a75",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9140a49-bf41-499d-9d51-2f4fafb86d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Args = namedtuple(\"Args\", [\"task_name\"])\n",
    "args = Args(task_name=\"sentiment_analysis\")\n",
    "\n",
    "# TODO: move more configs into my args?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27bdbc-4654-4e63-9eee-44fa90845da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate a date string\n",
    "date_str = datetime.now().strftime(\"%y%m%d\")\n",
    "\n",
    "# Generate a short UUID\n",
    "uid = str(uuid.uuid4())[:8]\n",
    "\n",
    "# Combine\n",
    "uid = f\"{uid}_{date_str}\"\n",
    "\n",
    "print(uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235715a-7bee-4cfc-a520-9b28121777e3",
   "metadata": {},
   "source": [
    "### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb44486-b7d1-4fb6-acc5-63a51ff2a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_auth = \"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\" \n",
    "# TODO REMOVE MY TOKEN FOR THE FINAL VERSION\n",
    "# TODO PROVIDE DIRECTIONS FOR HOW TO GET HF TOKEN HERE\n",
    "\n",
    "huggingface_hub.login(token=hf_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41242074-0980-463d-90e1-47c1672e5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "organization = \"gtfintechlab\"\n",
    "dataset = \"financial_phrasebank\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d865ad-6288-47a1-bf41-6091573f09d4",
   "metadata": {},
   "source": [
    "### Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5dfb4-c2c5-4dae-a889-05c175972840",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path.home() / f\"{dataset}_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d65aff-161f-4eb2-9744-1dfaa62410f9",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a531e-0b51-472a-baaa-1c8c7c4cb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"llama2-fpb-sft\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"false\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd23b2-a1e6-4981-9d28-0865462ca4ce",
   "metadata": {},
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd3968-b578-4fbb-a9d4-3d9b407fa0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "major, _ = torch.cuda.get_device_capability()\n",
    "if major >= 8:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.bfloat16\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU does not support bfloat16: using fp=16\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ef82bf-f204-48c3-8431-d6f66c6b5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_N_GPUS = torch.cuda.device_count()\n",
    "CUDA_MAX_MEMORY = f\"{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB\"\n",
    "CUDA_MAX_MEMORY = {i: CUDA_MAX_MEMORY for i in range(CUDA_N_GPUS)}\n",
    "logger.info(\n",
    "    f\"Using k={CUDA_N_GPUS} CUDA GPUs with max memory {CUDA_MAX_MEMORY}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e95cb5-2bd6-491c-a2a3-f9df4f037e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map = \"auto\" #{\"\":0},"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d02251-0dd6-477f-97cd-81123caf08d3",
   "metadata": {},
   "source": [
    "## Financial PhraseBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f0a4e-98d1-4386-9136-1fdfc4738469",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS = (5768, 78516, 944601)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fb611-9c02-4e8b-a88e-7708987c2037",
   "metadata": {},
   "source": [
    "### Create splits on HuggingFace Hub"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7083ef7-ce89-4a35-aef2-7d0e6ff09d02",
   "metadata": {},
   "source": [
    "def make_fpb_hub_datasets(SEEDS):\n",
    "    configs = [\n",
    "        \"sentences_allagree\",\n",
    "        \"sentences_75agree\",\n",
    "        \"sentences_66agree\",\n",
    "        \"sentences_50agree\",\n",
    "    ]\n",
    "\n",
    "    for config in tqdm(configs, desc=\"Configs\"):\n",
    "        try:\n",
    "            fpb_dataset = load_dataset(\"financial_phrasebank\", config)\n",
    "            config_short = config.replace(\"sentences_\", \"\")\n",
    "\n",
    "            texts = fpb_dataset[\"train\"][\"sentence\"]\n",
    "            labels = fpb_dataset[\"train\"][\"label\"]\n",
    "\n",
    "            splits = {}\n",
    "\n",
    "            for seed in tqdm(SEEDS, desc=\"Seeds\", leave=False):\n",
    "                # Splitting the data\n",
    "                train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "                    texts, labels, test_size=0.2, random_state=seed\n",
    "                )\n",
    "\n",
    "                # Storing in the dictionary\n",
    "                splits[seed] = {\n",
    "                    \"train\": Dataset.from_dict({\"text\": train_texts, \"label\": train_labels}),\n",
    "                    \"test\": Dataset.from_dict({\"text\": test_texts, \"label\": test_labels}),\n",
    "                }\n",
    "\n",
    "                # Push to HF Hub\n",
    "                splits[seed][\"train\"].push_to_hub(\n",
    "                    f\"{organization}/{dataset}-{config_short}-{seed}\",\n",
    "                    config_name=\"train\",\n",
    "                    private=True,\n",
    "                )\n",
    "                splits[seed][\"test\"].push_to_hub(\n",
    "                    f\"{organization}/{dataset}-{config_short}-{seed}\",\n",
    "                    config_name=\"test\",\n",
    "                    private=True,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing config {config}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4ae00c5-8b5a-4733-be2b-4fd3ffc007e2",
   "metadata": {},
   "source": [
    "# Execute the function\n",
    "make_fpb_hub_datasets(SEEDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da785f-67d3-45a0-8b9e-ea4a103769c6",
   "metadata": {},
   "source": [
    "### Load split dataset from HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd6573-f4e7-4643-8cb4-49f7e084b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = \"allagree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3402f-5129-4f6c-a041-35b49d8ce899",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpb_train_dataset = load_dataset(f\"{organization}/{dataset}-{CONFIG}-{SEEDS[0]}\", \"train\")['train']\n",
    "fpb_test_dataset = load_dataset(f\"{organization}/{dataset}-{CONFIG}-{SEEDS[0]}\", \"test\")['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2e938-6266-4435-8eac-0db4b70ba322",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07169f75-8535-40a5-bdc3-c1e7a7cabd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.instructions import TASK_MAP, llama2_prompt_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b5472-3fea-47fe-b4a0-36995e801fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(label_number):\n",
    "    if label_number == 0:\n",
    "        return \"positive\"\n",
    "    elif label_number == 1:\n",
    "        return \"negative\"\n",
    "    elif label_number == 2:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label number\")\n",
    "\n",
    "TASK_INSTRUCTION, TASK_DATA = (\n",
    "    TASK_MAP[args.task_name][\"instruction\"],\n",
    "    TASK_MAP[args.task_name][\"data\"],\n",
    ")\n",
    "\n",
    "def convert_dataset(ds):\n",
    "    prompts = llama2_prompt_generator(TASK_INSTRUCTION, ds['text'])\n",
    "    labels = [decode(L).upper() for L in ds['label']]\n",
    "    df = pd.DataFrame.from_dict({'prompt': prompts, 'response': labels})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c362e-d42a-4367-be50-267dcfd0aa83",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a2866c-c5fe-4403-bddc-da2d81b64f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name = model_id.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4599d-dff1-442c-bf9f-dccedfe32ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    # Activate 4-bit precision base model loading\n",
    "    load_in_4bit=True,\n",
    "    # Activate nested quantization for 4-bit base models (double quantization)\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # Compute dtype for 4-bit base models\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd1ea6-b477-4f94-befe-40879d77b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7f0a1e-d046-401e-8c76-9150356e50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = LlamaConfig.from_pretrained(\n",
    "                            model_id,\n",
    "                            bos_token_id = 1,\n",
    "                            eos_token_id = 2,\n",
    "                            hidden_act = \"silu\",\n",
    "                            hidden_size = 8192,\n",
    "                            initializer_range = 0.02,\n",
    "                            intermediate_size = 28672,\n",
    "                            max_position_embeddings = 4096,\n",
    "                            model_type = \"llama\",\n",
    "                            num_attention_heads = 64,\n",
    "                            num_hidden_layers = 80,\n",
    "                            num_key_value_heads = 8,\n",
    "                            pretraining_tp = 1,\n",
    "                            rms_norm_eps = 1e-05,\n",
    "                            rope_scaling = None,\n",
    "                            tie_word_embeddings = False,\n",
    "                            # torch_dtype = \"float16\",\n",
    "                            # transformers_version = \"4.32.0.dev0\",\n",
    "                            use_cache = False, # TODO: double check use cache\n",
    "                            vocab_size = 32000\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30222269-b7cf-48f0-9b26-adcc3438a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55b4f2-078d-44a7-a081-1cbad1cad35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                  load_in_4bit=True,\n",
    "                                              device_map=device_map,\n",
    "                                                  max_memory=CUDA_MAX_MEMORY,\n",
    "                                                  torch_dtype=compute_dtype,\n",
    "                                              # use_auth_token=True,\n",
    "                                              quantization_config=bnb_config\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408be9f-63a6-4a67-afd3-4e5b552d7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    # offload_state_dict=True,\n",
    "    # offload_folder=\"offload\",\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f9bf3-eb2a-4288-bd2f-848785dfe24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e4663-5da9-4336-b9f0-6d13e7b2ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501b636-e562-46d0-94b6-f3c39846eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    # Alpha parameter for LoRA scaling\n",
    "    lora_alpha=16,\n",
    "    # Dropout probability for LoRA layers\n",
    "    lora_dropout=0.1,\n",
    "    # LoRA attention dimension\n",
    "    r=64,\n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb9169c5-1aad-410f-a8a9-d6a8cb1c3e3d",
   "metadata": {},
   "source": [
    "output_dir = Path.cwd() / \"llama2-fpb-sft-results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d212367-1ce5-4d7c-9a7f-8a077e263f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    report_to=\"wandb\",\n",
    "    # Batch size per GPU for training\n",
    "    per_device_train_batch_size=4,\n",
    "    # Batch size per GPU for evaluation\n",
    "    per_device_eval_batch_size=4,\n",
    "    # Number of update steps to accumulate the gradients for\n",
    "    gradient_accumulation_steps=4,\n",
    "    # Enable gradient checkpointing\n",
    "    gradient_checkpointing = True,\n",
    "    # Maximum gradient normal (gradient clipping)\n",
    "    max_grad_norm = 0.3,\n",
    "    # Initial learning rate (AdamW optimizer)\n",
    "    learning_rate = 2e-4,\n",
    "    # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "    weight_decay = 0.001,\n",
    "    # Optimizer to use\n",
    "    optim = \"paged_adamw_32bit\",    \n",
    "    # Learning rate schedule (constant a bit better than cosine)\n",
    "    lr_scheduler_type = \"constant\",\n",
    "    # Number of training steps (overrides num_train_epochs)\n",
    "    max_steps = -1,\n",
    "    # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "    warmup_ratio = 0.03,\n",
    "    # Group sequences into batches with same length to save memory and speed up training\n",
    "    group_by_length = True,\n",
    "    # Save checkpoint every X updates steps\n",
    "    save_steps = 25,\n",
    "    # Log every X updates steps\n",
    "    logging_steps = 25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7ec89-151f-4b10-8b10-544984a0c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=fpb_train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    # Maximum sequence length to use\n",
    "    max_seq_length = None,\n",
    "    # max_seq_length = 512,\n",
    "    # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "    packing = False,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d404e650-cb01-4244-b634-9717b61d0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c263e-e221-4155-a25d-71b9a6d7bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir / \"final_checkpoint\")\n",
    "trainer.model.config.save_pretrained(output_dir / \"final_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2a028-a3d9-48ed-8d32-18add30e0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = f\"{organization}/{model_name}_{dataset}_{CONFIG}_{SEEDS[0]}\"\n",
    "print(repo_name)\n",
    "trainer.model.push_to_hub(repo_name, private=True, use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc76d0-8b92-49c7-80e8-77415587ffaf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7000d-29a5-4706-8665-8a75199021c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = AutoPeftModelForCausalLM.from_pretrained(output_dir/\"final_checkpoint\",\n",
    "                                             device_map=device_map, max_memory=CUDA_MAX_MEMORY,\n",
    "                                             torch_dtype=compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac607d2-157f-4ea3-85c6-5f155c9f4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = convert_dataset(fpb_test_dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc5417f4-4afa-4621-ad6d-768a475958ab",
   "metadata": {},
   "source": [
    "# get pipeline ready for instruction text generation\n",
    "generation_pipeline = TextGenerationPipeline(model=sft_model,\n",
    "                                             tokenizer=tokenizer,\n",
    "                                             # NOTE: Set `do_sample = True` when `temperature > 0.0`\n",
    "                                             # https://github.com/huggingface/transformers/issues/25326\n",
    "                                             temperature=0.0,  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "                                             do_sample=False,\n",
    "                                             max_new_tokens=512,\n",
    "                                             top_k=10,\n",
    "                                             top_p=0.92,\n",
    "                                             # Penalize the model for repeating text; 1.0 means no penalty\n",
    "                                             repetition_penalty=1.0,\n",
    "                                             # Only generate and return one response (sequence)\n",
    "                                             num_return_sequences=1\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c8eaa-c3d3-433c-a4b6-46983377503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0abec3-cfa3-4731-b1d2-a3e3c1eb729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature=0.0  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "do_sample=False\n",
    "max_new_tokens=256\n",
    "top_k=10\n",
    "top_p=0.92\n",
    "repetition_penalty=1.0  # 1.0 means no penalty\n",
    "num_return_sequences=1  # Only generate one response\n",
    "num_beams=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddc6df-f72d-4db5-9f0c-da038c87c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model=None, tokenizer=None, input=None):\n",
    "    inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(temperature=temperature,\n",
    "                                           do_sample=do_sample,\n",
    "                                           max_new_tokens=max_new_tokens,\n",
    "                                           top_k=top_k,\n",
    "                                           top_p=top_p,\n",
    "                                           repetition_penalty=repetition_penalty,\n",
    "                                           num_return_sequences=num_return_sequences,\n",
    "                                           num_beams=num_beams,\n",
    "                                           return_dict_in_generate=True,\n",
    "                                           output_scores=False,\n",
    "                                          ))\n",
    "    seq = generation_output.sequences\n",
    "    output = tokenizer.decode(seq[0])\n",
    "    return output.split('[/INST]')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbbf35-896a-43c1-8d59-eccf4c00531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_list = df_test['prompt'].to_list()\n",
    "output_list = []\n",
    "for i in range(len(input_list)):\n",
    "    output_list.append(\n",
    "        generate(model=sft_model, tokenizer=tokenizer, input=inputs_list[i]).replace('</s>','')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985526f0-74d0-4597-ad70-1fb710a88ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conference Environment",
   "language": "python",
   "name": "conference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
