{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf37c31a-e659-477b-a0bc-fec3df7b1b6b",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning of Llama2 on FOMC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "id": "5f24ab92-aae1-4082-85b4-305ee4e3badd",
   "metadata": {},
   "source": [
    "!pip install cupy-cuda11x\n",
    "\n",
    "!pip install cuda-python\n",
    "\n",
    "conda install -c conda-forge cudatoolkit-dev\n",
    "\n",
    "!pip3 install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "!pip install trl\n",
    "\n",
    "!pip install transformers datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33951517-366d-4863-a9fc-439de8145b83",
   "metadata": {},
   "source": [
    "## Not for Public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb44486-b7d1-4fb6-acc5-63a51ff2a9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/AD/gmatlin3/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "HF_AUTH = \"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\"\n",
    "huggingface_hub.login(token=HF_AUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d12fda-d164-4699-afc9-f1b1dd90639c",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c428c4bf-1348-496d-bd72-b8c03612a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import wandb\n",
    "\n",
    "# # set the wandb project where this run will be logged\n",
    "# os.environ[\"WANDB_PROJECT\"] = f\"llama2_sft_fomc\"\n",
    "# # save your trained model checkpoint to wandb\n",
    "# os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
    "# # turn off watch to log faster\n",
    "# os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"fa69ffc6a97578da0410b553042cbb8b3bf5fcaf\"\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = f\"llama2_sft\"\n",
    "\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2424d-31dc-4d8e-a71f-8087ec428ee6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07d79c3-9fa8-4cd2-8089-431db6e5c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79f553-61c2-46e9-90fe-306180ac37be",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd658e99-4cc9-41f6-b592-d436ab94c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "SRC_DIRECTORY = Path().cwd().resolve().parent\n",
    "\n",
    "if str(SRC_DIRECTORY) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIRECTORY))\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"llama2_finetune\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "consoleHandler.setFormatter(formatter)\n",
    "logger.addHandler(consoleHandler)\n",
    "\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import uuid\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from peft import (\n",
    "    AutoPeftModelForCausalLM,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LlamaConfig,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaModel,\n",
    "    LlamaTokenizer,\n",
    "    TextGenerationPipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from llama.instructions import (\n",
    "    B_INST,\n",
    "    B_SYS,\n",
    "    E_INST,\n",
    "    E_SYS,\n",
    "    TASK_MAP,\n",
    "    llama2_prompt_generator,\n",
    ")\n",
    "\n",
    "# from trl import SFTTrainer\n",
    "\n",
    "\n",
    "BOS, EOS = \"<s>\", \"</s>\"\n",
    "\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8483511b-f826-4016-90d1-6d424c99cd6a",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86ee5afe-e618-44b5-972b-55ef4cb0d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uid(id_length=8, dt_format=\"%y%m%d\"):\n",
    "    date_str = datetime.now().strftime(dt_format)\n",
    "\n",
    "    # Generate a short UUID\n",
    "    uid = str(uuid.uuid4())[:id_length]\n",
    "\n",
    "    # Combine\n",
    "    uid = f\"{uid}_{date_str}\"\n",
    "\n",
    "    return uid\n",
    "\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    # SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    cls = (\n",
    "        bnb.nn.Linear4bit\n",
    "    )  # if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def print_dtypes(model):\n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes:\n",
    "            dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items():\n",
    "        total += v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v / total)\n",
    "\n",
    "\n",
    "def create_peft_config(modules):\n",
    "    peft_config = LoraConfig(\n",
    "        # Pass our list as an argument to the PEFT config for your model\n",
    "        target_modules=modules,\n",
    "        # Dimension of the LoRA matrices we update in adapaters\n",
    "        r=lora_r,\n",
    "        # Alpha parameter for LoRA scaling\n",
    "        lora_alpha=lora_alpha,\n",
    "        # Dropout probability for LoRA layers\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    return peft_config\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "# def convert_dataset(ds):\n",
    "#     prompts = llama2_prompt_generator(TASK_INSTRUCTION, ds['text'])\n",
    "#     labels = [decode(L).upper() for L in ds['label']]\n",
    "#     df = pd.DataFrame.from_dict({'prompt': prompts, 'response': labels})\n",
    "#     return df\n",
    "\n",
    "\n",
    "def create_prompt_formats(\n",
    "    sample, context_field=\"sentence\", response_field=\"label_decoded\"\n",
    "):\n",
    "    SYS_PROMPT = f\"\"\"\"Discard all the previous instructions.\n",
    "    Below is an instruction that describes a task.\n",
    "    Write a response that appropriately completes the request.\"\"\"\n",
    "\n",
    "    INST_PROMPT = TASK_INSTRUCTION\n",
    "\n",
    "    if not INST_PROMPT or not isinstance(INST_PROMPT, str):\n",
    "        raise ValueError(\"Instruction must be a non-empty string.\")\n",
    "    if not sample or not all(\n",
    "        isinstance(sample[field], str) for field in [context_field, response_field]\n",
    "    ):\n",
    "        raise ValueError(\"Fields must be a non-empty strings.\")\n",
    "\n",
    "    prompt = (\n",
    "        B_INST\n",
    "        + B_SYS\n",
    "        + SYS_PROMPT\n",
    "        + E_SYS\n",
    "        + INST_PROMPT\n",
    "        + sample[context_field]\n",
    "        + E_INST\n",
    "    )\n",
    "    sample[\"text\"] = prompt\n",
    "    return sample\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_dataset(\n",
    "    tokenizer: AutoTokenizer, max_length: int, seed: int, dataset: Dataset\n",
    "):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)  # , batched=True)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(\n",
    "        preprocess_batch, max_length=max_length, tokenizer=tokenizer\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        # remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ce6ed-6ca7-45dd-ac24-fc4589898a75",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "822c685b-1918-48af-b80a-4c11fca3598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 19:33:45,767 - llama2_finetune - INFO - Using k=2 CUDA GPUs with max memory {0: '41GB', 1: '41GB'}\n",
      "2023-10-17 19:33:45,767 - llama2_finetune - INFO - Using k=2 CUDA GPUs with max memory {0: '41GB', 1: '41GB'}\n",
      "2023-10-17 19:33:45,767 - llama2_finetune - INFO - Using k=2 CUDA GPUs with max memory {0: '41GB', 1: '41GB'}\n",
      "2023-10-17 19:33:45,767 - llama2_finetune - INFO - Using k=2 CUDA GPUs with max memory {0: '41GB', 1: '41GB'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# CUDA Parameters\n",
    "################################################################################\n",
    "major, _ = torch.cuda.get_device_capability()\n",
    "if major >= 8:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.bfloat16\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU does not support bfloat16: using fp=16\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.float16\n",
    "\n",
    "CUDA_N_GPUS = torch.cuda.device_count()\n",
    "CUDA_MAX_MEMORY = f\"{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB\"\n",
    "CUDA_MAX_MEMORY = {i: CUDA_MAX_MEMORY for i in range(CUDA_N_GPUS)}\n",
    "logger.info(f\"Using k={CUDA_N_GPUS} CUDA GPUs with max memory {CUDA_MAX_MEMORY}\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# User parameters\n",
    "################################################################################\n",
    "organization = \"gtfintechlab\"\n",
    "\n",
    "task_name = \"fomc_communication\"\n",
    "\n",
    "seeds = (5768, 78516, 944601)\n",
    "\n",
    "seed = seeds[0]\n",
    "\n",
    "model_parameters = \"7b\"\n",
    "model_id = f\"meta-llama/Llama-2-{model_parameters}-chat-hf\"\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "\n",
    "repo_name = f\"{organization}/{model_name}_{task_name}\"\n",
    "\n",
    "task_instruction, task_data = (\n",
    "    TASK_MAP[task_name][\"instruction\"],\n",
    "    TASK_MAP[task_name][\"data\"],\n",
    ")\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 16\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 8\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.05\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "# device_map = {\"\": 0}\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = compute_dtype\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = Path(f\"/fintech_storage/results/{MODEL_NAME}_{TASK}\")\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\" #\"paged_adamw_8bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\" #\"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b79b9f3-054d-429b-b6f9-6b3a7dc0d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple(\n",
    "    \"Args\",\n",
    "    [\n",
    "        \"task_name\",\n",
    "        \"seed\",\n",
    "        \"model_id\",\n",
    "        \"model_name\",\n",
    "        \"organization\",\n",
    "        \"uid\",\n",
    "        \"lora_r\",\n",
    "        \"lora_alpha\",\n",
    "        \"lora_dropout\",\n",
    "        \"max_seq_length\",\n",
    "        \"packing\",\n",
    "        \"device_map\",\n",
    "        \"load_in_4bit\",\n",
    "        \"bnb_4bit_compute_dtype\",\n",
    "        \"bnb_4bit_use_double_quant\",\n",
    "        \"bnb_4bit_quant_type\",\n",
    "        \"output_dir\",\n",
    "        \"num_train_epochs\",\n",
    "        \"fp16\",\n",
    "        \"bf16\",\n",
    "        \"per_device_train_batch_size\",\n",
    "        \"per_device_eval_batch_size\",\n",
    "        \"gradient_accumulation_steps\",\n",
    "        \"gradient_checkpointing\",\n",
    "        \"max_grad_norm\",\n",
    "        \"learning_rate\",\n",
    "        \"weight_decay\",\n",
    "        \"optim\",\n",
    "        \"lr_scheduler_type\",\n",
    "        \"max_steps\",\n",
    "        \"warmup_ratio\",\n",
    "        \"group_by_length\",\n",
    "        \"save_steps\",\n",
    "        \"logging_steps\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "args = Args(\n",
    "    task_name=task_name,\n",
    "    seed=seed,\n",
    "    model_id=model_id,\n",
    "    model_name=model_id.split(\"/\")[-1],\n",
    "    organization=organization,\n",
    "    uid=generate_uid(),\n",
    "    lora_r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=packing,\n",
    "    device_map=device_map,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    optim=optim,\n",
    "    lr_scheduler_type = lr_scheduler_type,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio= warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a213e-05b6-4b88-8773-b685cca59a61",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95933ede-6bdd-45c1-8092-7bf1b07724d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):  # , gradient_checkpointing_enabled=False):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        # Activate 4-bit precision base model loading\n",
    "        load_in_4bit=args.load_in_4bit,\n",
    "        # Activate nested quantization for 4-bit base models (double quantization)\n",
    "        bnb_4bit_use_double_quant=args.bnb_4bit_use_double_quant,\n",
    "        # Quantization type (fp4 or nf4)\n",
    "        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n",
    "        # Compute dtype for 4-bit base models\n",
    "        bnb_4bit_compute_dtype=args.bnb_4bit_compute_dtype,\n",
    "    )\n",
    "\n",
    "    # Tokenizer configured to fix overflow issues with fp16 training\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, pad_token=EOS)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        device_map=args.device_map,\n",
    "        # max_memory=CUDA_MAX_MEMORY,\n",
    "        torch_dtype=args.bnb_4bit_compute_dtype,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "    if args.max_length:\n",
    "        max_length = args.max_length\n",
    "    else:\n",
    "        max_length = get_max_length(model)\n",
    "\n",
    "    train_dataset = load_dataset(\n",
    "        f\"{args.organization}/{args.task_name}\", str(args.seed)\n",
    "    )[\"train\"]\n",
    "    preprocessed_train_dataset = preprocess_dataset(\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        seed=args.seed,\n",
    "        dataset=train_dataset,\n",
    "    )\n",
    "    # test_dataset = load_dataset(f\"{args.organization}/{args.task_name}\", str(args.seed))[\n",
    "    #     \"test\"max_grad_norm\n",
    "    # ]\n",
    "    # preprocessed_test_dataset = preprocess_dataset(\n",
    "    # tokenizer=tokenizer, max_length=max_length, seed=args.seed, dataset=test_dataset\n",
    "    # )\n",
    "\n",
    "    # Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    # Get lora module names\n",
    "    layers_for_adapters = find_all_linear_names(model)\n",
    "    print(f\"Layers for PEFT Adaptation: {layers_for_adapters}\")\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(layers_for_adapters)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    output_dir = args.output_dir / \"final_checkpoint\"\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        bf16=args.bf16,\n",
    "        # report_to=\"wandb\",\n",
    "        # Batch size per GPU for training\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        # Batch size per GPU for evaluation\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "        # Number of update steps to accumulate the gradients for\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        # Maximum gradient normal (gradient clipping)\n",
    "        max_grad_norm = args.max_grad_norm,\n",
    "        # Initial learning rate (AdamW optimizer)\n",
    "        learning_rate=args.learning_Rate,\n",
    "        # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "        weight_decay = args.weight_decay,\n",
    "        # Optimizer to use\n",
    "        optim=args.optim,\n",
    "        # Learning rate schedule (constant a bit better than cosine)\n",
    "        lr_scheduler_type=args.lr_scheduler_type,\n",
    "        # Number of training steps (overrides num_train_epochs)\n",
    "        max_steps=args.max_steps,\n",
    "        # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        # Save checkpoint every X updates steps\n",
    "        save_steps=args.save_steps,\n",
    "        # Log every X updates steps\n",
    "        logging_steps=args.logging_steps,\n",
    "        # Group sequences into batches with same length to save memory and speed up training\n",
    "        group_by_length = args.group_by_length,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=preprocessed_train_dataset,\n",
    "        args=training_arguments,\n",
    "        # data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "\n",
    "    # trainer = SFTTrainer(\n",
    "    #     model=model,\n",
    "    #     args=training_arguments,\n",
    "    #     max_seq_length=max_length,\n",
    "    #     train_dataset=preprocessed_train_dataset,\n",
    "    #     eval_dataset=preprocessed_test_dataset,\n",
    "    #     peft_config=peft_config,\n",
    "    #     dataset_text_field=\"text\",\n",
    "    #     tokenizer=tokenizer,\n",
    "    # )\n",
    "\n",
    "    # # Re-enable for inference to speed up predictions for similar inputs\n",
    "    model.config.use_cache = False\n",
    "    print(\"Training...\")\n",
    "    # Suppress specific warnings from torch.utils.checkpoint\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\", category=UserWarning, module=\"torch.utils.checkpoint\"\n",
    "        )\n",
    "        train_result = trainer.train()\n",
    "        metrics = trainer.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print_dtypes(trainer.model)\n",
    "        print(metrics)\n",
    "        # Saving model\n",
    "        print(\"Saving last checkpoint of the model...\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        trainer.model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Empty VRAM\n",
    "    del model\n",
    "    del pipe\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "\n",
    "    gc.collect()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e95ecfe-ec53-4565-9e3d-af64d3653e6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`torch_dtype` can be either `torch.dtype` or `\"auto\"`, but received bfloat16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Tokenizer configured to fix overflow issues with fp16 training\u001b[39;00m\n\u001b[1;32m     14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(args\u001b[38;5;241m.\u001b[39mmodel_id, pad_token\u001b[38;5;241m=\u001b[39mEOS)\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_memory=CUDA_MAX_MEMORY,\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m     24\u001b[0m     max_length \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mmax_length\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/transformers/modeling_utils.py:2917\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m                 logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2913\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSince the `torch_dtype` attribute can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be found in model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms config object, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2914\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill use torch_dtype=\u001b[39m\u001b[38;5;132;01m{torch_dtype}\u001b[39;00m\u001b[38;5;124m as derived from model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms weights\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2915\u001b[0m                 )\n\u001b[1;32m   2916\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2917\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2918\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`torch_dtype` can be either `torch.dtype` or `\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2919\u001b[0m             )\n\u001b[1;32m   2920\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_set_default_torch_dtype(torch_dtype)\n\u001b[1;32m   2922\u001b[0m \u001b[38;5;66;03m# Check if `_keep_in_fp32_modules` is not None\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: `torch_dtype` can be either `torch.dtype` or `\"auto\"`, but received bfloat16"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b57a8b-a390-483c-b9db-76864f6b0bed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248dabe-d89a-4800-baab-d64985c83f50",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8f947-63c4-40f5-b267-a476e2592e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload final model checkpoint and save\n",
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir / \"final_checkpoint\",\n",
    "    device_map=args.device_map,\n",
    "    torch_dtype=compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db431f-2ca3-4f1b-b658-9da05372572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dtypes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c88ae3-0dd2-4bc0-b848-96d7074492c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=args.device_map,\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    "    torch_dtype=compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d20c9-effc-4ec1-abc6-2774f978e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dtypes(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42375f0e-a798-4d8d-9273-94aeafd7b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method merges the LoRa layers into the base model. This is needed to use it as a standalone model.\n",
    "peft_model = PeftModel.from_pretrained(base_model, args.output_dir / \"final_checkpoint\")\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.output_dir / \"final_checkpoint\", pad_token=EOS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cda631-a219-4e0d-bd7d-bc26fe452661",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dtypes(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e52f15-e8b1-4d44-9854-1fbccd0fe14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save inference\n",
    "merged_checkpoint_dir = args.output_dir / \"final_merged_checkpoint\"\n",
    "model.save_pretrained(merged_checkpoint_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(merged_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6423cb2a-a1a5-4dcf-bee3-629837625d1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# push to hub\n",
    "peft_model.push_to_hub(repo_name, private=True, use_temp_dir=True)\n",
    "tokenizer.push_to_hub(repo_name, private=True, use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc76d0-8b92-49c7-80e8-77415587ffaf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c20714-ba12-4654-bf78-6fc1f539e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move to configs or args\n",
    "temperature = 0.0  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "do_sample = False\n",
    "max_new_tokens = 256\n",
    "top_k = 10\n",
    "top_p = 0.92\n",
    "repetition_penalty = 1.0  # 1.0 means no penalty\n",
    "num_return_sequences = 1  # Only generate one response\n",
    "num_beams = 1\n",
    "\n",
    "\n",
    "def generate(model=None, tokenizer=None, dataset=None):\n",
    "    input_ids = tokenizer(dataset[\"text\"])\n",
    "\n",
    "    # Ensure that input_ids is a PyTorch tensor\n",
    "    # input_ids = torch.tensor(input_ids).long()\n",
    "\n",
    "    # Move the tensor to the GPU\n",
    "    input_ids = input_ids.cuda()\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_beams,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "        ),\n",
    "    )\n",
    "    seq = generation_output.sequences\n",
    "    output = tokenizer.decode(seq[0])\n",
    "    return output.split(\"[/INST]\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e1e8c-b9cd-4b33-8cba-7118b90825a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(f\"{args.organization}/{args.task_name}\", str(args.seed))[\n",
    "    \"test\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8d781-e6d4-442a-a766-9ae456d3fd79",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d203a4-8ffc-4835-9292-f4ed71c34f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=args.device_map,\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    "    torch_dtype=compute_dtype,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = EOS\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "\n",
    "preprocessed_test_dataset = preprocess_dataset(\n",
    "    tokenizer=tokenizer, max_length=max_length, seed=args.seed, dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4208bdb-4d0f-4b0d-87f3-364bd8e874bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b576e3-4143-4a20-9b74-f38149fcea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"What is a large language model?\"\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=max_length\n",
    ")\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603f1d2-2f9b-46a2-a4bb-159ce7ff03fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4493b30-a580-4300-ba59-f04bd20dd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_GENS = preprocessed_test_dataset.num_rows\n",
    "N_GENS = 10\n",
    "\n",
    "output_list = []\n",
    "for i in range(N_GENS):\n",
    "    output_list.append(\n",
    "        generate(model=model, tokenizer=tokenizer, dataset=preprocessed_test_dataset)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "id": "178af452-6cdb-446f-9593-1743ae29ba58",
   "metadata": {},
   "source": [
    "output_list\n",
    "\n",
    ".replace(\n",
    "            \"</s>\", \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c99f9-57bf-46a9-914a-b4ae7c9df252",
   "metadata": {},
   "source": [
    "### Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a025ddd-fd45-44e3-a6a9-93cf76c3c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_checkpoint_dir,\n",
    "    device_map=args.device_map,\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    "    torch_dtype=compute_dtype,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_checkpoint_dir)\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "\n",
    "preprocessed_dataset = preprocess_dataset(\n",
    "    tokenizer=tokenizer, max_length=max_length, seed=args.seed, dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27a41155-eec1-43f6-9645-84ac71c2609b",
   "metadata": {},
   "source": [
    "df_test_dataset = convert_dataset(dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a59ca28a-3f65-4e96-9aef-47a9149e51e2",
   "metadata": {},
   "source": [
    "input_list = df_test_dataset[\"prompt\"].to_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conference Environment",
   "language": "python",
   "name": "conference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
