{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf37c31a-e659-477b-a0bc-fec3df7b1b6b",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning of Llama2 on FOMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33951517-366d-4863-a9fc-439de8145b83",
   "metadata": {},
   "source": [
    "## Not for Public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb44486-b7d1-4fb6-acc5-63a51ff2a9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/AD/gmatlin3/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "HF_AUTH = \"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\"\n",
    "huggingface_hub.login(token=HF_AUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d12fda-d164-4699-afc9-f1b1dd90639c",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c428c4bf-1348-496d-bd72-b8c03612a895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "# # set the wandb project where this run will be logged\n",
    "# os.environ[\"WANDB_PROJECT\"] = f\"llama2_sft_fomc\"\n",
    "# # save your trained model checkpoint to wandb\n",
    "# os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
    "# # turn off watch to log faster\n",
    "# os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"fa69ffc6a97578da0410b553042cbb8b3bf5fcaf\"\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = f\"llama2_sft\"\n",
    "\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2424d-31dc-4d8e-a71f-8087ec428ee6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79f553-61c2-46e9-90fe-306180ac37be",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "540c80a3-1c30-42aa-b5b3-48f5262a8fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "SRC_DIRECTORY = Path().cwd().resolve().parent\n",
    "\n",
    "if str(SRC_DIRECTORY) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIRECTORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a84af4a-da5d-4192-bac5-2ef2e764bb1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "hf_logging.set_verbosity(hf_logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(\"llama2_finetune\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create handlers\n",
    "c_handler = logging.StreamHandler()\n",
    "f_handler = logging.FileHandler('llama2_finetune.log')\n",
    "c_handler.setLevel(logging.DEBUG)\n",
    "f_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "# Create formatters and add it to handlers\n",
    "format = '%(name)s - %(levelname)s - %(message)s'\n",
    "c_format = logging.Formatter(format)\n",
    "f_format = logging.Formatter(format)\n",
    "c_handler.setFormatter(c_format)\n",
    "f_handler.setFormatter(f_format)\n",
    "\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(c_handler)\n",
    "logger.addHandler(f_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acfe49ed-209e-4ec7-97c4-3ae2abdc909b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n",
      "Detected PIL version 10.0.1\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import uuid\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from peft import (\n",
    "    AutoPeftModelForCausalLM,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LlamaConfig,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaModel,\n",
    "    LlamaTokenizer,\n",
    "    TextGenerationPipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from llama.instructions import (\n",
    "    B_INST,\n",
    "    B_SYS,\n",
    "    E_INST,\n",
    "    E_SYS,\n",
    "    TASK_MAP,\n",
    "    llama2_prompt_generator,\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# TODO: Import EOS from elsewhere\n",
    "EOS = \"</s>\"\n",
    "\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8483511b-f826-4016-90d1-6d424c99cd6a",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ee5afe-e618-44b5-972b-55ef4cb0d913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_uid(id_length=8, dt_format=\"%y%m%d\"):\n",
    "    date_str = datetime.now().strftime(dt_format)\n",
    "\n",
    "    # Generate a short UUID\n",
    "    uid = str(uuid.uuid4())[:id_length]\n",
    "\n",
    "    # Combine\n",
    "    uid = f\"{uid}_{date_str}\"\n",
    "\n",
    "    return uid\n",
    "\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    # SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    cls = (\n",
    "        bnb.nn.Linear4bit\n",
    "    )  # if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def print_dtypes(model):\n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes:\n",
    "            dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items():\n",
    "        total += v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v / total)\n",
    "\n",
    "\n",
    "def create_peft_config(args, modules):\n",
    "    peft_config = LoraConfig(\n",
    "        # Pass our list as an argument to the PEFT config for your model\n",
    "        target_modules=modules,\n",
    "        # Dimension of the LoRA matrices we update in adapaters\n",
    "        r=args.lora_r,\n",
    "        # Alpha parameter for LoRA scaling\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        # Dropout probability for LoRA layers\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    return peft_config\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "# def convert_dataset(ds):\n",
    "#     prompts = llama2_prompt_generator(TASK_INSTRUCTION, ds['text'])\n",
    "#     labels = [decode(L).upper() for L in ds['label']]\n",
    "#     df = pd.DataFrame.from_dict({'prompt': prompts, 'response': labels})\n",
    "#     return df\n",
    "\n",
    "\n",
    "def create_prompt_formats(\n",
    "    sample, context_field=\"sentence\", response_field=\"label_decoded\"\n",
    "):\n",
    "    SYS_PROMPT = f\"\"\"\"Discard all the previous instructions.\n",
    "    Below is an instruction that describes a task.\n",
    "    Write a response that appropriately completes the request.\"\"\"\n",
    "\n",
    "    INST_PROMPT = args.task_instruction\n",
    "\n",
    "    if not INST_PROMPT or not isinstance(INST_PROMPT, str):\n",
    "        raise ValueError(\"Instruction must be a non-empty string.\")\n",
    "    if not sample or not all(\n",
    "        isinstance(sample[field], str) for field in [context_field, response_field]\n",
    "    ):\n",
    "        raise ValueError(\"Fields must be a non-empty strings.\")\n",
    "\n",
    "    prompt = (\n",
    "        B_INST\n",
    "        + B_SYS\n",
    "        + SYS_PROMPT\n",
    "        + E_SYS\n",
    "        + INST_PROMPT\n",
    "        + sample[context_field]\n",
    "        + E_INST\n",
    "    )\n",
    "    sample[\"text\"] = prompt\n",
    "    return sample\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_dataset(\n",
    "    tokenizer: AutoTokenizer, max_length: int, seed: int, dataset: Dataset\n",
    "):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)  # , batched=True)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(\n",
    "        preprocess_batch, max_length=max_length, tokenizer=tokenizer\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        # remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ce6ed-6ca7-45dd-ac24-fc4589898a75",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "822c685b-1918-48af-b80a-4c11fca3598c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - Using k=2 CUDA GPUs with max memory {0: '41GB', 1: '41GB'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# User parameters\n",
    "################################################################################\n",
    "organization = \"gtfintechlab\"\n",
    "\n",
    "task_name = \"fomc_communication\"\n",
    "\n",
    "seeds = (5768, 78516, 944601)\n",
    "\n",
    "seed = seeds[0]\n",
    "\n",
    "model_parameters = \"7b\"\n",
    "model_id = f\"meta-llama/Llama-2-{model_parameters}-chat-hf\"\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "\n",
    "repo_name = f\"{organization}/{model_name}_{task_name}\"\n",
    "\n",
    "task_instruction, task_data = (\n",
    "    TASK_MAP[task_name][\"instruction\"],\n",
    "    TASK_MAP[task_name][\"data\"],\n",
    ")\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 16\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 32\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "################################################################################\n",
    "# CUDA Parameters\n",
    "################################################################################\n",
    "\n",
    "# Enable fp16/bf16 training\n",
    "\n",
    "major, _ = torch.cuda.get_device_capability()\n",
    "if major >= 8:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.bfloat16\n",
    "    fp16, bf16 = False, True\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU does not support bfloat16: using fp=16\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.float16\n",
    "    fp16, bf16 = True, False\n",
    "\n",
    "CUDA_N_GPUS = torch.cuda.device_count()\n",
    "CUDA_MAX_MEMORY = f\"{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB\"\n",
    "CUDA_MAX_MEMORY = {i: CUDA_MAX_MEMORY for i in range(CUDA_N_GPUS)}\n",
    "logger.info(f\"Using k={CUDA_N_GPUS} CUDA GPUs with max memory {CUDA_MAX_MEMORY}\")\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "# device_map = {\"\": 0}\n",
    "device_map = \"auto\"\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate 8-bit precision base model loading\n",
    "load_in_8bit = False\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = compute_dtype\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = Path(f\"/fintech_3/20231018/results/{model_name}_{task_name}\")\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 3\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"adamw_bnb_8bit\" #\"paged_adamw_8bit\" #\"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"constant\" #\"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = 250\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "#new\n",
    "save_strategy=\"epoch\"\n",
    "disable_tqdm=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b79b9f3-054d-429b-b6f9-6b3a7dc0d88d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Args = namedtuple(\n",
    "    \"Args\",\n",
    "    [\n",
    "        \"task_name\",\n",
    "        \"task_instruction\",\n",
    "        \"seed\",\n",
    "        \"model_id\",\n",
    "        \"model_name\",\n",
    "        \"organization\",\n",
    "        \"uid\",\n",
    "        \"lora_r\",\n",
    "        \"lora_alpha\",\n",
    "        \"lora_dropout\",\n",
    "        \"max_seq_length\",\n",
    "        \"packing\",\n",
    "        \"device_map\",\n",
    "        \"load_in_4bit\",\n",
    "        \"load_in_8bit\",\n",
    "        \"bnb_4bit_compute_dtype\",\n",
    "        \"bnb_4bit_use_double_quant\",\n",
    "        \"bnb_4bit_quant_type\",\n",
    "        \"output_dir\",\n",
    "        \"num_train_epochs\",\n",
    "        \"fp16\",\n",
    "        \"bf16\",\n",
    "        \"per_device_train_batch_size\",\n",
    "        \"per_device_eval_batch_size\",\n",
    "        \"gradient_accumulation_steps\",\n",
    "        \"gradient_checkpointing\",\n",
    "        \"max_grad_norm\",\n",
    "        \"learning_rate\",\n",
    "        \"weight_decay\",\n",
    "        \"optim\",\n",
    "        \"lr_scheduler_type\",\n",
    "        \"max_steps\",\n",
    "        \"warmup_ratio\",\n",
    "        \"group_by_length\",\n",
    "        \"save_steps\",\n",
    "        \"logging_steps\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "args = Args(\n",
    "    task_name=task_name,\n",
    "    task_instruction=task_instruction,\n",
    "    seed=seed,\n",
    "    model_id=model_id,\n",
    "    model_name=model_id.split(\"/\")[-1],\n",
    "    organization=organization,\n",
    "    uid=generate_uid(),\n",
    "    lora_r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=packing,\n",
    "    device_map=device_map,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    optim=optim,\n",
    "    lr_scheduler_type = lr_scheduler_type,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio= warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a213e-05b6-4b88-8773-b685cca59a61",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95933ede-6bdd-45c1-8092-7bf1b07724d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(args):  # , gradient_checkpointing_enabled=False):\n",
    "    \n",
    "    logger.info(\"Starting the training process...\")\n",
    "\n",
    "    logger.info(\"Creating BitsAndBytesConfig...\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        # Activate k-bit precision base model loading\n",
    "        load_in_4bit=args.load_in_4bit,\n",
    "        load_in_8bit=args.load_in_8bit,\n",
    "\n",
    "        # Activate nested quantization for 4-bit base models (double quantization)\n",
    "        bnb_4bit_use_double_quant=args.bnb_4bit_use_double_quant,\n",
    "        bnb_8bit_use_double_quant=args.bnb_4bit_use_double_quant,\n",
    "        \n",
    "        # Quantization type (fp4 or nf4)\n",
    "        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n",
    "        bnb_8bit_quant_type=args.bnb_4bit_quant_type,\n",
    "        \n",
    "        # Compute dtype for 4-bit base models\n",
    "        bnb_4bit_compute_dtype=args.bnb_4bit_compute_dtype,\n",
    "        bnb_8bit_compute_dtype=args.bnb_4bit_compute_dtype,\n",
    "        \n",
    "    )\n",
    "\n",
    "    logger.info(\"Loading the Tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=False)\n",
    "    logger.info(\"Tokenizer pad token specified as the EOS token\")\n",
    "    tokenizer.pad_token = EOS\n",
    "    # logger.info(\"Tokenizer configured to fix overflow issues with fp16 training\"\n",
    "    # tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "    logger.info(\"Loading the CausalLM...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        load_in_4bit=args.load_in_4bit,\n",
    "        load_in_8bit=args.load_in_8bit,\n",
    "        device_map=args.device_map,\n",
    "        max_memory=CUDA_MAX_MEMORY,\n",
    "        torch_dtype=compute_dtype,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=False\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    max_seq_length = get_max_length(model)\n",
    "\n",
    "\n",
    "    logger.info(\"Loading train dataset...\")\n",
    "    train_dataset = load_dataset(\n",
    "        f\"{args.organization}/{args.task_name}\", str(args.seed)\n",
    "    )[\"train\"]\n",
    "\n",
    "    logger.info(\"Preprocessing train dataset...\")\n",
    "    preprocessed_train_dataset = preprocess_dataset(\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_seq_length,\n",
    "        seed=args.seed,\n",
    "        dataset=train_dataset,\n",
    "    )\n",
    "    logger.info(\"Train dataset preprocessed.\")\n",
    "\n",
    "    logger.info(\"Loading test dataset...\")\n",
    "    test_dataset = load_dataset(f\"{args.organization}/{args.task_name}\", str(args.seed))[\n",
    "        \"test\"\n",
    "    ]\n",
    "    logger.info(\"Preprocessing test dataset...\")\n",
    "    preprocessed_test_dataset = preprocess_dataset(\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_seq_length,\n",
    "        seed=args.seed,\n",
    "        dataset=test_dataset\n",
    "    )\n",
    "    logger.info(\"Test dataset preprocessed.\")\n",
    "\n",
    "    logger.info(\"Getting the model's memory footprint...\")\n",
    "    logger.info(model.get_memory_footprint())\n",
    "    logger.info(\"Using the prepare_model_for_kbit_training method from PEFT...\")\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    logger.info(\"Get lora module names...\")\n",
    "    layers_for_adapters = find_all_linear_names(model)\n",
    "    logger.info(f\"Layers for PEFT Adaptation: {layers_for_adapters}\")\n",
    "    logger.info(\"Create PEFT config for these modules and wrap the model to PEFT...\")\n",
    "    peft_config = create_peft_config(args, layers_for_adapters)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    logger.info(\"Print information about the percentage of trainable parameters...\")\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    logger.info(\"Make output directory...\")\n",
    "    output_dir = args.output_dir / \"final_checkpoint\"\n",
    "    output_dir.mkdir(mode=0o777, parents=True, exist_ok=True)\n",
    "\n",
    "    logger.info(\"Define TrainingArguments...\")\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        fp16=args.fp16,\n",
    "        bf16=args.bf16,\n",
    "        report_to=\"wandb\",\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        max_grad_norm = args.max_grad_norm,\n",
    "        weight_decay = args.weight_decay,\n",
    "        optim=args.optim,\n",
    "        learning_rate=args.learning_rate,\n",
    "        lr_scheduler_type=args.lr_scheduler_type,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        max_steps=args.max_steps,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        save_steps=args.save_steps,\n",
    "        logging_steps=args.logging_steps,\n",
    "        group_by_length = args.group_by_length,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Defining SFTTrainer...\")    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_arguments,\n",
    "        packing=args.packing,\n",
    "        max_seq_length=max_seq_length,\n",
    "        train_dataset=preprocessed_train_dataset,\n",
    "        eval_dataset=preprocessed_test_dataset,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # # Suppress specific warnings from torch.utils.checkpoint\n",
    "    # with warnings.catch_warnings():\n",
    "    #     warnings.filterwarnings(\n",
    "    #         \"ignore\", category=UserWarning, module=\"torch.utils.checkpoint\"\n",
    "    #     )\n",
    "    logger.info(\"Running trainer.train() ...\")\n",
    "    trainer.train()\n",
    "    metrics = trainer.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    print(metrics)\n",
    "    trainer.save_state()\n",
    "    logger.info(\"Saving model\")\n",
    "    model = trainer.model\n",
    "    print_dtypes(model)\n",
    "    logger.info(\"Saving last checkpoint of the model...\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95ecfe-ec53-4565-9e3d-af64d3653e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - Starting the training process...\n",
      "llama2_finetune - INFO - Creating BitsAndBytesConfig...\n",
      "llama2_finetune - INFO - Loading the Tokenizer...\n",
      "loading file tokenizer.model from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/tokenizer.model\n",
      "loading file tokenizer.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/tokenizer_config.json\n",
      "llama2_finetune - INFO - Tokenizer pad token specified as the EOS token\n",
      "llama2_finetune - INFO - Loading the CausalLM...\n",
      "loading configuration file config.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Detected PIL version 10.0.1\n",
      "loading weights file model.safetensors from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "Detected 8-bit loading: activating 8-bit loading for this model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51a6c365cd14f01aaef503b59933da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/generation_config.json\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "llama2_finetune - INFO - Loading train dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - Preprocessing train dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77ad6e96a0247f1845e26597afc767d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde519c118374d1fbaded3988206795c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2570532204d548709a56203d0cd014f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - Train dataset preprocessed.\n",
      "llama2_finetune - INFO - Loading test dataset...\n",
      "llama2_finetune - INFO - Preprocessing test dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478e6305844f4e229486c9b44e277d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10885a16150b4e6a802f8659080379c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a97e9b5d93848a28fc692cb639d9f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - Test dataset preprocessed.\n",
      "llama2_finetune - INFO - Getting the model's memory footprint...\n",
      "llama2_finetune - INFO - 3829936128\n",
      "llama2_finetune - INFO - Using the prepare_model_for_kbit_training method from PEFT...\n",
      "llama2_finetune - INFO - Get lora module names...\n",
      "llama2_finetune - INFO - Layers for PEFT Adaptation: ['k_proj', 'v_proj', 'gate_proj', 'up_proj', 'down_proj', 'o_proj', 'q_proj']\n",
      "llama2_finetune - INFO - Create PEFT config for these modules and wrap the model to PEFT...\n",
      "llama2_finetune - INFO - Print information about the percentage of trainable parameters...\n",
      "llama2_finetune - INFO - Make output directory...\n",
      "llama2_finetune - INFO - Define TrainingArguments...\n",
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n",
      "llama2_finetune - INFO - Defining SFTTrainer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39976960 || all params: 3540389888 || trainable%: 1.1291682911958425\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5c15b939e241f19393f0707fde1844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90ce70d862b462c881b1c13b546cf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "The model is loaded in 8-bit precision. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check  the examples in https://github.com/huggingface/peft for more details.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "llama2_finetune - INFO - Running trainer.train() ...\n",
      "Currently training with a batch size of: 1\n",
      "***** Running training *****\n",
      "  Num examples = 1,984\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 250\n",
      "  Number of trainable parameters = 39,976,960\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mglennmatlin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/AD/gmatlin3/zero-shot/src/notebooks/wandb/run-20231019_005059-7e6cxlus</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/glennmatlin/huggingface/runs/7e6cxlus' target=\"_blank\">legendary-star-17</a></strong> to <a href='https://wandb.ai/glennmatlin/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/glennmatlin/huggingface' target=\"_blank\">https://wandb.ai/glennmatlin/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/glennmatlin/huggingface/runs/7e6cxlus' target=\"_blank\">https://wandb.ai/glennmatlin/huggingface/runs/7e6cxlus</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='111' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [111/250 02:46 < 03:31, 0.66 it/s, Epoch 0.11/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.900700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.538700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.290500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/checkpoint-25\n",
      "tokenizer config file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/checkpoint-25/tokenizer_config.json\n",
      "Special tokens file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/checkpoint-25/special_tokens_map.json\n",
      "Saving model checkpoint to /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/checkpoint-50\n",
      "tokenizer config file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/checkpoint-50/special_tokens_map.json\n",
      "Saving model checkpoint to /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/checkpoint-75\n",
      "tokenizer config file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/checkpoint-75/tokenizer_config.json\n",
      "Special tokens file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/checkpoint-75/special_tokens_map.json\n",
      "Saving model checkpoint to /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/checkpoint-100\n",
      "tokenizer config file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /fintech_3/20231018/results/Llama-2-7b-chat-hf_fomc_communication/checkpoint-100/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train(args)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # Empty VRAM\n",
    "    del model\n",
    "    del pipe\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "\n",
    "    gc.collect()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b57a8b-a390-483c-b9db-76864f6b0bed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248dabe-d89a-4800-baab-d64985c83f50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8f947-63c4-40f5-b267-a476e2592e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload final model checkpoint and save\n",
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir / \"final_checkpoint\",\n",
    "    device_map=args.device_map,\n",
    "    torch_dtype=compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db431f-2ca3-4f1b-b658-9da05372572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dtypes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c88ae3-0dd2-4bc0-b848-96d7074492c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=args.device_map,\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    "    torch_dtype=compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d20c9-effc-4ec1-abc6-2774f978e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dtypes(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42375f0e-a798-4d8d-9273-94aeafd7b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method merges the LoRa layers into the base model. This is needed to use it as a standalone model.\n",
    "peft_model = PeftModel.from_pretrained(base_model, args.output_dir / \"final_checkpoint\")\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.output_dir / \"final_checkpoint\", pad_token=EOS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cda631-a219-4e0d-bd7d-bc26fe452661",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dtypes(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e52f15-e8b1-4d44-9854-1fbccd0fe14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save inference\n",
    "merged_checkpoint_dir = args.output_dir / \"final_merged_checkpoint\"\n",
    "model.save_pretrained(merged_checkpoint_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(merged_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6423cb2a-a1a5-4dcf-bee3-629837625d1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# push to hub\n",
    "peft_model.push_to_hub(repo_name, private=True, use_temp_dir=True)\n",
    "tokenizer.push_to_hub(repo_name, private=True, use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc76d0-8b92-49c7-80e8-77415587ffaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c20714-ba12-4654-bf78-6fc1f539e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move to configs or args\n",
    "temperature = 0.0  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "do_sample = False\n",
    "max_new_tokens = 256\n",
    "top_k = 10\n",
    "top_p = 0.92\n",
    "repetition_penalty = 1.0  # 1.0 means no penalty\n",
    "num_return_sequences = 1  # Only generate one response\n",
    "num_beams = 1\n",
    "\n",
    "\n",
    "def generate(model=None, tokenizer=None, dataset=None):\n",
    "    input_ids = tokenizer(dataset[\"text\"])\n",
    "\n",
    "    # Ensure that input_ids is a PyTorch tensor\n",
    "    # input_ids = torch.tensor(input_ids).long()\n",
    "\n",
    "    # Move the tensor to the GPU\n",
    "    input_ids = input_ids.cuda()\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_beams,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "        ),\n",
    "    )\n",
    "    seq = generation_output.sequences\n",
    "    output = tokenizer.decode(seq[0])\n",
    "    return output.split(\"[/INST]\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e1e8c-b9cd-4b33-8cba-7118b90825a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(f\"{args.organization}/{args.task_name}\", str(args.seed))[\n",
    "    \"test\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8d781-e6d4-442a-a766-9ae456d3fd79",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d203a4-8ffc-4835-9292-f4ed71c34f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=args.device_map,\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    "    torch_dtype=compute_dtype,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = EOS\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "\n",
    "preprocessed_test_dataset = preprocess_dataset(\n",
    "    tokenizer=tokenizer, max_length=max_length, seed=args.seed, dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4208bdb-4d0f-4b0d-87f3-364bd8e874bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b576e3-4143-4a20-9b74-f38149fcea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"What is a large language model?\"\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=max_length\n",
    ")\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603f1d2-2f9b-46a2-a4bb-159ce7ff03fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4493b30-a580-4300-ba59-f04bd20dd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_GENS = preprocessed_test_dataset.num_rows\n",
    "N_GENS = 10\n",
    "\n",
    "output_list = []\n",
    "for i in range(N_GENS):\n",
    "    output_list.append(\n",
    "        generate(model=model, tokenizer=tokenizer, dataset=preprocessed_test_dataset)\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "178af452-6cdb-446f-9593-1743ae29ba58",
   "metadata": {},
   "source": [
    "output_list\n",
    "\n",
    ".replace(\n",
    "            \"</s>\", \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c99f9-57bf-46a9-914a-b4ae7c9df252",
   "metadata": {},
   "source": [
    "### Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a025ddd-fd45-44e3-a6a9-93cf76c3c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_checkpoint_dir,\n",
    "    device_map=args.device_map,\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    "    torch_dtype=compute_dtype,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_checkpoint_dir)\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "\n",
    "preprocessed_dataset = preprocess_dataset(\n",
    "    tokenizer=tokenizer, max_length=max_length, seed=args.seed, dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27a41155-eec1-43f6-9645-84ac71c2609b",
   "metadata": {},
   "source": [
    "df_test_dataset = convert_dataset(dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a59ca28a-3f65-4e96-9aef-47a9149e51e2",
   "metadata": {},
   "source": [
    "input_list = df_test_dataset[\"prompt\"].to_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conference",
   "language": "python",
   "name": "conference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
