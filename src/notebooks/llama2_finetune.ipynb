{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf37c31a-e659-477b-a0bc-fec3df7b1b6b",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning of Llama2 on Financial Phrasebank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79f553-61c2-46e9-90fe-306180ac37be",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d3ba20-0627-4f51-9c91-4df4c8783991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SRC_DIRECTORY = Path().cwd().resolve().parent\n",
    "\n",
    "if str(SRC_DIRECTORY) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIRECTORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03982813-c016-4780-8b67-3931384881d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('llama2_finetune')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "consoleHandler.setFormatter(formatter)\n",
    "logger.addHandler(consoleHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb97797-1ecc-498b-ae91-75692518831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4413c42-65b9-404c-9ca7-02e39d1238bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaModel, LlamaConfig, TextGenerationPipeline\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a32f77-63d4-4bcf-b19b-85cc5110ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d880d156-7894-483f-98e9-c6e74ac8d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c97d6ab1-8981-4ef2-afe5-4d95a8a7472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab82d5d-2a20-4dd5-aaa6-a4bf5d9f000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7668a30-2546-48b0-b2bd-9110c4810db9",
   "metadata": {},
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbcfe766-8f48-499b-aaf4-f0de0af4c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec31578c-71a8-4616-8a8d-98d5eca8bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "30222269-b7cf-48f0-9b26-adcc3438a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07169f75-8535-40a5-bdc3-c1e7a7cabd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.instructions import TASK_MAP, llama2_prompt_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a50701a-b12a-46e6-a90f-01549cb05f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea22f0a0-70b3-4f80-b922-a664ab971570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4895e04-3acf-4c95-accd-d654c3bc5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ce6ed-6ca7-45dd-ac24-fc4589898a75",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9140a49-bf41-499d-9d51-2f4fafb86d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple(\"Args\", [\"task_name\"])\n",
    "args = Args(task_name=\"sentiment_analysis\")\n",
    "\n",
    "# TODO: move more configs into my args?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c27bdbc-4654-4e63-9eee-44fa90845da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37d709fb_231007\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate a date string\n",
    "date_str = datetime.now().strftime(\"%y%m%d\")\n",
    "\n",
    "# Generate a short UUID\n",
    "uid = str(uuid.uuid4())[:8]\n",
    "\n",
    "# Combine\n",
    "uid = f\"{uid}_{date_str}\"\n",
    "\n",
    "print(uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235715a-7bee-4cfc-a520-9b28121777e3",
   "metadata": {},
   "source": [
    "### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeb44486-b7d1-4fb6-acc5-63a51ff2a9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/AD/gmatlin3/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "hf_auth = \"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\" \n",
    "# TODO REMOVE MY TOKEN FOR THE FINAL VERSION\n",
    "# TODO PROVIDE DIRECTIONS FOR HOW TO GET HF TOKEN HERE\n",
    "\n",
    "huggingface_hub.login(token=hf_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "41242074-0980-463d-90e1-47c1672e5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORGANIZATION = \"gtfintechlab\"\n",
    "DATASET = \"financial_phrasebank\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d865ad-6288-47a1-bf41-6091573f09d4",
   "metadata": {},
   "source": [
    "### Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "77a24fbb-1721-48e5-aae3-4bc0f0749f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path.home() / \"{model_name}-{dataset}-results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d65aff-161f-4eb2-9744-1dfaa62410f9",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd1a531e-0b51-472a-baaa-1c8c7c4cb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"llama2-fpb-sft\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"false\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd23b2-a1e6-4981-9d28-0865462ca4ce",
   "metadata": {},
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4270d1a-aab8-408d-86ec-0af14b6b38e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "major, _ = torch.cuda.get_device_capability()\n",
    "if major >= 8:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.bfloat16\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU does not support bfloat16: using fp=16\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61ef82bf-f204-48c3-8431-d6f66c6b5a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 22:45:12,891 - llama2_finetune - INFO - Using k=2 CUDA GPUs with max memory {0: '41GB', 1: '41GB'}\n"
     ]
    }
   ],
   "source": [
    "CUDA_N_GPUS = torch.cuda.device_count()\n",
    "CUDA_MAX_MEMORY = f\"{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB\"\n",
    "CUDA_MAX_MEMORY = {i: CUDA_MAX_MEMORY for i in range(CUDA_N_GPUS)}\n",
    "logger.info(\n",
    "    f\"Using k={CUDA_N_GPUS} CUDA GPUs with max memory {CUDA_MAX_MEMORY}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5e95cb5-2bd6-491c-a2a3-f9df4f037e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_map = {\"\":0}\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d02251-0dd6-477f-97cd-81123caf08d3",
   "metadata": {},
   "source": [
    "## Financial PhraseBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "375f0a4e-98d1-4386-9136-1fdfc4738469",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS = (5768, 78516, 944601)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8cd74d75-108f-4802-8e31-a782bd9177a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = SEEDS[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fb611-9c02-4e8b-a88e-7708987c2037",
   "metadata": {},
   "source": [
    "### Create splits on HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3518f075-bcb2-4d56-8605-ba1d383cfbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(label_number):\n",
    "    if label_number == 0:\n",
    "        return \"POSITIVE\"\n",
    "    elif label_number == 1:\n",
    "        return \"NEGATIVE\"\n",
    "    elif label_number == 2:\n",
    "        return \"NEUTRAL\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c2cf2a84-d424-4eb6-a225-f2363fdd7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fpb_hub_datasets(seed=None):\n",
    "    configs = [\n",
    "        \"sentences_allagree\",\n",
    "        \"sentences_75agree\",\n",
    "        \"sentences_66agree\",\n",
    "        \"sentences_50agree\",\n",
    "    ]\n",
    "\n",
    "    for config in tqdm(configs, desc=\"Configs\"):\n",
    "        try:\n",
    "            fpb_dataset = load_dataset(\"financial_phrasebank\", config)\n",
    "            config_short = config.replace(\"sentences_\", \"\")\n",
    "\n",
    "            texts = fpb_dataset[\"train\"][\"sentence\"]\n",
    "            labels = fpb_dataset[\"train\"][\"label\"]\n",
    "\n",
    "            splits = {}\n",
    "\n",
    "            # Splitting the data\n",
    "            train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "                texts, labels, test_size=0.2, random_state=seed\n",
    "            )\n",
    "\n",
    "            # Storing in the dictionary\n",
    "            splits[seed] = {\n",
    "                \"train\": Dataset.from_dict({\"context\": train_texts, \"response\": list(map(decode,train_labels))}),\n",
    "                \"test\": Dataset.from_dict({\"context\": test_texts, \"response\": list(map(decode, test_labels))}),\n",
    "            }\n",
    "\n",
    "            # Push to HF Hub\n",
    "            splits[seed][\"train\"].push_to_hub(\n",
    "                f\"{ORGANIZATION}/{DATASET}-{config_short}-{seed}\",\n",
    "                config_name=\"train\",\n",
    "                private=True,\n",
    "            )\n",
    "            splits[seed][\"test\"].push_to_hub(\n",
    "                f\"{ORGANIZATION}/{DATASET}-{config_short}-{seed}\",\n",
    "                config_name=\"test\",\n",
    "                private=True,\n",
    "            )\n",
    "\n",
    "            return splits\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing config {config}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2e755781-518d-4f99-8cab-a9766bde132f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340dcdaf95ea429a812d03451352df55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Configs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cb3aaea0a24c7a9bf244c5a707b961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510a07ef609a49f1a4de4307ea9c8de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbc37faefee4cc7b6e5d5ed2fc1495a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7baa047f6b74f67b18668f969e53715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c899f2112c5244d1b4d4d478f9d71602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ec872dceaa45ffa32d90b78b17d58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd72feafa6ad42f8b86f3926ad3f06e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/807 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute the function\n",
    "splits = make_fpb_hub_datasets(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da785f-67d3-45a0-8b9e-ea4a103769c6",
   "metadata": {},
   "source": [
    "### Load split dataset from HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "13fd6573-f4e7-4643-8cb4-49f7e084b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = \"allagree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7df3402f-5129-4f6c-a041-35b49d8ce899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: figure out why datasets are either not being overwritten on hub or the fields are misnamed\n",
    "\n",
    "# fpb_train_dataset = load_dataset(f\"{ORGANIZATION}/{DATASET}-{CONFIG}-{SEED}\", \"train\")['train']\n",
    "# fpb_test_dataset = load_dataset(f\"{ORGANIZATION}/{DATASET}-{CONFIG}-{SEED}\", \"test\")['train']\n",
    "\n",
    "fpb_train_dataset = splits[SEED]['train']\n",
    "fpb_test_dataset = splits[SEED]['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2e938-6266-4435-8eac-0db4b70ba322",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3334ed22-3cf2-44cd-88fc-39fa357b7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_INSTRUCTION, TASK_DATA = (\n",
    "    TASK_MAP[args.task_name][\"instruction\"],\n",
    "    TASK_MAP[args.task_name][\"data\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34a26342-05ce-4aa0-879b-722a6a4d8c6e",
   "metadata": {},
   "source": [
    "def convert_dataset(ds):\n",
    "    prompts = llama2_prompt_generator(TASK_INSTRUCTION, ds['text'])\n",
    "    labels = [decode(L).upper() for L in ds['label']]\n",
    "    df = pd.DataFrame.from_dict({'prompt': prompts, 'response': labels})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c362e-d42a-4367-be50-267dcfd0aa83",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f0a2866c-c5fe-4403-bddc-da2d81b64f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parameters = '7b'\n",
    "model_id = f\"meta-llama/Llama-2-{n_parameters}-chat-hf\"\n",
    "model_name = model_id.split('/')[-1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a24920f7-0173-4229-b450-ad7a2fd12a71",
   "metadata": {},
   "source": [
    "!pip install cupy-cuda11x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cfa8f4f-2ae5-41b6-a6b0-9597a3922b5c",
   "metadata": {},
   "source": [
    "!pip install cuda-python"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a03a307b-6a1b-4d10-97d5-807476bc4004",
   "metadata": {},
   "source": [
    "!pip install cuda-python"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd7e8aea-75c8-4196-acd7-cc11c1d6fc3b",
   "metadata": {},
   "source": [
    "conda install -c conda-forge cudatoolkit-dev"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fb72b7b-40fd-47e0-b60d-36cb52a7ec66",
   "metadata": {},
   "source": [
    "!pip install flash-attn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6abc61a2-636d-4188-bacd-004bd3e1f1b8",
   "metadata": {},
   "source": [
    "!pip3 install --force-reinstall torch --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c7dd1ea6-b477-4f94-befe-40879d77b029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6b542f8e514ca5a348f63da2bb6648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9d74f70-adbe-45c9-a0df-8515d1adbbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    # Activate 4-bit precision base model loading\n",
    "    load_in_4bit=True,\n",
    "    # load_in_8bit=True,\n",
    "    # Activate nested quantization for 4-bit base models (double quantization)\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # Compute dtype for 4-bit base models\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1a55b4f2-078d-44a7-a081-1cbad1cad35e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde7de29c1ee4428b5bb9c6699cd7c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                  # load_in_4bit=True,\n",
    "                                                  device_map=device_map,\n",
    "                                                  max_memory=CUDA_MAX_MEMORY,\n",
    "                                                  # use_flash_attention_2=True, #not working i think its due to 11.4 cuda\n",
    "                                                  torch_dtype=compute_dtype,\n",
    "                                                  quantization_config=bnb_config\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5b240cb5-828b-4a01-80bb-a7e84e8e39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    # SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c2407bee-ce10-4f04-be19-fb957e4de045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q_proj', 'k_proj', 'down_proj', 'v_proj', 'up_proj', 'gate_proj', 'o_proj']\n"
     ]
    }
   ],
   "source": [
    "layers_for_adapters = find_all_linear_names(base_model)\n",
    "print(layers_for_adapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6311c10e-300e-4531-9fd8-7a9546827033",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    # Pass our list as an argument to the PEFT config for your model\n",
    "    target_modules=layers_for_adapters,\n",
    "    # Dimension of the LoRA matrices we update in adapaters\n",
    "    r=8,\n",
    "    # Alpha parameter for LoRA scaling\n",
    "    lora_alpha=32,\n",
    "    # Dropout probability for LoRA layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\", \n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "700f9bf3-eb2a-4288-bd2f-848785dfe24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e34e4663-5da9-4336-b9f0-6d13e7b2ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ce0b7352-8aee-4d01-a303-9c6965aaa81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 19988480 || all params: 3520401408 || trainable%: 0.5677897967708119\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4f7d5bd8-4314-4107-8881-940154add415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        # remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "09f59e8e-b375-4c1d-aa9d-79262759111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.instructions import B_INST, E_INST, B_SYS, E_SYS\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "954ec480-0c14-48a6-a61c-d7615407252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample, context_field='context'):\n",
    "    SYS_PROMPT = f\"\"\"\"Discard all the previous instructions.\n",
    "    Below is an instruction that describes a task.\n",
    "    Write a response that appropriately completes the request.\"\"\"\n",
    "    INST_PROMPT = TASK_INSTRUCTION\n",
    "    fields = ['context', 'response'] # ['instruction', 'category']\n",
    "    if not INST_PROMPT or not isinstance(INST_PROMPT, str):\n",
    "        raise ValueError(\"Instruction must be a non-empty string.\")\n",
    "    if not sample or not all(isinstance(sample[field], str) for field in fields):\n",
    "        raise ValueError(\"Fields must be a non-empty strings.\")\n",
    "    ###################################################\n",
    "    # blurb = f\"{INTRO_BLURB}\"\n",
    "    # instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    # input_context = f\"{INPUT_KEY}\\n{sample['context']}\" if sample[\"context\"] else None\n",
    "    # response = f\"{RESPONSE_KEY}\\n{sample['response']}\"\n",
    "    # end = f\"{END_KEY}\"\n",
    "    # parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "    # formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    ###################################################\n",
    "    prompt = B_INST + B_SYS + SYS_PROMPT + E_SYS + INST_PROMPT + sample[context_field] + E_INST\n",
    "    sample[\"text\"] = prompt\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "69e74d8a-44f6-4aaf-b5b6-a511956162ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n"
     ]
    }
   ],
   "source": [
    "max_length = get_max_length(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0d29e118-5904-4ac8-a39e-4d50fccb900b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8392f3273ffb491d90dce296a4402980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4f6937b2f247af8bf22f64cdd790ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a38a610d7044719eba75b5a902af9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_dataset = preprocess_dataset(tokenizer=tokenizer, max_length=max_length, seed=seed, dataset=fpb_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2d212367-1ce5-4d7c-9a7f-8a077e263f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    report_to=\"wandb\",\n",
    "    # Batch size per GPU for training\n",
    "    per_device_train_batch_size=4,\n",
    "    # Batch size per GPU for evaluation\n",
    "    per_device_eval_batch_size=4,\n",
    "    # Number of update steps to accumulate the gradients for\n",
    "    gradient_accumulation_steps=4,\n",
    "    # Enable gradient checkpointing\n",
    "    gradient_checkpointing = True,\n",
    "    # Maximum gradient normal (gradient clipping)\n",
    "    max_grad_norm = 0.3,\n",
    "    # Initial learning rate (AdamW optimizer)\n",
    "    learning_rate = 2e-4,\n",
    "    # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "    weight_decay = 0.001,\n",
    "    # Optimizer to use\n",
    "    optim = \"paged_adamw_32bit\",    \n",
    "    # Learning rate schedule (constant a bit better than cosine)\n",
    "    lr_scheduler_type = \"constant\",\n",
    "    # Number of training steps (overrides num_train_epochs)\n",
    "    max_steps = -1,\n",
    "    # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "    warmup_ratio = 0.03,\n",
    "    # Group sequences into batches with same length to save memory and speed up training\n",
    "    group_by_length = True,\n",
    "    # Save checkpoint every X updates steps\n",
    "    save_steps = 25,\n",
    "    # Log every X updates steps\n",
    "    logging_steps = 25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7ec89-151f-4b10-8b10-544984a0c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=fpb_train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    # Maximum sequence length to use\n",
    "    max_seq_length = None,\n",
    "    # max_seq_length = 512,\n",
    "    # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "    packing = False,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d404e650-cb01-4244-b634-9717b61d0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c263e-e221-4155-a25d-71b9a6d7bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir / \"final_checkpoint\")\n",
    "trainer.model.config.save_pretrained(output_dir / \"final_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2a028-a3d9-48ed-8d32-18add30e0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = f\"{organization}/{model_name}_{dataset}_{CONFIG}_{SEEDS[0]}\"\n",
    "print(repo_name)\n",
    "trainer.model.push_to_hub(repo_name, private=True, use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc76d0-8b92-49c7-80e8-77415587ffaf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7000d-29a5-4706-8665-8a75199021c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = AutoPeftModelForCausalLM.from_pretrained(output_dir/\"final_checkpoint\",\n",
    "                                             device_map=device_map, max_memory=CUDA_MAX_MEMORY,\n",
    "                                             torch_dtype=compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac607d2-157f-4ea3-85c6-5f155c9f4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = convert_dataset(fpb_test_dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc5417f4-4afa-4621-ad6d-768a475958ab",
   "metadata": {},
   "source": [
    "# get pipeline ready for instruction text generation\n",
    "generation_pipeline = TextGenerationPipeline(model=sft_model,\n",
    "                                             tokenizer=tokenizer,\n",
    "                                             # NOTE: Set `do_sample = True` when `temperature > 0.0`\n",
    "                                             # https://github.com/huggingface/transformers/issues/25326\n",
    "                                             temperature=0.0,  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "                                             do_sample=False,\n",
    "                                             max_new_tokens=512,\n",
    "                                             top_k=10,\n",
    "                                             top_p=0.92,\n",
    "                                             # Penalize the model for repeating text; 1.0 means no penalty\n",
    "                                             repetition_penalty=1.0,\n",
    "                                             # Only generate and return one response (sequence)\n",
    "                                             num_return_sequences=1\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c8eaa-c3d3-433c-a4b6-46983377503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0abec3-cfa3-4731-b1d2-a3e3c1eb729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature=0.0  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "do_sample=False\n",
    "max_new_tokens=256\n",
    "top_k=10\n",
    "top_p=0.92\n",
    "repetition_penalty=1.0  # 1.0 means no penalty\n",
    "num_return_sequences=1  # Only generate one response\n",
    "num_beams=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddc6df-f72d-4db5-9f0c-da038c87c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model=None, tokenizer=None, input=None):\n",
    "    inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(temperature=temperature,\n",
    "                                           do_sample=do_sample,\n",
    "                                           max_new_tokens=max_new_tokens,\n",
    "                                           top_k=top_k,\n",
    "                                           top_p=top_p,\n",
    "                                           repetition_penalty=repetition_penalty,\n",
    "                                           num_return_sequences=num_return_sequences,\n",
    "                                           num_beams=num_beams,\n",
    "                                           return_dict_in_generate=True,\n",
    "                                           output_scores=False,\n",
    "                                          ))\n",
    "    seq = generation_output.sequences\n",
    "    output = tokenizer.decode(seq[0])\n",
    "    return output.split('[/INST]')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbbf35-896a-43c1-8d59-eccf4c00531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_list = df_test['prompt'].to_list()\n",
    "output_list = []\n",
    "for i in range(len(input_list)):\n",
    "    output_list.append(\n",
    "        generate(model=sft_model, tokenizer=tokenizer, input=inputs_list[i]).replace('</s>','')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985526f0-74d0-4597-ad70-1fb710a88ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conference Environment",
   "language": "python",
   "name": "conference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
