{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf37c31a-e659-477b-a0bc-fec3df7b1b6b",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning of Llama2 on Financial Phrasebank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1c34b-f17a-40f1-9dc4-ab7f692ef04e",
   "metadata": {},
   "source": [
    "!python -m ipykernel install --user --name=conference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79f553-61c2-46e9-90fe-306180ac37be",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d3ba20-0627-4f51-9c91-4df4c8783991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SRC_DIRECTORY = Path().cwd().resolve().parent\n",
    "\n",
    "if str(SRC_DIRECTORY) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIRECTORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03982813-c016-4780-8b67-3931384881d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('llama2_finetune')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "consoleHandler.setFormatter(formatter)\n",
    "logger.addHandler(consoleHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb97797-1ecc-498b-ae91-75692518831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4413c42-65b9-404c-9ca7-02e39d1238bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaModel, LlamaConfig, TextGenerationPipeline\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a32f77-63d4-4bcf-b19b-85cc5110ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d880d156-7894-483f-98e9-c6e74ac8d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ab82d5d-2a20-4dd5-aaa6-a4bf5d9f000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb71e7f4-3e12-446e-8648-a0828c906c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "================================================================================\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('pf=\\\\E[4i'), PosixPath('k3=\\\\EOR'), PosixPath('AB=\\\\E[4%dm'), PosixPath('sr=\\\\EM'), PosixPath('sc=\\\\E7'), PosixPath('k5=\\\\E[15~'), PosixPath('nd=\\\\E[C'), PosixPath('Co#8'), PosixPath('ks=\\\\E[?1h\\\\E='), PosixPath('AF=\\\\E[3%dm'), PosixPath('ct=\\\\E[3g'), PosixPath('xn'), PosixPath('kH=\\\\E[4~'), PosixPath('cs=\\\\E[%i%d;%dr'), PosixPath('DL=\\\\E[%dM'), PosixPath('cr=^M'), PosixPath('k;=\\\\E[21~'), PosixPath('me=\\\\E[m'), PosixPath('cd=\\\\E[J'), PosixPath('kd=\\\\EOB'), PosixPath('ue=\\\\E[24m'), PosixPath('te=\\\\E[?1049l'), PosixPath('ac=\\\\140\\\\140aaffggjjkkllmmnnooppqqrrssttuuvvwwxxyyzz{{||}}~~..--++,,hhII00'), PosixPath('ce=\\\\E[K'), PosixPath('ku=\\\\EOA'), PosixPath('kN=\\\\E[6~'), PosixPath('kI=\\\\E[2~'), PosixPath('k7=\\\\E[18~'), PosixPath('k6=\\\\E[17~'), PosixPath('F2=\\\\E[24~'), PosixPath('kh=\\\\E[1~'), PosixPath('DC=\\\\E[%dP'), PosixPath('dc=\\\\E[P'), PosixPath('k2=\\\\EOQ'), PosixPath('dl=\\\\E[M'), PosixPath('so=\\\\E[3m'), PosixPath('@1=\\\\E[1~'), PosixPath('le=^H'), PosixPath('ke=\\\\E[?1l\\\\E>'), PosixPath('k1=\\\\EOP'), PosixPath('co#105'), PosixPath('AX'), PosixPath('bt=\\\\E[Z'), PosixPath('mi'), PosixPath('us=\\\\E[4m'), PosixPath('G0'), PosixPath('SC|screen.xterm-256color|VT 100/ANSI X3.64 virtual terminal'), PosixPath('rc=\\\\E8'), PosixPath('op=\\\\E[39;49m'), PosixPath('LP'), PosixPath('ms'), PosixPath('ta=^I'), PosixPath('kB=\\\\E[Z'), PosixPath('as=\\\\E(0'), PosixPath('vi=\\\\E[?25l'), PosixPath('xv'), PosixPath('kD=\\\\E[3~'), PosixPath('cl=\\\\E[H\\\\E[J'), PosixPath('LE=\\\\E[%dD'), PosixPath('ae=\\\\E(B'), PosixPath('RI=\\\\E[%dC'), PosixPath('se=\\\\E[23m'), PosixPath('st=\\\\EH'), PosixPath('AL=\\\\E[%dL'), PosixPath('it#8'), PosixPath('Km=\\\\E[<'), PosixPath('is=\\\\E)0'), PosixPath('im=\\\\E[4h'), PosixPath('mr=\\\\E[7m'), PosixPath('kl=\\\\EOD'), PosixPath('ho=\\\\E[H'), PosixPath('up=\\\\EM'), PosixPath('kr=\\\\EOC'), PosixPath('do=^J'), PosixPath('kP=\\\\E[5~'), PosixPath('pt'), PosixPath('nw=\\\\EE'), PosixPath('po=\\\\E[5i'), PosixPath('li#83'), PosixPath('bs'), PosixPath('UP=\\\\E[%dA'), PosixPath('DO=\\\\E[%dB'), PosixPath('km'), PosixPath('am'), PosixPath('pa#64'), PosixPath('@7=\\\\E[4~'), PosixPath('vb=\\\\Eg'), PosixPath('F1=\\\\E[23~'), PosixPath('al=\\\\E[L'), PosixPath('IC=\\\\E[%d@'), PosixPath('bl=^G'), PosixPath('k9=\\\\E[20~'), PosixPath('k0=\\\\E[10~'), PosixPath('cm=\\\\E[%i%d;%dH'), PosixPath('ti=\\\\E[?1049h'), PosixPath('ve=\\\\E[34h\\\\E[?25h'), PosixPath('k4=\\\\EOS'), PosixPath('md=\\\\E[1m'), PosixPath('k8=\\\\E[19~'), PosixPath('vs=\\\\E[34l'), PosixPath('mh=\\\\E[2m'), PosixPath('mb=\\\\E[5m'), PosixPath('ei=\\\\E[4l'), PosixPath('rs=\\\\Ec')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('FILE')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/home/AD/gmatlin3/.cache/dotnet_bundle_extract')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "DEBUG: Possible options found for libcudart.so: set()\n",
      "CUDA SETUP: PyTorch settings found: CUDA_VERSION=118, Highest Compute Capability: 8.6.\n",
      "CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "CUDA SETUP: Loading binary /home/AD/gmatlin3/.conda/envs/conference/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
      "libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "CUDA SETUP: Problem: The main issue seems to be that the main CUDA runtime library was not detected.\n",
      "CUDA SETUP: Solution 1: To solve the issue the libcudart.so location needs to be added to the LD_LIBRARY_PATH variable\n",
      "CUDA SETUP: Solution 1a): Find the cuda runtime library via: find / -name libcudart.so 2>/dev/null\n",
      "CUDA SETUP: Solution 1b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_1a\n",
      "CUDA SETUP: Solution 1c): For a permanent solution add the export from 1b into your .bashrc file, located at ~/.bashrc\n",
      "CUDA SETUP: Solution 2: If no library was found in step 1a) you need to install CUDA.\n",
      "CUDA SETUP: Solution 2a): Download CUDA install script: wget https://github.com/TimDettmers/bitsandbytes/blob/main/cuda_install.sh\n",
      "CUDA SETUP: Solution 2b): Install desired CUDA version to desired location. The syntax is bash cuda_install.sh CUDA_VERSION PATH_TO_INSTALL_INTO.\n",
      "CUDA SETUP: Solution 2b): For example, \"bash cuda_install.sh 113 ~/local/\" will download CUDA 11.3 and install into the folder ~/local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AD/gmatlin3/.conda/envs/conference/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "\n",
      "  warn(msg)\n",
      "/home/AD/gmatlin3/.conda/envs/conference/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: /home/AD/gmatlin3/.conda/envs/conference did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/trl/__init__.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_seed\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextEnvironment, TextHistory\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BestOfNSampler\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_diffusers_available, is_peft_available\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     AutoModelForCausalLMWithValueHead,\n\u001b[1;32m     11\u001b[0m     AutoModelForSeq2SeqLMWithValueHead,\n\u001b[1;32m     12\u001b[0m     PreTrainedModelWrapper,\n\u001b[1;32m     13\u001b[0m     create_reference_model,\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/trl/extras/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbest_of_n_sampler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BestOfNSampler\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/trl/extras/best_of_n_sampler.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerationConfig, PreTrainedTokenizer, PreTrainedTokenizerFast\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_seed\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SUPPORTED_ARCHITECTURES, PreTrainedModelWrapper\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBestOfNSampler\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     13\u001b[0m         model: PreTrainedModelWrapper,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m         generation_config: Optional[GenerationConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/trl/models/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModelWrapper, create_reference_model\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_value_head\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead\n\u001b[1;32m     20\u001b[0m SUPPORTED_ARCHITECTURES \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     21\u001b[0m     AutoModelForCausalLMWithValueHead,\n\u001b[1;32m     22\u001b[0m     AutoModelForSeq2SeqLMWithValueHead,\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/trl/models/modeling_base.py:30\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_peft_available\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_peft_available():\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m         LoraConfig,\n\u001b[1;32m     32\u001b[0m         PeftConfig,\n\u001b[1;32m     33\u001b[0m         PeftModel,\n\u001b[1;32m     34\u001b[0m         PeftModelForCausalLM,\n\u001b[1;32m     35\u001b[0m         PeftModelForSeq2SeqLM,\n\u001b[1;32m     36\u001b[0m         PromptLearningConfig,\n\u001b[1;32m     37\u001b[0m         get_peft_model,\n\u001b[1;32m     38\u001b[0m         prepare_model_for_int8_training,\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_peft_model_state_dict\n\u001b[1;32m     42\u001b[0m LAYER_PATTERNS \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer.h.\u001b[39m\u001b[38;5;132;01m{layer}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.decoder.layers.\u001b[39m\u001b[38;5;132;01m{layer}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_neox.layers.\u001b[39m\u001b[38;5;132;01m{layer}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.layers.\u001b[39m\u001b[38;5;132;01m{layer}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     47\u001b[0m ]\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/peft/__init__.py:22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# module, but to preserve other warnings. So, don't check this module at all.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.4.0.dev0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING, PEFT_TYPE_TO_CONFIG_MAPPING, get_peft_config, get_peft_model\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     PeftModel,\n\u001b[1;32m     25\u001b[0m     PeftModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     PeftModelForQuestionAnswering,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     AdaptionPromptConfig,\n\u001b[1;32m     33\u001b[0m     AdaptionPromptModel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     PromptTuningInit,\n\u001b[1;32m     46\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/peft/mapping.py:20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Dict\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     PeftModel,\n\u001b[1;32m     22\u001b[0m     PeftModelForCausalLM,\n\u001b[1;32m     23\u001b[0m     PeftModelForQuestionAnswering,\n\u001b[1;32m     24\u001b[0m     PeftModelForSeq2SeqLM,\n\u001b[1;32m     25\u001b[0m     PeftModelForSequenceClassification,\n\u001b[1;32m     26\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     AdaLoraConfig,\n\u001b[1;32m     30\u001b[0m     AdaptionPromptConfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     PromptTuningConfig,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptLearningConfig\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/peft/peft_model.py:39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     40\u001b[0m     AdaLoraModel,\n\u001b[1;32m     41\u001b[0m     AdaptionPromptModel,\n\u001b[1;32m     42\u001b[0m     LoraModel,\n\u001b[1;32m     43\u001b[0m     PrefixEncoder,\n\u001b[1;32m     44\u001b[0m     PromptEmbedding,\n\u001b[1;32m     45\u001b[0m     PromptEncoder,\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     48\u001b[0m     SAFETENSORS_WEIGHTS_NAME,\n\u001b[1;32m     49\u001b[0m     TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     shift_tokens_right,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     65\u001b[0m PEFT_TYPE_TO_MODEL_MAPPING \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     66\u001b[0m     PeftType\u001b[38;5;241m.\u001b[39mLORA: LoraModel,\n\u001b[1;32m     67\u001b[0m     PeftType\u001b[38;5;241m.\u001b[39mPROMPT_TUNING: PromptEmbedding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     PeftType\u001b[38;5;241m.\u001b[39mADAPTION_PROMPT: AdaptionPromptModel,\n\u001b[1;32m     72\u001b[0m }\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/peft/tuners/__init__.py:21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# module, but to preserve other warnings. So, don't check this module at all\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madaption_prompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaptionPromptConfig, AdaptionPromptModel\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, LoraModel\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madalora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaLoraConfig, AdaLoraModel\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mp_tuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptEncoder, PromptEncoderConfig, PromptEncoderReparameterizationType\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/peft/tuners/lora.py:41\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     COMMON_LAYERS_PATTERN,\n\u001b[1;32m     30\u001b[0m     TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     transpose,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bnb_available():\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLoraConfig\u001b[39;00m(PeftConfig):\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    This is the configuration class to store the configuration of a [`LoraModel`].\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m            pattern is not in the common layers pattern.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/bitsandbytes/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cuda_setup, utils, research\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     MatmulLtState,\n\u001b[1;32m      9\u001b[0m     bmm_cublas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     matmul_4bit\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/bitsandbytes/research/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     switchback_bnb,\n\u001b[1;32m      4\u001b[0m     matmul_fp8_global,\n\u001b[1;32m      5\u001b[0m     matmul_fp8_mixed,\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/bitsandbytes/research/nn/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearFP8Mixed, LinearFP8Global\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/bitsandbytes/research/nn/modules.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, device, dtype, nn\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalOptimManager\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutlierTracer, find_outlier_dims\n\u001b[1;32m     11\u001b[0m T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.nn.Module\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/bitsandbytes/optim/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madagrad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adagrad, Adagrad8bit, Adagrad32bit\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam, Adam8bit, Adam32bit, PagedAdam, PagedAdam8bit, PagedAdam32bit\n",
      "File \u001b[0;32m~/.conda/envs/conference/lib/python3.8/site-packages/bitsandbytes/cextension.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mgenerate_instructions()\n\u001b[1;32m     19\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mprint_log_stack()\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m    CUDA Setup failed despite GPU being available. Please run the following command to get more information:\u001b[39m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m    python -m bitsandbytes\u001b[39m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m    Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m    to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m    and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\u001b[39m\u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m     28\u001b[0m lib\u001b[38;5;241m.\u001b[39mcadam32bit_grad_fp32 \u001b[38;5;66;03m# runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False\u001b[39;00m\n\u001b[1;32m     29\u001b[0m lib\u001b[38;5;241m.\u001b[39mget_context\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mc_void_p\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcfe766-8f48-499b-aaf4-f0de0af4c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec31578c-71a8-4616-8a8d-98d5eca8bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ce6ed-6ca7-45dd-ac24-fc4589898a75",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9140a49-bf41-499d-9d51-2f4fafb86d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Args = namedtuple(\"Args\", [\"task_name\"])\n",
    "args = Args(task_name=\"sentiment_analysis\")\n",
    "\n",
    "# TODO: move more configs into my args?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27bdbc-4654-4e63-9eee-44fa90845da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate a date string\n",
    "date_str = datetime.now().strftime(\"%y%m%d\")\n",
    "\n",
    "# Generate a short UUID\n",
    "uid = str(uuid.uuid4())[:8]\n",
    "\n",
    "# Combine\n",
    "uid = f\"{uid}_{date_str}\"\n",
    "\n",
    "print(uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235715a-7bee-4cfc-a520-9b28121777e3",
   "metadata": {},
   "source": [
    "### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb44486-b7d1-4fb6-acc5-63a51ff2a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_auth = \"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\" \n",
    "# TODO REMOVE MY TOKEN FOR THE FINAL VERSION\n",
    "# TODO PROVIDE DIRECTIONS FOR HOW TO GET HF TOKEN HERE\n",
    "\n",
    "huggingface_hub.login(token=hf_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41242074-0980-463d-90e1-47c1672e5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "organization = \"gtfintechlab\"\n",
    "dataset = \"financial_phrasebank\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d865ad-6288-47a1-bf41-6091573f09d4",
   "metadata": {},
   "source": [
    "### Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5dfb4-c2c5-4dae-a889-05c175972840",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path.home() / f\"{dataset}_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d65aff-161f-4eb2-9744-1dfaa62410f9",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a531e-0b51-472a-baaa-1c8c7c4cb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"llama2-fpb-sft\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"false\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd23b2-a1e6-4981-9d28-0865462ca4ce",
   "metadata": {},
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd3968-b578-4fbb-a9d4-3d9b407fa0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "major, _ = torch.cuda.get_device_capability()\n",
    "if major >= 8:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.bfloat16\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU does not support bfloat16: using fp=16\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ef82bf-f204-48c3-8431-d6f66c6b5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_N_GPUS = torch.cuda.device_count()\n",
    "CUDA_MAX_MEMORY = f\"{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB\"\n",
    "CUDA_MAX_MEMORY = {i: CUDA_MAX_MEMORY for i in range(CUDA_N_GPUS)}\n",
    "logger.info(\n",
    "    f\"Using k={CUDA_N_GPUS} CUDA GPUs with max memory {CUDA_MAX_MEMORY}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e95cb5-2bd6-491c-a2a3-f9df4f037e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map = \"auto\" #{\"\":0},"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d02251-0dd6-477f-97cd-81123caf08d3",
   "metadata": {},
   "source": [
    "## Financial PhraseBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f0a4e-98d1-4386-9136-1fdfc4738469",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS = (5768, 78516, 944601)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fb611-9c02-4e8b-a88e-7708987c2037",
   "metadata": {},
   "source": [
    "### Create splits on HuggingFace Hub"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7083ef7-ce89-4a35-aef2-7d0e6ff09d02",
   "metadata": {},
   "source": [
    "def make_fpb_hub_datasets(SEEDS):\n",
    "    configs = [\n",
    "        \"sentences_allagree\",\n",
    "        \"sentences_75agree\",\n",
    "        \"sentences_66agree\",\n",
    "        \"sentences_50agree\",\n",
    "    ]\n",
    "\n",
    "    for config in tqdm(configs, desc=\"Configs\"):\n",
    "        try:\n",
    "            fpb_dataset = load_dataset(\"financial_phrasebank\", config)\n",
    "            config_short = config.replace(\"sentences_\", \"\")\n",
    "\n",
    "            texts = fpb_dataset[\"train\"][\"sentence\"]\n",
    "            labels = fpb_dataset[\"train\"][\"label\"]\n",
    "\n",
    "            splits = {}\n",
    "\n",
    "            for seed in tqdm(SEEDS, desc=\"Seeds\", leave=False):\n",
    "                # Splitting the data\n",
    "                train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "                    texts, labels, test_size=0.2, random_state=seed\n",
    "                )\n",
    "\n",
    "                # Storing in the dictionary\n",
    "                splits[seed] = {\n",
    "                    \"train\": Dataset.from_dict({\"text\": train_texts, \"label\": train_labels}),\n",
    "                    \"test\": Dataset.from_dict({\"text\": test_texts, \"label\": test_labels}),\n",
    "                }\n",
    "\n",
    "                # Push to HF Hub\n",
    "                splits[seed][\"train\"].push_to_hub(\n",
    "                    f\"{organization}/{dataset}-{config_short}-{seed}\",\n",
    "                    config_name=\"train\",\n",
    "                    private=True,\n",
    "                )\n",
    "                splits[seed][\"test\"].push_to_hub(\n",
    "                    f\"{organization}/{dataset}-{config_short}-{seed}\",\n",
    "                    config_name=\"test\",\n",
    "                    private=True,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing config {config}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4ae00c5-8b5a-4733-be2b-4fd3ffc007e2",
   "metadata": {},
   "source": [
    "# Execute the function\n",
    "make_fpb_hub_datasets(SEEDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da785f-67d3-45a0-8b9e-ea4a103769c6",
   "metadata": {},
   "source": [
    "### Load split dataset from HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd6573-f4e7-4643-8cb4-49f7e084b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = \"allagree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3402f-5129-4f6c-a041-35b49d8ce899",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpb_train_dataset = load_dataset(f\"{organization}/{dataset}-{CONFIG}-{SEEDS[0]}\", \"train\")['train']\n",
    "fpb_test_dataset = load_dataset(f\"{organization}/{dataset}-{CONFIG}-{SEEDS[0]}\", \"test\")['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2e938-6266-4435-8eac-0db4b70ba322",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07169f75-8535-40a5-bdc3-c1e7a7cabd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.instructions import TASK_MAP, llama2_prompt_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b5472-3fea-47fe-b4a0-36995e801fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(label_number):\n",
    "    if label_number == 0:\n",
    "        return \"positive\"\n",
    "    elif label_number == 1:\n",
    "        return \"negative\"\n",
    "    elif label_number == 2:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label number\")\n",
    "\n",
    "TASK_INSTRUCTION, TASK_DATA = (\n",
    "    TASK_MAP[args.task_name][\"instruction\"],\n",
    "    TASK_MAP[args.task_name][\"data\"],\n",
    ")\n",
    "\n",
    "def convert_dataset(ds):\n",
    "    prompts = llama2_prompt_generator(TASK_INSTRUCTION, ds['text'])\n",
    "    labels = [decode(L).upper() for L in ds['label']]\n",
    "    df = pd.DataFrame.from_dict({'prompt': prompts, 'response': labels})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c362e-d42a-4367-be50-267dcfd0aa83",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a2866c-c5fe-4403-bddc-da2d81b64f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name = model_id.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4599d-dff1-442c-bf9f-dccedfe32ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    # Activate 4-bit precision base model loading\n",
    "    load_in_4bit=True,\n",
    "    # Activate nested quantization for 4-bit base models (double quantization)\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # Compute dtype for 4-bit base models\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd1ea6-b477-4f94-befe-40879d77b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7f0a1e-d046-401e-8c76-9150356e50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = LlamaConfig.from_pretrained(\n",
    "                            model_id,\n",
    "                            bos_token_id = 1,\n",
    "                            eos_token_id = 2,\n",
    "                            hidden_act = \"silu\",\n",
    "                            hidden_size = 8192,\n",
    "                            initializer_range = 0.02,\n",
    "                            intermediate_size = 28672,\n",
    "                            max_position_embeddings = 4096,\n",
    "                            model_type = \"llama\",\n",
    "                            num_attention_heads = 64,\n",
    "                            num_hidden_layers = 80,\n",
    "                            num_key_value_heads = 8,\n",
    "                            pretraining_tp = 1,\n",
    "                            rms_norm_eps = 1e-05,\n",
    "                            rope_scaling = None,\n",
    "                            tie_word_embeddings = False,\n",
    "                            # torch_dtype = \"float16\",\n",
    "                            # transformers_version = \"4.32.0.dev0\",\n",
    "                            use_cache = False, # TODO: double check use cache\n",
    "                            vocab_size = 32000\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30222269-b7cf-48f0-9b26-adcc3438a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55b4f2-078d-44a7-a081-1cbad1cad35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                  load_in_4bit=True,\n",
    "                                              device_map=device_map,\n",
    "                                                  max_memory=CUDA_MAX_MEMORY,\n",
    "                                                  torch_dtype=compute_dtype,\n",
    "                                              # use_auth_token=True,\n",
    "                                              quantization_config=bnb_config\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408be9f-63a6-4a67-afd3-4e5b552d7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    # offload_state_dict=True,\n",
    "    # offload_folder=\"offload\",\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f9bf3-eb2a-4288-bd2f-848785dfe24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e4663-5da9-4336-b9f0-6d13e7b2ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501b636-e562-46d0-94b6-f3c39846eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    # Alpha parameter for LoRA scaling\n",
    "    lora_alpha=16,\n",
    "    # Dropout probability for LoRA layers\n",
    "    lora_dropout=0.1,\n",
    "    # LoRA attention dimension\n",
    "    r=64,\n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb9169c5-1aad-410f-a8a9-d6a8cb1c3e3d",
   "metadata": {},
   "source": [
    "output_dir = Path.cwd() / \"llama2-fpb-sft-results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d212367-1ce5-4d7c-9a7f-8a077e263f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    report_to=\"wandb\",\n",
    "    # Batch size per GPU for training\n",
    "    per_device_train_batch_size=4,\n",
    "    # Batch size per GPU for evaluation\n",
    "    per_device_eval_batch_size=4,\n",
    "    # Number of update steps to accumulate the gradients for\n",
    "    gradient_accumulation_steps=4,\n",
    "    # Enable gradient checkpointing\n",
    "    gradient_checkpointing = True,\n",
    "    # Maximum gradient normal (gradient clipping)\n",
    "    max_grad_norm = 0.3,\n",
    "    # Initial learning rate (AdamW optimizer)\n",
    "    learning_rate = 2e-4,\n",
    "    # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "    weight_decay = 0.001,\n",
    "    # Optimizer to use\n",
    "    optim = \"paged_adamw_32bit\",    \n",
    "    # Learning rate schedule (constant a bit better than cosine)\n",
    "    lr_scheduler_type = \"constant\",\n",
    "    # Number of training steps (overrides num_train_epochs)\n",
    "    max_steps = -1,\n",
    "    # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "    warmup_ratio = 0.03,\n",
    "    # Group sequences into batches with same length to save memory and speed up training\n",
    "    group_by_length = True,\n",
    "    # Save checkpoint every X updates steps\n",
    "    save_steps = 25,\n",
    "    # Log every X updates steps\n",
    "    logging_steps = 25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7ec89-151f-4b10-8b10-544984a0c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=fpb_train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    # Maximum sequence length to use\n",
    "    max_seq_length = None,\n",
    "    # max_seq_length = 512,\n",
    "    # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "    packing = False,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d404e650-cb01-4244-b634-9717b61d0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c263e-e221-4155-a25d-71b9a6d7bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir / \"final_checkpoint\")\n",
    "trainer.model.config.save_pretrained(output_dir / \"final_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2a028-a3d9-48ed-8d32-18add30e0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = f\"{organization}/{model_name}_{dataset}_{CONFIG}_{SEEDS[0]}\"\n",
    "print(repo_name)\n",
    "trainer.model.push_to_hub(repo_name, private=True, use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc76d0-8b92-49c7-80e8-77415587ffaf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7000d-29a5-4706-8665-8a75199021c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = AutoPeftModelForCausalLM.from_pretrained(output_dir/\"final_checkpoint\",\n",
    "                                             device_map=device_map, max_memory=CUDA_MAX_MEMORY,\n",
    "                                             torch_dtype=compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac607d2-157f-4ea3-85c6-5f155c9f4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = convert_dataset(fpb_test_dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc5417f4-4afa-4621-ad6d-768a475958ab",
   "metadata": {},
   "source": [
    "# get pipeline ready for instruction text generation\n",
    "generation_pipeline = TextGenerationPipeline(model=sft_model,\n",
    "                                             tokenizer=tokenizer,\n",
    "                                             # NOTE: Set `do_sample = True` when `temperature > 0.0`\n",
    "                                             # https://github.com/huggingface/transformers/issues/25326\n",
    "                                             temperature=0.0,  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "                                             do_sample=False,\n",
    "                                             max_new_tokens=512,\n",
    "                                             top_k=10,\n",
    "                                             top_p=0.92,\n",
    "                                             # Penalize the model for repeating text; 1.0 means no penalty\n",
    "                                             repetition_penalty=1.0,\n",
    "                                             # Only generate and return one response (sequence)\n",
    "                                             num_return_sequences=1\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c8eaa-c3d3-433c-a4b6-46983377503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0abec3-cfa3-4731-b1d2-a3e3c1eb729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature=0.0  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "do_sample=False\n",
    "max_new_tokens=256\n",
    "top_k=10\n",
    "top_p=0.92\n",
    "repetition_penalty=1.0  # 1.0 means no penalty\n",
    "num_return_sequences=1  # Only generate one response\n",
    "num_beams=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddc6df-f72d-4db5-9f0c-da038c87c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model=None, tokenizer=None, input=None):\n",
    "    inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(temperature=temperature,\n",
    "                                           do_sample=do_sample,\n",
    "                                           max_new_tokens=max_new_tokens,\n",
    "                                           top_k=top_k,\n",
    "                                           top_p=top_p,\n",
    "                                           repetition_penalty=repetition_penalty,\n",
    "                                           num_return_sequences=num_return_sequences,\n",
    "                                           num_beams=num_beams,\n",
    "                                           return_dict_in_generate=True,\n",
    "                                           output_scores=False,\n",
    "                                          ))\n",
    "    seq = generation_output.sequences\n",
    "    output = tokenizer.decode(seq[0])\n",
    "    return output.split('[/INST]')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbbf35-896a-43c1-8d59-eccf4c00531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_list = df_test['prompt'].to_list()\n",
    "output_list = []\n",
    "for i in range(len(input_list)):\n",
    "    output_list.append(\n",
    "        generate(model=sft_model, tokenizer=tokenizer, input=inputs_list[i]).replace('</s>','')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985526f0-74d0-4597-ad70-1fb710a88ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conference",
   "language": "python",
   "name": "conference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
