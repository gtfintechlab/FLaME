{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf37c31a-e659-477b-a0bc-fec3df7b1b6b",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning of Llama2 on Financial Phrasebank"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "id": "5f24ab92-aae1-4082-85b4-305ee4e3badd",
   "metadata": {},
   "source": [
    "!pip install trl\n",
    "\n",
    "!pip install cupy-cuda11x\n",
    "\n",
    "!pip install cuda-python\n",
    "\n",
    "!pip install cuda-python\n",
    "\n",
    "conda install -c conda-forge cudatoolkit-dev\n",
    "\n",
    "!pip install flash-attn\n",
    "\n",
    "!pip3 install --force-reinstall torch --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79f553-61c2-46e9-90fe-306180ac37be",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1bf5f74-f79e-42fc-a778-d5a8f4eeb0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SRC_DIRECTORY = Path().cwd().resolve().parent\n",
    "\n",
    "if str(SRC_DIRECTORY) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIRECTORY))\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"llama2_finetune\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "consoleHandler.setFormatter(formatter)\n",
    "logger.addHandler(consoleHandler)\n",
    "\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import uuid\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import huggingface_hub\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from peft import (\n",
    "    AutoPeftModelForCausalLM,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LlamaConfig,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaModel,\n",
    "    LlamaTokenizer,\n",
    "    TextGenerationPipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from llama.instructions import (\n",
    "    B_INST,\n",
    "    B_SYS,\n",
    "    E_INST,\n",
    "    E_SYS,\n",
    "    TASK_MAP,\n",
    "    llama2_prompt_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ce6ed-6ca7-45dd-ac24-fc4589898a75",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b79b9f3-054d-429b-b6f9-6b3a7dc0d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_ORGANIZATION = \"gtfintechlab\"\n",
    "HF_AUTH = \"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\"\n",
    "\n",
    "TASK = \"fomc_communication\"\n",
    "\n",
    "SEEDS = (5768, 78516, 944601)\n",
    "\n",
    "SEED = SEEDS[0]\n",
    "\n",
    "device_map = \"auto\"  # {\"\":0}\n",
    "DEVICE_MAP = \"auto\"\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "\n",
    "MODEL_PARAMETERS = \"7b\"\n",
    "MODEL_ID = f\"meta-llama/Llama-2-{n_parameters}-chat-hf\"\n",
    "\n",
    "OUTPUT_DIR = Path.home() / \"results\" / f\"{model_name}_{TASK}\"\n",
    "\n",
    "repo_name = f\"{HF_ORGANIZATION}/{model_name}_{TASK}\"\n",
    "\n",
    "Args = namedtuple(\n",
    "    \"Args\",\n",
    "    [\n",
    "        \"task_name\",\n",
    "        \"seed\",\n",
    "        \"device_map\",\n",
    "        \"epochs\",\n",
    "        \"model_id\",\n",
    "        \"model_name\",\n",
    "        \"output_dir\",\n",
    "        \"organization\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "args = Args(\n",
    "    task_name=TASK,\n",
    "    seed=SEED,\n",
    "    device_map=DEVICE_MAP,\n",
    "    epochs=EPOCHS,\n",
    "    model_id=MODEL_ID,\n",
    "    model_name=MODEL_ID.split(\"/\")[-1],\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    organization=HF_ORGANIZATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235715a-7bee-4cfc-a520-9b28121777e3",
   "metadata": {},
   "source": [
    "### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb44486-b7d1-4fb6-acc5-63a51ff2a9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/AD/gmatlin3/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "huggingface_hub.login(token=HF_AUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d65aff-161f-4eb2-9744-1dfaa62410f9",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c428c4bf-1348-496d-bd72-b8c03612a895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"] = f\"{model_name}_{TASK}\"\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"fa69ffc6a97578da0410b553042cbb8b3bf5fcaf\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = f\"{model_name}_{TASK}\"\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd23b2-a1e6-4981-9d28-0865462ca4ce",
   "metadata": {},
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbcea4e9-1445-4064-b36b-2355ddca1f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 02:15:22,449 - llama2_finetune - INFO - Using k=2 CUDA GPUs with max memory {0: '41GB', 1: '41GB'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "major, _ = torch.cuda.get_device_capability()\n",
    "if major >= 8:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.bfloat16\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU does not support bfloat16: using fp=16\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.float16\n",
    "\n",
    "CUDA_N_GPUS = torch.cuda.device_count()\n",
    "CUDA_MAX_MEMORY = f\"{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB\"\n",
    "CUDA_MAX_MEMORY = {i: CUDA_MAX_MEMORY for i in range(CUDA_N_GPUS)}\n",
    "logger.info(f\"Using k={CUDA_N_GPUS} CUDA GPUs with max memory {CUDA_MAX_MEMORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c362e-d42a-4367-be50-267dcfd0aa83",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "86ee5afe-e618-44b5-972b-55ef4cb0d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uid(id_length=8, dt_format=\"%y%m%d\"):\n",
    "    date_str = datetime.now().strftime(dt_format)\n",
    "\n",
    "    # Generate a short UUID\n",
    "    uid = str(uuid.uuid4())[:id_length]\n",
    "\n",
    "    # Combine\n",
    "    uid = f\"{uid}_{date_str}\"\n",
    "\n",
    "    return uid\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    # SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    cls = (\n",
    "        bnb.nn.Linear4bit\n",
    "    )  # if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "def create_peft_config(modules):\n",
    "    peft_config = LoraConfig(\n",
    "        # Pass our list as an argument to the PEFT config for your model\n",
    "        target_modules=modules,\n",
    "        # Dimension of the LoRA matrices we update in adapaters\n",
    "        r=8,\n",
    "        # Alpha parameter for LoRA scaling\n",
    "        lora_alpha=32,\n",
    "        # Dropout probability for LoRA layers\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    return peft_config\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def create_prompt_formats(sample, context_field=\"sentence\", response_field=\"label_decoded\"):\n",
    "    SYS_PROMPT = f\"\"\"\"Discard all the previous instructions.\n",
    "    Below is an instruction that describes a task.\n",
    "    Write a response that appropriately completes the request.\"\"\"\n",
    "\n",
    "    INST_PROMPT = TASK_INSTRUCTION\n",
    "\n",
    "    if not INST_PROMPT or not isinstance(INST_PROMPT, str):\n",
    "        raise ValueError(\"Instruction must be a non-empty string.\")\n",
    "    if not sample or not all(isinstance(sample[field], str) for field in [context_field, response_field]):\n",
    "        raise ValueError(\"Fields must be a non-empty strings.\")\n",
    "\n",
    "    ###################################################\n",
    "    # blurb = f\"{INTRO_BLURB}\"\n",
    "    # instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    # input_context = f\"{INPUT_KEY}\\n{sample['context']}\" if sample[\"context\"] else None\n",
    "    # response = f\"{RESPONSE_KEY}\\n{sample['response']}\"\n",
    "    # end = f\"{END_KEY}\"\n",
    "    # parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "    # formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    ###################################################\n",
    "\n",
    "    prompt = (\n",
    "        B_INST\n",
    "        + B_SYS\n",
    "        + SYS_PROMPT\n",
    "        + E_SYS\n",
    "        + INST_PROMPT\n",
    "        + sample[context_field]\n",
    "        + E_INST\n",
    "    )\n",
    "    sample[\"text\"] = prompt\n",
    "    return sample\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed: int, dataset: Dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)  # , batched=True)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(\n",
    "        preprocess_batch, max_length=max_length, tokenizer=tokenizer\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        # remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a213e-05b6-4b88-8773-b685cca59a61",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9e1177c6-62a0-4a6a-95a9-bcaae8e5907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = generate_uid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3334ed22-3cf2-44cd-88fc-39fa357b7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_INSTRUCTION, TASK_DATA = (\n",
    "    TASK_MAP[args.task_name][\"instruction\"],\n",
    "    TASK_MAP[args.task_name][\"data\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fe74af46-7ecd-4fa9-8c02-911001080311",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(f\"{args.organization}/{args.task_name}\", str(args.seed))[\n",
    "    \"train\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "950cf3a0-489c-4c9b-96e2-440bf9544719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, gradient_checkpointing_enabled=False):\n",
    "    dataset = load_dataset(f\"{args.organization}/{args.task_name}\", str(args.seed))[\n",
    "        \"train\"\n",
    "    ]\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        # Activate 4-bit precision base model loading\n",
    "        load_in_4bit=True,\n",
    "        # Activate nested quantization for 4-bit base models (double quantization)\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        # Quantization type (fp4 or nf4)\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        # Compute dtype for 4-bit base models\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        # load_in_4bit=True,\n",
    "        device_map=args.device_map,\n",
    "        max_memory=CUDA_MAX_MEMORY,\n",
    "        # use_flash_attention_2=True, #not working i think its due to 11.4 cuda\n",
    "        torch_dtype=compute_dtype,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "    max_length = get_max_length(model)\n",
    "    preprocessed_dataset = preprocess_dataset(\n",
    "        tokenizer=tokenizer, max_length=max_length, seed=args.seed, dataset=dataset\n",
    "    )\n",
    "\n",
    "    # Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    # gradient_checkpointing_enabled will slow down the compute time, but reduce memory usage\n",
    "    if gradient_checkpointing_enabled:\n",
    "        notfailing_checkpoint = partial(\n",
    "            torch.utils.checkpoint.checkpoint, use_reentrant=False\n",
    "        )\n",
    "        torch.utils.checkpoint.checkpoint = notfailing_checkpoint\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    # Get lora module names\n",
    "    layers_for_adapters = find_all_linear_names(model)\n",
    "    print(f\"Layers for PEFT Adaptation: {layers_for_adapters}\")\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(layers_for_adapters)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    final_checkpoint_dir = args.output_dir / \"final_checkpoint\"\n",
    "    merged_checkpoint_dir = args.output_dir / \"final_merged_checkpoint\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=final_checkpoint_dir,\n",
    "        bf16=True,\n",
    "        report_to=\"wandb\",\n",
    "        # Batch size per GPU for training\n",
    "        per_device_train_batch_size=1,\n",
    "        # Batch size per GPU for evaluation\n",
    "        per_device_eval_batch_size=4,\n",
    "        # Number of update steps to accumulate the gradients for\n",
    "        gradient_accumulation_steps=4,\n",
    "        # Maximum gradient normal (gradient clipping)\n",
    "        # max_grad_norm = 0.3,\n",
    "        # Initial learning rate (AdamW optimizer)\n",
    "        learning_rate=2e-4,\n",
    "        # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "        # weight_decay = 0.001,\n",
    "        # Optimizer to use\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        # Learning rate schedule (constant a bit better than cosine)\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        # Number of training steps (overrides num_train_epochs)\n",
    "        max_steps=epochs * len(preprocessed_dataset),\n",
    "        # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "        warmup_ratio=0.05,\n",
    "        # Save checkpoint every X updates steps\n",
    "        save_steps=int(epochs * len(preprocessed_dataset) / 20),\n",
    "        # Log every X updates steps\n",
    "        logging_steps=int(epochs * len(preprocessed_dataset) / 20),\n",
    "        # Group sequences into batches with same length to save memory and speed up training\n",
    "        # group_by_length = True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=preprocessed_dataset,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = (\n",
    "        False  # re-enable for inference to speed up predictions for similar inputs\n",
    "    )\n",
    "\n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes:\n",
    "            dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items():\n",
    "        total += v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v / total)\n",
    "\n",
    "    do_train = True\n",
    "\n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "\n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)\n",
    "\n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95ecfe-ec53-4565-9e3d-af64d3653e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d07a9d22ff645a8bb84a73b451bdfb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3d939d292045188e768874eb3c32e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e404bc854c649f5bca54ab041dfc216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deef834273684bb59cba25cec088d4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers for PEFT Adaptation: ['q_proj', 'v_proj', 'up_proj', 'gate_proj', 'k_proj', 'down_proj', 'o_proj']\n"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc76d0-8b92-49c7-80e8-77415587ffaf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca28f3-0431-4848-8c0d-dc0b0c300742",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir, device_map=args.device_map, torch_dtype=compute_dtype\n",
    ")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# os.makedirs(output_merged_dir, exist_ok=True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "\n",
    "# save tokenizer for easy inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.save_pretrained(output_merged_dir)\n",
    "\n",
    "model.push_to_hub(repo_name, private=True, use_temp_dir=True)\n",
    "\n",
    "tokenizer.push_to_hub(repo_name, private=True, use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7000d-29a5-4706-8665-8a75199021c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir / \"final_checkpoint\",\n",
    "    device_map=device_map,\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    "    torch_dtype=compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac607d2-157f-4ea3-85c6-5f155c9f4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = convert_dataset(fpb_test_dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc5417f4-4afa-4621-ad6d-768a475958ab",
   "metadata": {},
   "source": [
    "# get pipeline ready for instruction text generation\n",
    "generation_pipeline = TextGenerationPipeline(model=sft_model,\n",
    "                                             tokenizer=tokenizer,\n",
    "                                             # NOTE: Set `do_sample = True` when `temperature > 0.0`\n",
    "                                             # https://github.com/huggingface/transformers/issues/25326\n",
    "                                             temperature=0.0,  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "                                             do_sample=False,\n",
    "                                             max_new_tokens=512,\n",
    "                                             top_k=10,\n",
    "                                             top_p=0.92,\n",
    "                                             # Penalize the model for repeating text; 1.0 means no penalty\n",
    "                                             repetition_penalty=1.0,\n",
    "                                             # Only generate and return one response (sequence)\n",
    "                                             num_return_sequences=1\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c8eaa-c3d3-433c-a4b6-46983377503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0abec3-cfa3-4731-b1d2-a3e3c1eb729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.0  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "do_sample = False\n",
    "max_new_tokens = 256\n",
    "top_k = 10\n",
    "top_p = 0.92\n",
    "repetition_penalty = 1.0  # 1.0 means no penalty\n",
    "num_return_sequences = 1  # Only generate one response\n",
    "num_beams = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddc6df-f72d-4db5-9f0c-da038c87c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model=None, tokenizer=None, input=None):\n",
    "    inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_beams,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "        ),\n",
    "    )\n",
    "    seq = generation_output.sequences\n",
    "    output = tokenizer.decode(seq[0])\n",
    "    return output.split(\"[/INST]\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbbf35-896a-43c1-8d59-eccf4c00531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_list = df_test[\"prompt\"].to_list()\n",
    "output_list = []\n",
    "for i in range(len(input_list)):\n",
    "    output_list.append(\n",
    "        generate(model=sft_model, tokenizer=tokenizer, input=inputs_list[i]).replace(\n",
    "            \"</s>\", \"\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985526f0-74d0-4597-ad70-1fb710a88ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conference Environment",
   "language": "python",
   "name": "conference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
