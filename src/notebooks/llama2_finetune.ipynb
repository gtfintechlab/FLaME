{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf37c31a-e659-477b-a0bc-fec3df7b1b6b",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning of Llama2 on Financial Phrasebank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79f553-61c2-46e9-90fe-306180ac37be",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d3ba20-0627-4f51-9c91-4df4c8783991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SRC_DIRECTORY = Path().cwd().resolve().parent\n",
    "\n",
    "if str(SRC_DIRECTORY) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIRECTORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03982813-c016-4780-8b67-3931384881d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('llama2_finetune')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "consoleHandler.setFormatter(formatter)\n",
    "logger.addHandler(consoleHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb97797-1ecc-498b-ae91-75692518831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4413c42-65b9-404c-9ca7-02e39d1238bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaModel, LlamaConfig, TextGenerationPipeline\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a32f77-63d4-4bcf-b19b-85cc5110ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d880d156-7894-483f-98e9-c6e74ac8d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97d6ab1-8981-4ef2-afe5-4d95a8a7472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ab82d5d-2a20-4dd5-aaa6-a4bf5d9f000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09921958-5d8e-48f6-b6a1-511aa17af7a6",
   "metadata": {},
   "source": [
    "!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d15aee4-0130-4fb6-85cb-d8daec92d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbcfe766-8f48-499b-aaf4-f0de0af4c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec31578c-71a8-4616-8a8d-98d5eca8bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30222269-b7cf-48f0-9b26-adcc3438a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07169f75-8535-40a5-bdc3-c1e7a7cabd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.instructions import TASK_MAP, llama2_prompt_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a50701a-b12a-46e6-a90f-01549cb05f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea22f0a0-70b3-4f80-b922-a664ab971570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4895e04-3acf-4c95-accd-d654c3bc5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ce6ed-6ca7-45dd-ac24-fc4589898a75",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9140a49-bf41-499d-9d51-2f4fafb86d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple(\"Args\", [\"task_name\"])\n",
    "args = Args(task_name=\"sentiment_analysis\")\n",
    "\n",
    "# TODO: move more configs into my args?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c27bdbc-4654-4e63-9eee-44fa90845da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7a029f70_231008\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate a date string\n",
    "date_str = datetime.now().strftime(\"%y%m%d\")\n",
    "\n",
    "# Generate a short UUID\n",
    "uid = str(uuid.uuid4())[:8]\n",
    "\n",
    "# Combine\n",
    "uid = f\"{uid}_{date_str}\"\n",
    "\n",
    "print(uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235715a-7bee-4cfc-a520-9b28121777e3",
   "metadata": {},
   "source": [
    "### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeb44486-b7d1-4fb6-acc5-63a51ff2a9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/AD/gmatlin3/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "hf_auth = \"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\" \n",
    "# TODO REMOVE MY TOKEN FOR THE FINAL VERSION\n",
    "# TODO PROVIDE DIRECTIONS FOR HOW TO GET HF TOKEN HERE\n",
    "\n",
    "huggingface_hub.login(token=hf_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41242074-0980-463d-90e1-47c1672e5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORGANIZATION = \"gtfintechlab\"\n",
    "DATASET = \"financial_phrasebank\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d65aff-161f-4eb2-9744-1dfaa62410f9",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd1a531e-0b51-472a-baaa-1c8c7c4cb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"llama2-fpb-sft\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"false\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"]=\"fa69ffc6a97578da0410b553042cbb8b3bf5fcaf\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"]=\"llama2_finetune_fpb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be8c0b58-30c5-4aa3-8c7f-95e47406d455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd23b2-a1e6-4981-9d28-0865462ca4ce",
   "metadata": {},
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4270d1a-aab8-408d-86ec-0af14b6b38e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "major, _ = torch.cuda.get_device_capability()\n",
    "if major >= 8:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.bfloat16\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU does not support bfloat16: using fp=16\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61ef82bf-f204-48c3-8431-d6f66c6b5a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 00:44:14,268 - llama2_finetune - INFO - Using k=2 CUDA GPUs with max memory {0: '41GB', 1: '41GB'}\n"
     ]
    }
   ],
   "source": [
    "CUDA_N_GPUS = torch.cuda.device_count()\n",
    "CUDA_MAX_MEMORY = f\"{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB\"\n",
    "CUDA_MAX_MEMORY = {i: CUDA_MAX_MEMORY for i in range(CUDA_N_GPUS)}\n",
    "logger.info(\n",
    "    f\"Using k={CUDA_N_GPUS} CUDA GPUs with max memory {CUDA_MAX_MEMORY}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c5e95cb5-2bd6-491c-a2a3-f9df4f037e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_map = {\"\":0}\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d02251-0dd6-477f-97cd-81123caf08d3",
   "metadata": {},
   "source": [
    "## Financial PhraseBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "375f0a4e-98d1-4386-9136-1fdfc4738469",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS = (5768, 78516, 944601)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cd74d75-108f-4802-8e31-a782bd9177a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = SEEDS[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fb611-9c02-4e8b-a88e-7708987c2037",
   "metadata": {},
   "source": [
    "### Create splits on HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3518f075-bcb2-4d56-8605-ba1d383cfbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(label_number):\n",
    "    if label_number == 0:\n",
    "        return \"POSITIVE\"\n",
    "    elif label_number == 1:\n",
    "        return \"NEGATIVE\"\n",
    "    elif label_number == 2:\n",
    "        return \"NEUTRAL\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c2cf2a84-d424-4eb6-a225-f2363fdd7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fpb_hub_datasets(seed=None):\n",
    "    configs = [\n",
    "        \"sentences_allagree\",\n",
    "        \"sentences_75agree\",\n",
    "        \"sentences_66agree\",\n",
    "        \"sentences_50agree\",\n",
    "    ]\n",
    "\n",
    "    for config in tqdm(configs, desc=\"Configs\"):\n",
    "        try:\n",
    "            fpb_dataset = load_dataset(\"financial_phrasebank\", config)\n",
    "            config_short = config.replace(\"sentences_\", \"\")\n",
    "\n",
    "            texts = fpb_dataset[\"train\"][\"sentence\"]\n",
    "            labels = fpb_dataset[\"train\"][\"label\"]\n",
    "\n",
    "            splits = {}\n",
    "\n",
    "            # Splitting the data\n",
    "            train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "                texts, labels, test_size=0.2, random_state=seed\n",
    "            )\n",
    "\n",
    "            # Storing in the dictionary\n",
    "            splits[seed] = {\n",
    "                \"train\": Dataset.from_dict({\"context\": train_texts, \"response\": list(map(decode,train_labels))}),\n",
    "                \"test\": Dataset.from_dict({\"context\": test_texts, \"response\": list(map(decode, test_labels))}),\n",
    "            }\n",
    "\n",
    "            # Push to HF Hub\n",
    "            splits[seed][\"train\"].push_to_hub(\n",
    "                f\"{ORGANIZATION}/{DATASET}-{config_short}-{seed}\",\n",
    "                config_name=\"train\",\n",
    "                private=True,\n",
    "            )\n",
    "            splits[seed][\"test\"].push_to_hub(\n",
    "                f\"{ORGANIZATION}/{DATASET}-{config_short}-{seed}\",\n",
    "                config_name=\"test\",\n",
    "                private=True,\n",
    "            )\n",
    "\n",
    "            return splits\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing config {config}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e755781-518d-4f99-8cab-a9766bde132f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615c6c3d4bd940f499f8a0e7958c4898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Configs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321bc93d571e46878fd4fb9f8be75368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683693180f404909bd9b952906e38011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/803 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15504c9e886a444ca6f4d94fe8d89914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute the function\n",
    "splits = make_fpb_hub_datasets(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da785f-67d3-45a0-8b9e-ea4a103769c6",
   "metadata": {},
   "source": [
    "### Load split dataset from HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13fd6573-f4e7-4643-8cb4-49f7e084b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = \"allagree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7df3402f-5129-4f6c-a041-35b49d8ce899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: figure out why datasets are either not being overwritten on hub or the fields are misnamed\n",
    "\n",
    "# fpb_train_dataset = load_dataset(f\"{ORGANIZATION}/{DATASET}-{CONFIG}-{SEED}\", \"train\")['train']\n",
    "# fpb_test_dataset = load_dataset(f\"{ORGANIZATION}/{DATASET}-{CONFIG}-{SEED}\", \"test\")['train']\n",
    "\n",
    "fpb_train_dataset = splits[SEED]['train']\n",
    "fpb_test_dataset = splits[SEED]['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2e938-6266-4435-8eac-0db4b70ba322",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3334ed22-3cf2-44cd-88fc-39fa357b7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_INSTRUCTION, TASK_DATA = (\n",
    "    TASK_MAP[args.task_name][\"instruction\"],\n",
    "    TASK_MAP[args.task_name][\"data\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34a26342-05ce-4aa0-879b-722a6a4d8c6e",
   "metadata": {},
   "source": [
    "def convert_dataset(ds):\n",
    "    prompts = llama2_prompt_generator(TASK_INSTRUCTION, ds['text'])\n",
    "    labels = [decode(L).upper() for L in ds['label']]\n",
    "    df = pd.DataFrame.from_dict({'prompt': prompts, 'response': labels})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c362e-d42a-4367-be50-267dcfd0aa83",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a24920f7-0173-4229-b450-ad7a2fd12a71",
   "metadata": {},
   "source": [
    "!pip install cupy-cuda11x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cfa8f4f-2ae5-41b6-a6b0-9597a3922b5c",
   "metadata": {},
   "source": [
    "!pip install cuda-python"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a03a307b-6a1b-4d10-97d5-807476bc4004",
   "metadata": {},
   "source": [
    "!pip install cuda-python"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd7e8aea-75c8-4196-acd7-cc11c1d6fc3b",
   "metadata": {},
   "source": [
    "conda install -c conda-forge cudatoolkit-dev"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fb72b7b-40fd-47e0-b60d-36cb52a7ec66",
   "metadata": {},
   "source": [
    "!pip install flash-attn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6abc61a2-636d-4188-bacd-004bd3e1f1b8",
   "metadata": {},
   "source": [
    "!pip3 install --force-reinstall torch --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b240cb5-828b-4a01-80bb-a7e84e8e39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    # SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e34e4663-5da9-4336-b9f0-6d13e7b2ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6311c10e-300e-4531-9fd8-7a9546827033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    peft_config = LoraConfig(\n",
    "        # Pass our list as an argument to the PEFT config for your model\n",
    "        target_modules=modules,\n",
    "        # Dimension of the LoRA matrices we update in adapaters\n",
    "        r=8,\n",
    "        # Alpha parameter for LoRA scaling\n",
    "        lora_alpha=32,\n",
    "        # Dropout probability for LoRA layers\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\", \n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "        )\n",
    "    return peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f7d5bd8-4314-4107-8881-940154add415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        # remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "954ec480-0c14-48a6-a61c-d7615407252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample, context_field='context'):\n",
    "    SYS_PROMPT = f\"\"\"\"Discard all the previous instructions.\n",
    "    Below is an instruction that describes a task.\n",
    "    Write a response that appropriately completes the request.\"\"\"\n",
    "    INST_PROMPT = TASK_INSTRUCTION\n",
    "    fields = ['context', 'response'] # ['instruction', 'category']\n",
    "    if not INST_PROMPT or not isinstance(INST_PROMPT, str):\n",
    "        raise ValueError(\"Instruction must be a non-empty string.\")\n",
    "    if not sample or not all(isinstance(sample[field], str) for field in fields):\n",
    "        raise ValueError(\"Fields must be a non-empty strings.\")\n",
    "    ###################################################\n",
    "    # blurb = f\"{INTRO_BLURB}\"\n",
    "    # instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    # input_context = f\"{INPUT_KEY}\\n{sample['context']}\" if sample[\"context\"] else None\n",
    "    # response = f\"{RESPONSE_KEY}\\n{sample['response']}\"\n",
    "    # end = f\"{END_KEY}\"\n",
    "    # parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "    # formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    ###################################################\n",
    "    prompt = B_INST + B_SYS + SYS_PROMPT + E_SYS + INST_PROMPT + sample[context_field] + E_INST\n",
    "    sample[\"text\"] = prompt\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b0c0b08c-5338-4d8a-8d58-e75009a0674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "09f59e8e-b375-4c1d-aa9d-79262759111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.instructions import B_INST, E_INST, B_SYS, E_SYS\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f0a2866c-c5fe-4403-bddc-da2d81b64f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parameters = '7b'\n",
    "model_id = f\"meta-llama/Llama-2-{n_parameters}-chat-hf\"\n",
    "model_name = model_id.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "950cf3a0-489c-4c9b-96e2-440bf9544719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_id, dataset, seed, output_dir, gradient_checkpointing_enabled=True, epochs=5):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        # Activate 4-bit precision base model loading\n",
    "        load_in_4bit=True,\n",
    "        # Activate nested quantization for 4-bit base models (double quantization)\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        # Quantization type (fp4 or nf4)\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        # Compute dtype for 4-bit base models\n",
    "        bnb_4bit_compute_dtype=compute_dtype\n",
    "        )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                  # load_in_4bit=True,\n",
    "                                                  device_map=device_map,\n",
    "                                                  max_memory=CUDA_MAX_MEMORY,\n",
    "                                                  # use_flash_attention_2=True, #not working i think its due to 11.4 cuda\n",
    "                                                  torch_dtype=compute_dtype,\n",
    "                                                  quantization_config=bnb_config\n",
    "                                                 )\n",
    "    max_length = get_max_length(model)\n",
    "    preprocessed_dataset = preprocess_dataset(tokenizer=tokenizer,\n",
    "                                              max_length=max_length,\n",
    "                                              seed=seed,\n",
    "                                              dataset=dataset)\n",
    "    \n",
    "    # Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    # gradient_checkpointing_enabled will slow down the compute time, but reduce memory usage\n",
    "    if gradient_checkpointing_enabled:\n",
    "        notfailing_checkpoint = partial(torch.utils.checkpoint.checkpoint, use_reentrant=False)\n",
    "        torch.utils.checkpoint.checkpoint = notfailing_checkpoint\n",
    "        model.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    # Get lora module names\n",
    "    layers_for_adapters = find_all_linear_names(model)\n",
    "    print(f\"Layers for PEFT Adaptation: {layers_for_adapters}\")\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(layers_for_adapters)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        bf16=True,\n",
    "        report_to=\"wandb\",\n",
    "\n",
    "        # Batch size per GPU for training\n",
    "        per_device_train_batch_size=1,\n",
    "        # Batch size per GPU for evaluation\n",
    "        per_device_eval_batch_size=4,\n",
    "        \n",
    "        # Number of update steps to accumulate the gradients for\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        # Maximum gradient normal (gradient clipping)\n",
    "        # max_grad_norm = 0.3,\n",
    "\n",
    "        # Initial learning rate (AdamW optimizer)\n",
    "        learning_rate = 2e-4,\n",
    "\n",
    "        # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "        # weight_decay = 0.001,\n",
    "        \n",
    "        # Optimizer to use\n",
    "        optim = \"paged_adamw_8bit\",    \n",
    "\n",
    "        # Learning rate schedule (constant a bit better than cosine)\n",
    "        lr_scheduler_type = \"constant\",\n",
    "        \n",
    "        # Number of training steps (overrides num_train_epochs)\n",
    "        max_steps = epochs*len(preprocessed_dataset),\n",
    "        \n",
    "        # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "        warmup_ratio = 0.05,\n",
    "        \n",
    "        # Save checkpoint every X updates steps\n",
    "        save_steps = int(epochs*len(preprocessed_dataset)/20),\n",
    "\n",
    "        # Log every X updates steps\n",
    "        logging_steps = int(epochs*len(preprocessed_dataset)/20),\n",
    "        \n",
    "        # Group sequences into batches with same length to save memory and speed up training\n",
    "        # group_by_length = True,\n",
    "        )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=preprocessed_dataset,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "     \n",
    "    do_train = True\n",
    "    \n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ee288d40-d361-48dd-a4fd-c1cd3e3b2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path.home() / \"results\" / f\"{model_name}-{DATASET}\" / \"final_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c6c1b4d7-a3cc-4a30-94e1-344e2f0f55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40fa00-8f1c-454e-b1f2-3794b2bf204a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827a3d35e93046b587620f2878f02914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b6fdbd07534593a873aee7a431120f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00eb9fdcb7404c0981bc616e297f284c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63711a31242c4197b4ea6626e8993fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers for PEFT Adaptation: ['v_proj', 'o_proj', 'down_proj', 'k_proj', 'up_proj', 'q_proj', 'gate_proj']\n",
      "trainable params: 19988480 || all params: 3520401408 || trainable%: 0.5677897967708119\n",
      "torch.float32 282398720 0.08021776134910578\n",
      "torch.uint8 3238002688 0.9197822386508943\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='253' max='9055' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 253/9055 11:37 < 6:47:41, 0.36 it/s, Epoch 0.56/21]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model_id=model_id, dataset=fpb_train_dataset,\n",
    "      seed=SEED, output_dir=output_dir, gradient_checkpointing_enabled=False, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "76d6d748-9bc2-4791-b5dd-b73c7f860269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/AD/gmatlin3/results/Llama-2-7b-chat-hf-financial_phrasebank')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_merged_dir = Path.home() / \"results\" / f\"{model_name}-{DATASET}\" / \"final_merged_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf85f44-ab64-4425-9858-462f9e2981de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# os.makedirs(output_merged_dir, exist_ok=True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "\n",
    "# save tokenizer for easy inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2a028-a3d9-48ed-8d32-18add30e0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = f\"{organization}/{model_name}_{dataset}_{CONFIG}_{SEEDS[0]}\"\n",
    "print(repo_name)\n",
    "trainer.model.push_to_hub(repo_name, private=True, use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc76d0-8b92-49c7-80e8-77415587ffaf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7000d-29a5-4706-8665-8a75199021c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = AutoPeftModelForCausalLM.from_pretrained(output_dir/\"final_checkpoint\",\n",
    "                                             device_map=device_map, max_memory=CUDA_MAX_MEMORY,\n",
    "                                             torch_dtype=compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac607d2-157f-4ea3-85c6-5f155c9f4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = convert_dataset(fpb_test_dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc5417f4-4afa-4621-ad6d-768a475958ab",
   "metadata": {},
   "source": [
    "# get pipeline ready for instruction text generation\n",
    "generation_pipeline = TextGenerationPipeline(model=sft_model,\n",
    "                                             tokenizer=tokenizer,\n",
    "                                             # NOTE: Set `do_sample = True` when `temperature > 0.0`\n",
    "                                             # https://github.com/huggingface/transformers/issues/25326\n",
    "                                             temperature=0.0,  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "                                             do_sample=False,\n",
    "                                             max_new_tokens=512,\n",
    "                                             top_k=10,\n",
    "                                             top_p=0.92,\n",
    "                                             # Penalize the model for repeating text; 1.0 means no penalty\n",
    "                                             repetition_penalty=1.0,\n",
    "                                             # Only generate and return one response (sequence)\n",
    "                                             num_return_sequences=1\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c8eaa-c3d3-433c-a4b6-46983377503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0abec3-cfa3-4731-b1d2-a3e3c1eb729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature=0.0  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "do_sample=False\n",
    "max_new_tokens=256\n",
    "top_k=10\n",
    "top_p=0.92\n",
    "repetition_penalty=1.0  # 1.0 means no penalty\n",
    "num_return_sequences=1  # Only generate one response\n",
    "num_beams=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddc6df-f72d-4db5-9f0c-da038c87c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model=None, tokenizer=None, input=None):\n",
    "    inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(temperature=temperature,\n",
    "                                           do_sample=do_sample,\n",
    "                                           max_new_tokens=max_new_tokens,\n",
    "                                           top_k=top_k,\n",
    "                                           top_p=top_p,\n",
    "                                           repetition_penalty=repetition_penalty,\n",
    "                                           num_return_sequences=num_return_sequences,\n",
    "                                           num_beams=num_beams,\n",
    "                                           return_dict_in_generate=True,\n",
    "                                           output_scores=False,\n",
    "                                          ))\n",
    "    seq = generation_output.sequences\n",
    "    output = tokenizer.decode(seq[0])\n",
    "    return output.split('[/INST]')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbbf35-896a-43c1-8d59-eccf4c00531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_list = df_test['prompt'].to_list()\n",
    "output_list = []\n",
    "for i in range(len(input_list)):\n",
    "    output_list.append(\n",
    "        generate(model=sft_model, tokenizer=tokenizer, input=inputs_list[i]).replace('</s>','')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985526f0-74d0-4597-ad70-1fb710a88ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conference Environment",
   "language": "python",
   "name": "conference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
