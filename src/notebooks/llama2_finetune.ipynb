{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85181dd-217b-47bd-a845-a82afe80e1db",
   "metadata": {},
   "source": [
    "change to using https://huggingface.co/datasets/gtfintechlab/fomc-example-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c0e3b-3e58-477b-a3dd-768a2857671f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Not for Public\n",
    "\n",
    "***!!!TODO!!!***\n",
    "- remove wandb logging from conference notebook, switch to tensorboard\n",
    "- Determine if its possible to just checkpoint and log adapters to wandb\n",
    "- MAKE CONFERENCE NOTEBOOK GENERIC HF WITH NO TOKEN, USE PUBLIC DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a55ce84-8471-47c8-95a8-242e32ab633f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/AD/gmatlin3/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import huggingface_hub\n",
    "\n",
    "HF_AUTH = \"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\"\n",
    "huggingface_hub.login(token=HF_AUTH)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "116e16d5-acf6-4a46-97b0-0322ec4ea901",
   "metadata": {
    "tags": []
   },
   "source": [
    "import wandb\n",
    "\n",
    "WANDB_PROJECT = f\"llama2_sft_fomc\"\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"] = WANDB_PROJECT\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"false\" # dont log any models\n",
    "# os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" # log all model checkpoints\n",
    "\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"fa69ffc6a97578da0410b553042cbb8b3bf5fcaf\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = f\"llama2_sft\"\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "# # Convert Args namedtuple to dictionary\n",
    "# args_dict = args._asdict()\n",
    "\n",
    "# run = wandb.init(\n",
    "#     project=WANDB_PROJECT,\n",
    "#     config=args_dict  # Passing the converted dictionary as config\n",
    "# )\n",
    "\n",
    "import uuid\n",
    "\n",
    "def generate_uid(id_length=8, dt_format=\"%y%m%d\"):\n",
    "    date_str = datetime.now().strftime(dt_format)\n",
    "\n",
    "    # Generate a short UUID\n",
    "    uid = str(uuid.uuid4())[:id_length]\n",
    "\n",
    "    # Combine\n",
    "    uid = f\"{uid}_{date_str}\"\n",
    "\n",
    "    return uid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbb20f-cd8a-40fd-bdf4-019313ce6fae",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning of Llama2 on FOMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2424d-31dc-4d8e-a71f-8087ec428ee6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79f553-61c2-46e9-90fe-306180ac37be",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ae58483-374e-4591-ac0a-0f13d92e2b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "540c80a3-1c30-42aa-b5b3-48f5262a8fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "SRC_DIRECTORY = Path().cwd().resolve().parent\n",
    "\n",
    "if str(SRC_DIRECTORY) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIRECTORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a84af4a-da5d-4192-bac5-2ef2e764bb1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "hf_logging.set_verbosity(hf_logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(\"llama2_finetune\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create handlers\n",
    "c_handler = logging.StreamHandler()\n",
    "f_handler = logging.FileHandler('llama2_finetune.log')\n",
    "c_handler.setLevel(logging.DEBUG)\n",
    "f_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "# Create formatters and add it to handlers\n",
    "format = '%(name)s - %(levelname)s - %(message)s'\n",
    "c_format = logging.Formatter(format)\n",
    "f_format = logging.Formatter(format)\n",
    "c_handler.setFormatter(c_format)\n",
    "f_handler.setFormatter(f_format)\n",
    "\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(c_handler)\n",
    "logger.addHandler(f_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acfe49ed-209e-4ec7-97c4-3ae2abdc909b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from peft import (\n",
    "    AutoPeftModelForCausalLM,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LlamaConfig,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaModel,\n",
    "    LlamaTokenizer,\n",
    "    TextGenerationPipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# Related to HuggingFace's Tokenizers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ab8382d-43cf-484a-a0f8-62b791fdbb52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "class PeftSavingCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        checkpoint_path = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "        kwargs[\"model\"].save_pretrained(checkpoint_path)\n",
    "\n",
    "        if \"pytorch_model.bin\" in os.listdir(checkpoint_path):\n",
    "            os.remove(os.path.join(checkpoint_path, \"pytorch_model.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ce6ed-6ca7-45dd-ac24-fc4589898a75",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ad4bf00-8a2a-40dc-844e-56d307e59e72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - Using k=2 CUDA GPUs with max memory {0: '38GB', 1: '38GB'}\n",
      "llama2_finetune - INFO - Using k=2 CUDA GPUs with max memory {0: '38GB', 1: '38GB'}\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# User parameters\n",
    "################################################################################\n",
    "organization = \"gtfintechlab\"\n",
    "report_to=\"tensorboard\"\n",
    "logging_dir=\"/home/AD/gmatlin3/tensorboard/logs\"\n",
    "\n",
    "################################################################################\n",
    "# Task parameters\n",
    "################################################################################\n",
    "task_name = \"fomc_communication\"\n",
    "seeds = (5768, 78516, 944601)\n",
    "seed = seeds[0]\n",
    "\n",
    "################################################################################\n",
    "# Model parameters\n",
    "################################################################################\n",
    "model_parameters = \"7b\"\n",
    "model_id = f\"meta-llama/Llama-2-{model_parameters}-chat-hf\"\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "\n",
    "################################################################################\n",
    "# Prompt parameters\n",
    "################################################################################\n",
    "system_prompt = f\"\"\"Discard all the previous instructions.\n",
    "Below is an instruction that describes a task.\n",
    "Write a response that appropriately completes the request.\n",
    "\"\"\"\n",
    "\n",
    "instruction_prompt = f\"\"\"Behave like you are an expert sentence classifier.\n",
    "Classify the following sentence from FOMC into 'HAWKISH', 'DOVISH', or 'NEUTRAL' class.\n",
    "Label 'HAWKISH' if it is corresponding to tightening of the monetary policy.\n",
    "Label 'DOVISH' if it is corresponding to easing of the monetary policy.\n",
    "Label 'NEUTRAL' if the stance is neutral.\n",
    "Provide a single label from the choices 'HAWKISH', 'DOVISH', or 'NEUTRAL' then stop generating text.\n",
    "\n",
    "The sentence: \"\n",
    "\"\"\"\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "BOS, EOS = \"<s>\", \"</s>\"\n",
    "\n",
    "repo_name = f\"{organization}/{model_name}_{task_name}\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "neftune_noise_alpha = 5\n",
    "\n",
    "################################################################################\n",
    "# CUDA Parameters\n",
    "################################################################################\n",
    "\n",
    "# Enable fp16/bf16 training\n",
    "compute_dtype = torch.bfloat16\n",
    "fp16, bf16 = False, True\n",
    "\n",
    "CUDA_N_GPUS = torch.cuda.device_count()\n",
    "CUDA_MAX_MEMORY = f\"{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB\"\n",
    "CUDA_MAX_MEMORY = {i: CUDA_MAX_MEMORY for i in range(CUDA_N_GPUS)}\n",
    "logger.info(f\"Using k={CUDA_N_GPUS} CUDA GPUs with max memory {CUDA_MAX_MEMORY}\")\n",
    "\n",
    "# device_map = {\"\": 0} # Load the entire model on the GPU 0\n",
    "device_map = \"auto\" # Automatically determine the device map\n",
    "\n",
    "save_safetensors = True\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate 8-bit precision base model loading\n",
    "load_in_8bit = False\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = compute_dtype\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = Path(f\"/fintech_3/20231018/results/{model_name}_{task_name}\")\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 8\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 8\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = False\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"adamw_bnb_8bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 200\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "load_best_model_at_end = True\n",
    "\n",
    "strategy=\"steps\"\n",
    "save_strategy=strategy\n",
    "logging_strategy=strategy\n",
    "evaluation_strategy=strategy\n",
    "\n",
    "disable_tqdm=True\n",
    "predict_with_generate=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b79b9f3-054d-429b-b6f9-6b3a7dc0d88d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Args = namedtuple(\n",
    "    \"Args\",\n",
    "    [\n",
    "        \"task_name\",\n",
    "        \"system_prompt\",\n",
    "        \"instruction_prompt\",\n",
    "        \"seed\",\n",
    "        \"model_id\",\n",
    "        \"model_name\",\n",
    "        \"organization\",\n",
    "        \"lora_r\",\n",
    "        \"lora_alpha\",\n",
    "        \"lora_dropout\",\n",
    "        \"max_seq_length\",\n",
    "        \"packing\",\n",
    "        \"device_map\",\n",
    "        \"load_in_4bit\",\n",
    "        \"load_in_8bit\",\n",
    "        \"bnb_4bit_compute_dtype\",\n",
    "        \"bnb_4bit_use_double_quant\",\n",
    "        \"bnb_4bit_quant_type\",\n",
    "        \"output_dir\",\n",
    "        \"num_train_epochs\",\n",
    "        \"fp16\",\n",
    "        \"bf16\",\n",
    "        \"per_device_train_batch_size\",\n",
    "        \"per_device_eval_batch_size\",\n",
    "        \"gradient_accumulation_steps\",\n",
    "        \"gradient_checkpointing\",\n",
    "        \"max_grad_norm\",\n",
    "        \"learning_rate\",\n",
    "        \"weight_decay\",\n",
    "        \"optim\",\n",
    "        \"lr_scheduler_type\",\n",
    "        \"max_steps\",\n",
    "        \"warmup_ratio\",\n",
    "        \"group_by_length\",\n",
    "        \"save_steps\",\n",
    "        \"save_strategy\",\n",
    "        \"logging_strategy\",\n",
    "        \"logging_steps\",\n",
    "        \"evaluation_strategy\",\n",
    "        \"neftune_noise_alpha\",\n",
    "        \"save_safetensors\",\n",
    "        \"load_best_model_at_end\",\n",
    "        \"disable_tqdm\",\n",
    "        \"B_INST\",\n",
    "        \"E_INST\",\n",
    "        \"B_SYS\",\n",
    "        \"E_SYS\",\n",
    "        \"BOS\",\n",
    "        \"EOS\",\n",
    "        \"report_to\",\n",
    "        \"logging_dir\",\n",
    "        \"predict_with_generate\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "args = Args(\n",
    "    task_name=task_name,\n",
    "    system_prompt = system_prompt,\n",
    "    instruction_prompt = instruction_prompt,\n",
    "    seed=seed,\n",
    "    model_id=model_id,\n",
    "    model_name=model_id.split(\"/\")[-1],\n",
    "    organization=organization,\n",
    "    lora_r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=packing,\n",
    "    device_map=device_map,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    optim=optim,\n",
    "    lr_scheduler_type = lr_scheduler_type,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio= warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    save_steps=save_steps,\n",
    "    save_strategy=save_strategy,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    logging_strategy=logging_strategy,\n",
    "    logging_steps=logging_steps,\n",
    "    neftune_noise_alpha=neftune_noise_alpha,\n",
    "    save_safetensors=save_safetensors,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    "    disable_tqdm=disable_tqdm,\n",
    "    B_INST = B_INST,\n",
    "    E_INST = E_INST,\n",
    "    B_SYS = B_SYS,\n",
    "    E_SYS = E_SYS,\n",
    "    BOS = BOS,\n",
    "    EOS = EOS,\n",
    "    report_to=report_to,\n",
    "    logging_dir=logging_dir,\n",
    "    predict_with_generate=predict_with_generate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e204ad9-e27c-4b01-9ff8-1182075ab098",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e5dcf21-6cb5-44d9-9683-5aa41e17133e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_trainable_parameters(model, logger):\n",
    "    \"\"\"\n",
    "    Logs the number of trainable parameters in the model.\n",
    "    \n",
    "    Parameters:\n",
    "    model : torch.nn.Module\n",
    "        The model whose parameters we want to log.\n",
    "    logger : logging.Logger\n",
    "        Logger object to log the information.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    # Logging the information instead of printing\n",
    "    logger.info(\n",
    "        f\"Trainable params: {trainable_params} || All params: {all_params} || Trainable%: {100 * trainable_params / all_params}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def log_dtypes(model, logger):\n",
    "    \"\"\"\n",
    "    Logs the data types of the model parameters and their proportions.\n",
    "    \n",
    "    Parameters:\n",
    "    model : torch.nn.Module\n",
    "        The model whose parameter data types we want to log.\n",
    "    logger : logging.Logger\n",
    "        Logger object to log the information.\n",
    "    \"\"\"\n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes:\n",
    "            dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    \n",
    "    total = sum(dtypes.values())\n",
    "    \n",
    "    # Logging the information instead of printing\n",
    "    for dtype, count in dtypes.items():\n",
    "        logger.info(f\"{dtype}: {count} ({100 * count / total:.2f}%)\")\n",
    "\n",
    "\n",
    "def create_prompt_format(\n",
    "    sample,\n",
    "    args: Args,\n",
    "    context_field=\"sentence\",\n",
    "    response_field=\"label_decoded\"\n",
    "):\n",
    "    if instruction_prompt is None or not isinstance(instruction_prompt, str) or instruction_prompt.strip() == \"\":\n",
    "        raise ValueError(f\"Invalid instruction prompt received: {instruction_prompt}. It must be a non-empty string.\")\n",
    "    if system_prompt is None or not isinstance(system_prompt, str) or system_prompt.strip() == \"\":\n",
    "        raise ValueError(f\"Invalid system prompt received: {system_prompt}. It must be a non-empty string.\")\n",
    "\n",
    "    if not sample or not all(\n",
    "        isinstance(sample[field], str) for field in [context_field, response_field]\n",
    "    ):\n",
    "        raise ValueError(\"Fields must be a non-empty strings.\")\n",
    "\n",
    "    prompt = (\n",
    "        args.B_INST\n",
    "        + args.B_SYS\n",
    "        + args.system_prompt\n",
    "        + args.E_SYS\n",
    "        + args.instruction_prompt\n",
    "        + sample[context_field]\n",
    "        + args.E_INST\n",
    "    )\n",
    "    sample[\"text\"] = str(prompt[0])\n",
    "    return sample\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_seq_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_dataset(\n",
    "    args: Args,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    max_seq_length: int,\n",
    "    dataset: Dataset\n",
    "):\n",
    "\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    seed = args.seed\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "\n",
    "    _prompt_format_function = partial(\n",
    "        create_prompt_format, args=args\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        _prompt_format_function,\n",
    "        batched=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(\n",
    "        preprocess_batch, max_seq_length=max_seq_length, tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_seq_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    # SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    cls = (\n",
    "        bnb.nn.Linear4bit\n",
    "    )  # if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "def create_peft_config(args:Args, modules):\n",
    "    peft_config = LoraConfig(\n",
    "        # Pass our list as an argument to the PEFT config for your model\n",
    "        target_modules=modules,\n",
    "        # Dimension of the LoRA matrices we update in adapaters\n",
    "        r=args.lora_r,\n",
    "        # Alpha parameter for LoRA scaling\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        # Dropout probability for LoRA layers\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    return peft_config\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "def merge_evaluation_results(baseline_results, final_results):\n",
    "    all_metrics = set(baseline_results.keys()).union(final_results.keys())\n",
    "\n",
    "    data = {\n",
    "        'Metric': list(all_metrics),\n",
    "        'Baseline': [baseline_results.get(metric, None) for metric in all_metrics],\n",
    "        'After Fine-tuning': [final_results.get(metric, None) for metric in all_metrics]\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a213e-05b6-4b88-8773-b685cca59a61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "add4fe4c-75bd-4a2d-af9a-b64803ad72d3",
   "metadata": {},
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    # Note that other metrics may not have a `use_aggregator` parameter\n",
    "    # and thus will return a list, computing a metric for each sentence.\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce28982d-5130-40a3-8b6a-36830fd4b40f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66f2ee95-5450-4f8d-924d-86e31e0862f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def metric_computer(tokenizer):\n",
    "    bleu_metric = evaluate.load('bleu')\n",
    "    rouge_metric = evaluate.load('rouge')\n",
    "\n",
    "    def compute_metrics(p):\n",
    "        \"\"\"\n",
    "        This function receives a datasets.arrow_dataset.Dataset instance that contains the model's predictions and labels\n",
    "        and computes the custom metrics (BLEU, ROUGE).\n",
    "        \"\"\"\n",
    "        predictions, references = p\n",
    "        \n",
    "        # Decode the logits to get predicted token ids\n",
    "        pred_ids = np.argmax(p.predictions, axis=2)\n",
    "\n",
    "        # Decode the predicted and label ids token ids to text\n",
    "        pred_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in pred_ids]\n",
    "        label_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in p.label_ids]\n",
    "\n",
    "        # You might need to preprocess predictions and labels, depending on your needs\n",
    "        # For example, you might want to decode the predicted token ids to text\n",
    "        # print(\"p:\",p)\n",
    "        # print(\"Predictions:\", p.predictions)\n",
    "        # print(\"Labels:\", p.label_ids)\n",
    "\n",
    "        # Calculate Metrics\n",
    "        bleu_score = bleu_metric.compute(predictions=pred_texts, references=label_texts)\n",
    "        rouge_score = rouge_metric.compute(predictions=pred_texts, references=label_texts)\n",
    "\n",
    "        # Combine the metrics in a dictionary and return\n",
    "        return {\n",
    "            'bleu': bleu_score,\n",
    "            'rouge': rouge_score,\n",
    "        }\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df6e0251-a31f-4fff-8ddd-1c2789db7501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    logger.info(\"Starting the training process...\")\n",
    "    logger.info(\"Creating BitsAndBytesConfig...\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        # Activate k-bit precision base model loading\n",
    "        load_in_4bit=args.load_in_4bit,\n",
    "        load_in_8bit=args.load_in_8bit,\n",
    "\n",
    "        # Activate nested quantization for 4-bit base models (double quantization)\n",
    "        bnb_4bit_use_double_quant=args.bnb_4bit_use_double_quant,\n",
    "        bnb_8bit_use_double_quant=args.bnb_4bit_use_double_quant,\n",
    "        \n",
    "        # Quantization type (fp4 or nf4)\n",
    "        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n",
    "        bnb_8bit_quant_type=args.bnb_4bit_quant_type,\n",
    "        \n",
    "        # Compute dtype for 4-bit base models\n",
    "        bnb_4bit_compute_dtype=args.bnb_4bit_compute_dtype,\n",
    "        bnb_8bit_compute_dtype=args.bnb_4bit_compute_dtype,\n",
    "        \n",
    "    )\n",
    "\n",
    "    logger.info(\"Loading the Tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=False)\n",
    "    logger.info(\"Tokenizer pad token specified as the EOS token\")\n",
    "    tokenizer.pad_token = EOS\n",
    "    # logger.info(\"Tokenizer configured to fix overflow issues with fp16 training\"\n",
    "    # tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    compute_metrics_function = metric_computer(tokenizer)\n",
    "    logger.info(compute_metrics_function)\n",
    "\n",
    "    logger.info(\"Loading the CausalLM...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        load_in_4bit=args.load_in_4bit,\n",
    "        load_in_8bit=args.load_in_8bit,\n",
    "        device_map=args.device_map,\n",
    "        max_memory=CUDA_MAX_MEMORY,\n",
    "        torch_dtype=compute_dtype,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=False\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    max_seq_length = get_max_length(model)\n",
    "    logger.info(f\"Model Dtypes after loading ...\")\n",
    "    log_dtypes(model, logger)\n",
    "\n",
    "    logger.info(\"Loading train dataset...\")\n",
    "    train_dataset = load_dataset(\n",
    "        f\"{args.organization}/{args.task_name}\", str(args.seed)\n",
    "    )[\"train\"]\n",
    "\n",
    "    logger.info(\"Preprocessing train dataset...\")\n",
    "    preprocessed_train_dataset = preprocess_dataset(\n",
    "        args=args,\n",
    "        dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length,\n",
    "    )\n",
    "    logger.info(\"Train dataset preprocessed.\")\n",
    "\n",
    "    logger.info(\"Loading test dataset...\")\n",
    "    test_dataset = load_dataset(f\"{args.organization}/{args.task_name}\", str(args.seed))[\n",
    "        \"test\"\n",
    "    ]\n",
    "    logger.info(\"Preprocessing test dataset...\")\n",
    "    preprocessed_test_dataset = preprocess_dataset(\n",
    "        args=args,\n",
    "        dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length,\n",
    "    )\n",
    "    logger.info(\"Test dataset preprocessed.\")\n",
    "\n",
    "    logger.info(\"Getting the model's memory footprint...\")\n",
    "    logger.info(model.get_memory_footprint())\n",
    "    logger.info(\"Using the prepare_model_for_kbit_training method from PEFT...\")\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)\n",
    "    logger.info(f\"Model Dtypes after preparing for kbit training ...\")\n",
    "    log_dtypes(model, logger)\n",
    "    logger.info(\"Get lora module names...\")\n",
    "    layers_for_adapters = find_all_linear_names(model)\n",
    "    logger.info(f\"Layers for PEFT Adaptation: {layers_for_adapters}\")\n",
    "    logger.info(\"Create PEFT config for these modules and wrap the model to PEFT...\")\n",
    "    peft_config = create_peft_config(args, layers_for_adapters)\n",
    "    logger.info(f\"Model Dtypes before PEFT model ...\")\n",
    "    log_dtypes(model, logger)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    logger.info(f\"Model Dtypes after PEFT model ...\")\n",
    "    log_dtypes(model, logger)\n",
    "    logger.info(\"Information about the percentage of trainable parameters...\")\n",
    "    log_trainable_parameters(model, logger)\n",
    "\n",
    "    logger.info(\"Make output directory...\")\n",
    "    output_dir = args.output_dir / \"final_checkpoint\"\n",
    "    output_dir.mkdir(mode=0o777, parents=True, exist_ok=True)\n",
    "\n",
    "    logger.info(\"Define TrainingArguments...\")\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        fp16=args.fp16,\n",
    "        bf16=args.bf16,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        max_grad_norm = args.max_grad_norm,\n",
    "        weight_decay = args.weight_decay,\n",
    "        optim=args.optim,\n",
    "        learning_rate=args.learning_rate,\n",
    "        lr_scheduler_type=args.lr_scheduler_type,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        max_steps=args.max_steps,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        save_safetensors=args.save_safetensors,\n",
    "        load_best_model_at_end=args.load_best_model_at_end,\n",
    "        push_to_hub=False,\n",
    "        evaluation_strategy=args.evaluation_strategy,\n",
    "        logging_dir=logging_dir,\n",
    "        report_to=args.report_to,\n",
    "        save_strategy=args.save_strategy,\n",
    "        save_steps=args.save_steps,\n",
    "        logging_strategy=args.logging_strategy,\n",
    "        logging_steps=args.logging_steps,\n",
    "        group_by_length = args.group_by_length\n",
    "    )\n",
    "\n",
    "    logger.info(\"Defining SFTTrainer...\")\n",
    "    callbacks = [PeftSavingCallback()]\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_arguments,\n",
    "        packing=args.packing,\n",
    "        max_seq_length=max_seq_length,\n",
    "        train_dataset=preprocessed_train_dataset,\n",
    "        eval_dataset=preprocessed_test_dataset,\n",
    "        peft_config=peft_config,\n",
    "        callbacks=callbacks,\n",
    "        dataset_text_field=\"text\",\n",
    "        neftune_noise_alpha=args.neftune_noise_alpha,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.predict_with_generate = args.predict_with_generate\n",
    "\n",
    "    # Evaluate the model before fine-tuning to get the baseline performance\n",
    "    logger.info(\"Evaluating the baseline performance of the model before fine-tuning...\")\n",
    "    baseline_results = trainer.evaluate()\n",
    "    logger.info(f\"Baseline evaluation results: {baseline_results}\")\n",
    "  \n",
    "    logger.info(\"Running trainer.train() ...\")\n",
    "    trainer.train()\n",
    "    if args.report_to == 'wandb':\n",
    "        wandb.finish()\n",
    "\n",
    "    try:\n",
    "        metrics = trainer.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "    except Exception as e:\n",
    "        logger.debug(\"metrics block failed\")\n",
    "        logger.error(e)\n",
    "\n",
    "    logger.info(\"trainer.evaluate() ...\")\n",
    "    final_results = trainer.evaluate()\n",
    "    logger.info(f\"Final evaluation results: {final_results}\")\n",
    "\n",
    "    results_df = merge_evaluation_results(baseline_results, final_results)\n",
    "    display(results_df)\n",
    "    \n",
    "    \n",
    "    logger.info(\"trainer.save_state() ...\")\n",
    "    trainer.save_state()\n",
    "    \n",
    "    logger.info(\"Saving tokenizer and last checkpoint of the model...\")\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    model = trainer.model\n",
    "    log_dtypes(model, logger)\n",
    "    model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14df431-ef9b-4c8b-ad02-76f3860b9dc6",
   "metadata": {},
   "source": [
    "## Run Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e95ecfe-ec53-4565-9e3d-af64d3653e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - Starting the training process...\n",
      "llama2_finetune - INFO - Starting the training process...\n",
      "llama2_finetune - INFO - Creating BitsAndBytesConfig...\n",
      "llama2_finetune - INFO - Creating BitsAndBytesConfig...\n",
      "llama2_finetune - INFO - Loading the Tokenizer...\n",
      "llama2_finetune - INFO - Loading the Tokenizer...\n",
      "loading file tokenizer.model from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/tokenizer.model\n",
      "loading file tokenizer.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/tokenizer_config.json\n",
      "llama2_finetune - INFO - Tokenizer pad token specified as the EOS token\n",
      "llama2_finetune - INFO - Tokenizer pad token specified as the EOS token\n",
      "llama2_finetune - INFO - <function metric_computer.<locals>.compute_metrics at 0x7f024da13ee0>\n",
      "llama2_finetune - INFO - <function metric_computer.<locals>.compute_metrics at 0x7f024da13ee0>\n",
      "llama2_finetune - INFO - Loading the CausalLM...\n",
      "llama2_finetune - INFO - Loading the CausalLM...\n",
      "loading configuration file config.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Detected 4-bit loading: activating 4-bit loading for this model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410678107b9e4a7ab5d57d4ba96f0828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/AD/gmatlin3/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/af6df14e494ef16d69ec55e9a016e900a2dde1c8/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "llama2_finetune - INFO - Model Dtypes after loading ...\n",
      "llama2_finetune - INFO - Model Dtypes after loading ...\n",
      "llama2_finetune - INFO - torch.bfloat16: 262410240 (7.50%)\n",
      "llama2_finetune - INFO - torch.bfloat16: 262410240 (7.50%)\n",
      "llama2_finetune - INFO - torch.uint8: 3238002688 (92.50%)\n",
      "llama2_finetune - INFO - torch.uint8: 3238002688 (92.50%)\n",
      "llama2_finetune - INFO - Loading train dataset...\n",
      "llama2_finetune - INFO - Loading train dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - Preprocessing train dataset...\n",
      "llama2_finetune - INFO - Preprocessing train dataset...\n",
      "llama2_finetune - INFO - Train dataset preprocessed.\n",
      "llama2_finetune - INFO - Train dataset preprocessed.\n",
      "llama2_finetune - INFO - Loading test dataset...\n",
      "llama2_finetune - INFO - Loading test dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - Preprocessing test dataset...\n",
      "llama2_finetune - INFO - Preprocessing test dataset...\n",
      "llama2_finetune - INFO - Test dataset preprocessed.\n",
      "llama2_finetune - INFO - Test dataset preprocessed.\n",
      "llama2_finetune - INFO - Getting the model's memory footprint...\n",
      "llama2_finetune - INFO - Getting the model's memory footprint...\n",
      "llama2_finetune - INFO - 3829940224\n",
      "llama2_finetune - INFO - 3829940224\n",
      "llama2_finetune - INFO - Using the prepare_model_for_kbit_training method from PEFT...\n",
      "llama2_finetune - INFO - Using the prepare_model_for_kbit_training method from PEFT...\n",
      "llama2_finetune - INFO - Model Dtypes after preparing for kbit training ...\n",
      "llama2_finetune - INFO - Model Dtypes after preparing for kbit training ...\n",
      "llama2_finetune - INFO - torch.float32: 262410240 (7.50%)\n",
      "llama2_finetune - INFO - torch.float32: 262410240 (7.50%)\n",
      "llama2_finetune - INFO - torch.uint8: 3238002688 (92.50%)\n",
      "llama2_finetune - INFO - torch.uint8: 3238002688 (92.50%)\n",
      "llama2_finetune - INFO - Get lora module names...\n",
      "llama2_finetune - INFO - Get lora module names...\n",
      "llama2_finetune - INFO - Layers for PEFT Adaptation: ['k_proj', 'v_proj', 'gate_proj', 'down_proj', 'up_proj', 'o_proj', 'q_proj']\n",
      "llama2_finetune - INFO - Layers for PEFT Adaptation: ['k_proj', 'v_proj', 'gate_proj', 'down_proj', 'up_proj', 'o_proj', 'q_proj']\n",
      "llama2_finetune - INFO - Create PEFT config for these modules and wrap the model to PEFT...\n",
      "llama2_finetune - INFO - Create PEFT config for these modules and wrap the model to PEFT...\n",
      "llama2_finetune - INFO - Model Dtypes before PEFT model ...\n",
      "llama2_finetune - INFO - Model Dtypes before PEFT model ...\n",
      "llama2_finetune - INFO - torch.float32: 262410240 (7.50%)\n",
      "llama2_finetune - INFO - torch.float32: 262410240 (7.50%)\n",
      "llama2_finetune - INFO - torch.uint8: 3238002688 (92.50%)\n",
      "llama2_finetune - INFO - torch.uint8: 3238002688 (92.50%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2_finetune - INFO - Model Dtypes after PEFT model ...\n",
      "llama2_finetune - INFO - Model Dtypes after PEFT model ...\n",
      "llama2_finetune - INFO - torch.float32: 422318080 (11.54%)\n",
      "llama2_finetune - INFO - torch.float32: 422318080 (11.54%)\n",
      "llama2_finetune - INFO - torch.uint8: 3238002688 (88.46%)\n",
      "llama2_finetune - INFO - torch.uint8: 3238002688 (88.46%)\n",
      "llama2_finetune - INFO - Information about the percentage of trainable parameters...\n",
      "llama2_finetune - INFO - Information about the percentage of trainable parameters...\n",
      "llama2_finetune - INFO - Trainable params: 159907840 || All params: 3660320768 || Trainable%: 4.368683788535114\n",
      "llama2_finetune - INFO - Trainable params: 159907840 || All params: 3660320768 || Trainable%: 4.368683788535114\n",
      "llama2_finetune - INFO - Make output directory...\n",
      "llama2_finetune - INFO - Make output directory...\n",
      "llama2_finetune - INFO - Define TrainingArguments...\n",
      "llama2_finetune - INFO - Define TrainingArguments...\n",
      "using `logging_steps` to initialize `eval_steps` to 25\n",
      "PyTorch: setting up devices\n",
      "llama2_finetune - INFO - Defining SFTTrainer...\n",
      "llama2_finetune - INFO - Defining SFTTrainer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'compute_metrics' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train(args)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # Empty VRAM\n",
    "    if 'trainer' in locals() or 'trainer' in globals():\n",
    "        del trainer\n",
    "    if 'model' in locals() or 'model' in globals():\n",
    "        del model\n",
    "    if 'pipe' in locals() or 'pipe' in globals():\n",
    "        del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca479ce5-c1fd-421b-9dc1-65eb9f6da283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b57a8b-a390-483c-b9db-76864f6b0bed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c031399-440e-4e5a-a027-f4ed807b5d03",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114bfec5-c6d6-42a5-ad5e-c98bb18e8d4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815d953d-ad90-4d14-b235-b96bfaa880b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebb578-40eb-457a-a808-69cea1073731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reload final model checkpoint and save\n",
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir / \"final_checkpoint\",\n",
    "    device_map=args.device_map,\n",
    "    torch_dtype=compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b9009-34c3-4cbd-8432-1f42d9a670f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dtypes(new_model, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0fa19-3387-4cbe-a198-02dfe0044a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=args.device_map,\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    "    torch_dtype=compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f9caf-0b19-4988-8e0a-e1a64b06db0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dtypes(base_model, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9e75e-9400-480d-a6f4-52885f1dd5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This method merges the LoRa layers into the base model. This is needed to use it as a standalone model.\n",
    "peft_model = PeftModel.from_pretrained(base_model, args.output_dir / \"final_checkpoint\")\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.output_dir / \"final_checkpoint\", pad_token=EOS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50493fcc-4465-48f8-b3aa-63dc9cbdd413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dtypes(peft_model, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f4eba1-306b-4103-b60f-936c0c934440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save inference\n",
    "merged_checkpoint_dir = args.output_dir / \"final_merged_checkpoint\"\n",
    "peft_model.save_pretrained(merged_checkpoint_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(merged_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aded31-09ba-44fe-a8d8-c5f325ade9c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5579b-20c9-4728-9c87-0c49d9b96477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# push to hub\n",
    "peft_model.push_to_hub(repo_name, private=True, use_temp_dir=True)\n",
    "tokenizer.push_to_hub(repo_name, private=True, use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a7f0f-b2aa-4747-96d9-bd2c573ff7ad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf3add-e3a1-48ff-992c-e2cbd3f84131",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219faba9-5221-4113-a2ad-7c9b75d99abb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d5341f3-ed43-4477-ab55-7f5e17132c07",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5946fdf6-a38d-4fcd-b95d-03ddee64fb5d",
   "metadata": {},
   "source": [
    "# TODO: move to configs or args\n",
    "temperature = 0.0  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "do_sample = False\n",
    "max_new_tokens = 256\n",
    "top_k = 10\n",
    "top_p = 0.92\n",
    "repetition_penalty = 1.0  # 1.0 means no penalty\n",
    "num_return_sequences = 1  # Only generate one response\n",
    "num_beams = 1\n",
    "\n",
    "\n",
    "def generate(model=None, tokenizer=None, dataset=None):\n",
    "    input_ids = tokenizer(dataset[\"text\"])\n",
    "\n",
    "    # Ensure that input_ids is a PyTorch tensor\n",
    "    # input_ids = torch.tensor(input_ids).long()\n",
    "\n",
    "    # Move the tensor to the GPU\n",
    "    input_ids = input_ids.cuda()\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_beams,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "        ),\n",
    "    )\n",
    "    seq = generation_output.sequences\n",
    "    output = tokenizer.decode(seq[0])\n",
    "    return output.split(\"[/INST]\")[-1].strip()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcf87bd2-5225-4bb2-aad6-21334f18abe9",
   "metadata": {},
   "source": [
    "test_dataset = load_dataset(f\"{args.organization}/{args.task_name}\", str(args.seed))[\n",
    "    \"test\"\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b58cf0cb-956d-4e8e-b671-51112f0b96b6",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f8edb2e-404a-4b7e-a99f-716df980ef1a",
   "metadata": {},
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=args.device_map,\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    "    torch_dtype=compute_dtype,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = EOS\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "\n",
    "preprocessed_test_dataset = preprocess_dataset(\n",
    "    tokenizer=tokenizer, max_length=max_length, seed=args.seed, dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfc41d53-c122-4bc8-a175-55c7d447da76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "eb54e720-441c-4720-8d10-1a97d18f8604",
   "metadata": {},
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"What is a large language model?\"\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=max_length\n",
    ")\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6780860-93a4-4ea0-be66-b5ee12fc6d98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "fe70ba14-2ea1-489e-a9ac-1f6c1e791f5d",
   "metadata": {},
   "source": [
    "# N_GENS = preprocessed_test_dataset.num_rows\n",
    "N_GENS = 10\n",
    "\n",
    "output_list = []\n",
    "for i in range(N_GENS):\n",
    "    output_list.append(\n",
    "        generate(model=model, tokenizer=tokenizer, dataset=preprocessed_test_dataset)\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "178af452-6cdb-446f-9593-1743ae29ba58",
   "metadata": {},
   "source": [
    "output_list\n",
    "\n",
    ".replace(\n",
    "            \"</s>\", \"\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "607c3c3a-7ba6-43ce-87b9-8e21c2f43164",
   "metadata": {},
   "source": [
    "### Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dae6d595-6f45-490c-a851-8568c1a98373",
   "metadata": {},
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_checkpoint_dir,\n",
    "    device_map=args.device_map,\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    "    torch_dtype=compute_dtype,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_checkpoint_dir)\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "\n",
    "preprocessed_dataset = preprocess_dataset(\n",
    "    tokenizer=tokenizer, max_length=max_length, seed=args.seed, dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27a41155-eec1-43f6-9645-84ac71c2609b",
   "metadata": {},
   "source": [
    "df_test_dataset = convert_dataset(dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a59ca28a-3f65-4e96-9aef-47a9149e51e2",
   "metadata": {},
   "source": [
    "input_list = df_test_dataset[\"prompt\"].to_list()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "079cd4b5-6bbe-47c9-afce-e387c61a8c11",
   "metadata": {},
   "source": [
    "# # Suppress specific warnings from torch.utils.checkpoint\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.filterwarnings(\n",
    "#         \"ignore\", category=UserWarning, module=\"torch.utils.checkpoint\"\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conference",
   "language": "python",
   "name": "conference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
