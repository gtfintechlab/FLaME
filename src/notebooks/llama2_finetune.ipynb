{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf37c31a-e659-477b-a0bc-fec3df7b1b6b",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning of Llama2 on Financial Phrasebank"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "id": "5f24ab92-aae1-4082-85b4-305ee4e3badd",
   "metadata": {},
   "source": [
    "!pip install trl\n",
    "\n",
    "!pip install cupy-cuda11x\n",
    "\n",
    "!pip install cuda-python\n",
    "\n",
    "!pip install cuda-python\n",
    "\n",
    "conda install -c conda-forge cudatoolkit-dev\n",
    "\n",
    "!pip install flash-attn\n",
    "\n",
    "!pip3 install --force-reinstall torch --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33951517-366d-4863-a9fc-439de8145b83",
   "metadata": {},
   "source": [
    "## Not for Public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb44486-b7d1-4fb6-acc5-63a51ff2a9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/AD/gmatlin3/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "HF_AUTH = \"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\"\n",
    "huggingface_hub.login(token=HF_AUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d12fda-d164-4699-afc9-f1b1dd90639c",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c428c4bf-1348-496d-bd72-b8c03612a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import wandb\n",
    "\n",
    "# # set the wandb project where this run will be logged\n",
    "# os.environ[\"WANDB_PROJECT\"] = f\"llama2_sft_fomc\"\n",
    "# # save your trained model checkpoint to wandb\n",
    "# os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
    "# # turn off watch to log faster\n",
    "# os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"fa69ffc6a97578da0410b553042cbb8b3bf5fcaf\"\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = f\"llama2_sft\"\n",
    "\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2424d-31dc-4d8e-a71f-8087ec428ee6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07d79c3-9fa8-4cd2-8089-431db6e5c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79f553-61c2-46e9-90fe-306180ac37be",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd658e99-4cc9-41f6-b592-d436ab94c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "SRC_DIRECTORY = Path().cwd().resolve().parent\n",
    "\n",
    "if str(SRC_DIRECTORY) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIRECTORY))\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"llama2_finetune\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "consoleHandler.setFormatter(formatter)\n",
    "logger.addHandler(consoleHandler)\n",
    "\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import uuid\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from peft import (\n",
    "    AutoPeftModelForCausalLM,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LlamaConfig,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaModel,\n",
    "    LlamaTokenizer,\n",
    "    TextGenerationPipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    "    pipeline,\n",
    ")\n",
    "# from trl import SFTTrainer\n",
    "\n",
    "from llama.instructions import (\n",
    "    B_INST,\n",
    "    B_SYS,\n",
    "    E_INST,\n",
    "    E_SYS,\n",
    "    TASK_MAP,\n",
    "    llama2_prompt_generator,\n",
    ")\n",
    "\n",
    "BOS, EOS = \"<s>\", \"</s>\"\n",
    "\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8483511b-f826-4016-90d1-6d424c99cd6a",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ee5afe-e618-44b5-972b-55ef4cb0d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uid(id_length=8, dt_format=\"%y%m%d\"):\n",
    "    date_str = datetime.now().strftime(dt_format)\n",
    "\n",
    "    # Generate a short UUID\n",
    "    uid = str(uuid.uuid4())[:id_length]\n",
    "\n",
    "    # Combine\n",
    "    uid = f\"{uid}_{date_str}\"\n",
    "\n",
    "    return uid\n",
    "\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    # SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    cls = (\n",
    "        bnb.nn.Linear4bit\n",
    "    )  # if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def print_dtypes(model):\n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes:\n",
    "            dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items():\n",
    "        total += v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v / total)\n",
    "\n",
    "\n",
    "def create_peft_config(modules):\n",
    "    peft_config = LoraConfig(\n",
    "        # Pass our list as an argument to the PEFT config for your model\n",
    "        target_modules=modules,\n",
    "        # Dimension of the LoRA matrices we update in adapaters\n",
    "        r=lora_r,\n",
    "        # Alpha parameter for LoRA scaling\n",
    "        lora_alpha=lora_alpha,\n",
    "        # Dropout probability for LoRA layers\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    return peft_config\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "# def convert_dataset(ds):\n",
    "#     prompts = llama2_prompt_generator(TASK_INSTRUCTION, ds['text'])\n",
    "#     labels = [decode(L).upper() for L in ds['label']]\n",
    "#     df = pd.DataFrame.from_dict({'prompt': prompts, 'response': labels})\n",
    "#     return df\n",
    "\n",
    "\n",
    "def create_prompt_formats(\n",
    "    sample, context_field=\"sentence\", response_field=\"label_decoded\"\n",
    "):\n",
    "    SYS_PROMPT = f\"\"\"\"Discard all the previous instructions.\n",
    "    Below is an instruction that describes a task.\n",
    "    Write a response that appropriately completes the request.\"\"\"\n",
    "\n",
    "    INST_PROMPT = TASK_INSTRUCTION\n",
    "\n",
    "    if not INST_PROMPT or not isinstance(INST_PROMPT, str):\n",
    "        raise ValueError(\"Instruction must be a non-empty string.\")\n",
    "    if not sample or not all(\n",
    "        isinstance(sample[field], str) for field in [context_field, response_field]\n",
    "    ):\n",
    "        raise ValueError(\"Fields must be a non-empty strings.\")\n",
    "\n",
    "    prompt = (\n",
    "        B_INST\n",
    "        + B_SYS\n",
    "        + SYS_PROMPT\n",
    "        + E_SYS\n",
    "        + INST_PROMPT\n",
    "        + sample[context_field]\n",
    "        + E_INST\n",
    "    )\n",
    "    sample[\"text\"] = prompt\n",
    "    return sample\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_dataset(\n",
    "    tokenizer: AutoTokenizer, max_length: int, seed: int, dataset: Dataset\n",
    "):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)  # , batched=True)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(\n",
    "        preprocess_batch, max_length=max_length, tokenizer=tokenizer\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        # remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ce6ed-6ca7-45dd-ac24-fc4589898a75",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b79b9f3-054d-429b-b6f9-6b3a7dc0d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORGANIZATION = \"gtfintechlab\"\n",
    "\n",
    "TASK = \"fomc_communication\"\n",
    "\n",
    "SEEDS = (5768, 78516, 944601)\n",
    "\n",
    "SEED = SEEDS[0]\n",
    "\n",
    "device_map = \"auto\"  # {\"\":0}\n",
    "DEVICE_MAP = \"auto\"\n",
    "\n",
    "MODEL_PARAMETERS = \"7b\"\n",
    "MODEL_ID = f\"meta-llama/Llama-2-{MODEL_PARAMETERS}-chat-hf\"\n",
    "MODEL_NAME = MODEL_ID.split(\"/\")[-1]\n",
    "OUTPUT_DIR = Path.home() / \"results\" / f\"{MODEL_NAME}_{TASK}\"\n",
    "\n",
    "repo_name = f\"{ORGANIZATION}/{MODEL_NAME}_{TASK}\"\n",
    "\n",
    "Args = namedtuple(\n",
    "    \"Args\",\n",
    "    [\n",
    "        \"task_name\",\n",
    "        \"seed\",\n",
    "        \"device_map\",\n",
    "        \"model_id\",\n",
    "        \"model_name\",\n",
    "        \"output_dir\",\n",
    "        \"organization\",\n",
    "        \"max_steps\",\n",
    "        \"uid\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "args = Args(\n",
    "    task_name=TASK,\n",
    "    seed=SEED,\n",
    "    device_map=DEVICE_MAP,\n",
    "    model_id=MODEL_ID,\n",
    "    model_name=MODEL_ID.split(\"/\")[-1],\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    organization=ORGANIZATION,\n",
    "    max_steps=50,\n",
    "    uid=generate_uid(),\n",
    ")\n",
    "\n",
    "\n",
    "TASK_INSTRUCTION, TASK_DATA = (\n",
    "    TASK_MAP[args.task_name][\"instruction\"],\n",
    "    TASK_MAP[args.task_name][\"data\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2085522a-21d7-4ba2-8440-a2d7b9598dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 32\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.05\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "# max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "# device_map = {\"\": 0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "id": "e9d37f34-cbcc-4551-92ab-cbd8ce4f9c98",
   "metadata": {},
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd23b2-a1e6-4981-9d28-0865462ca4ce",
   "metadata": {},
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbcea4e9-1445-4064-b36b-2355ddca1f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 21:04:18,816 - llama2_finetune - INFO - Using k=2 CUDA GPUs with max memory {0: '41GB', 1: '41GB'}\n",
      "2023-10-11 21:04:18,816 - llama2_finetune - INFO - Using k=2 CUDA GPUs with max memory {0: '41GB', 1: '41GB'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "major, _ = torch.cuda.get_device_capability()\n",
    "if major >= 8:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.bfloat16\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU does not support bfloat16: using fp=16\")\n",
    "    print(\"=\" * 80)\n",
    "    compute_dtype = torch.float16\n",
    "\n",
    "CUDA_N_GPUS = torch.cuda.device_count()\n",
    "CUDA_MAX_MEMORY = f\"{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB\"\n",
    "CUDA_MAX_MEMORY = {i: CUDA_MAX_MEMORY for i in range(CUDA_N_GPUS)}\n",
    "logger.info(f\"Using k={CUDA_N_GPUS} CUDA GPUs with max memory {CUDA_MAX_MEMORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a213e-05b6-4b88-8773-b685cca59a61",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "950cf3a0-489c-4c9b-96e2-440bf9544719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):  # , gradient_checkpointing_enabled=False):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        # Activate 4-bit precision base model loading\n",
    "        load_in_4bit=True,\n",
    "        # Activate nested quantization for 4-bit base models (double quantization)\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        # Quantization type (fp4 or nf4)\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        # Compute dtype for 4-bit base models\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "    )\n",
    "\n",
    "    # Tokenizer configured to fix overflow issues with fp16 training\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, pad_token=EOS)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        device_map=args.device_map,\n",
    "        max_memory=CUDA_MAX_MEMORY,\n",
    "        torch_dtype=compute_dtype,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "    max_length = get_max_length(model)\n",
    "\n",
    "    train_dataset = load_dataset(f\"{args.organization}/{args.task_name}\", str(args.seed))[\n",
    "        \"train\"\n",
    "    ]\n",
    "    preprocessed_train_dataset = preprocess_dataset(\n",
    "        tokenizer=tokenizer, max_length=max_length, seed=args.seed, dataset=train_dataset\n",
    "    )\n",
    "    # test_dataset = load_dataset(f\"{args.organization}/{args.task_name}\", str(args.seed))[\n",
    "    #     \"test\"\n",
    "    # ]\n",
    "    # preprocessed_test_dataset = preprocess_dataset(\n",
    "        # tokenizer=tokenizer, max_length=max_length, seed=args.seed, dataset=test_dataset\n",
    "    # )\n",
    "\n",
    "    # Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    # Get lora module names\n",
    "    layers_for_adapters = find_all_linear_names(model)\n",
    "    print(f\"Layers for PEFT Adaptation: {layers_for_adapters}\")\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(layers_for_adapters)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    output_dir = args.output_dir / \"final_checkpoint\"\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        bf16=True,\n",
    "        # report_to=\"wandb\",\n",
    "        # Batch size per GPU for training\n",
    "        per_device_train_batch_size=1,\n",
    "        # Batch size per GPU for evaluation\n",
    "        per_device_eval_batch_size=4,\n",
    "        # Number of update steps to accumulate the gradients for\n",
    "        gradient_accumulation_steps=4,\n",
    "        # Maximum gradient normal (gradient clipping)\n",
    "        # max_grad_norm = 0.3,\n",
    "        # Initial learning rate (AdamW optimizer)\n",
    "        learning_rate=2e-4,\n",
    "        # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "        # weight_decay = 0.001,\n",
    "        # Optimizer to use\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        # Learning rate schedule (constant a bit better than cosine)\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        # Number of training steps (overrides num_train_epochs)\n",
    "        max_steps=args.max_steps,\n",
    "        # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "        warmup_ratio=0.05,\n",
    "        # Save checkpoint every X updates steps\n",
    "        # save_steps=args.max_steps / 10,\n",
    "        # Log every X updates steps\n",
    "        logging_steps=10,\n",
    "        # Group sequences into batches with same length to save memory and speed up training\n",
    "        # group_by_length = True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=preprocessed_train_dataset,\n",
    "        args=training_arguments,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "    \n",
    "    # trainer = SFTTrainer(\n",
    "    #     model=model,\n",
    "    #     args=training_arguments,\n",
    "    #     max_seq_length=max_length,\n",
    "    #     train_dataset=preprocessed_train_dataset,\n",
    "    #     eval_dataset=preprocessed_test_dataset,\n",
    "    #     peft_config=peft_config,\n",
    "    #     dataset_text_field=\"text\",\n",
    "    #     tokenizer=tokenizer,\n",
    "    # )\n",
    "        \n",
    "    # # Re-enable for inference to speed up predictions for similar inputs\n",
    "    model.config.use_cache = False\n",
    "    print(\"Training...\")\n",
    "    # Suppress specific warnings from torch.utils.checkpoint\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\", category=UserWarning, module=\"torch.utils.checkpoint\"\n",
    "        )\n",
    "        train_result = trainer.train()\n",
    "        metrics = trainer.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print_dtypes(trainer.model)\n",
    "        print(metrics)\n",
    "        # Saving model\n",
    "        print(\"Saving last checkpoint of the model...\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        trainer.model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Empty VRAM\n",
    "    del model\n",
    "    del pipe\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "\n",
    "    gc.collect()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95ecfe-ec53-4565-9e3d-af64d3653e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d84436dba2417092070f504a006e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n",
      "Preprocessing dataset...\n",
      "Layers for PEFT Adaptation: ['gate_proj', 'v_proj', 'q_proj', 'o_proj', 'up_proj', 'down_proj', 'k_proj']\n",
      "trainable params: 79953920 || all params: 3580366848 || trainable%: 2.233120889404459\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mglennmatlin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/AD/gmatlin3/zero-shot/src/notebooks/wandb/run-20231011_210511-ljvfrl9j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/glennmatlin/huggingface/runs/ljvfrl9j' target=\"_blank\">swift-morning-2</a></strong> to <a href='https://wandb.ai/glennmatlin/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/glennmatlin/huggingface' target=\"_blank\">https://wandb.ai/glennmatlin/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/glennmatlin/huggingface/runs/ljvfrl9j' target=\"_blank\">https://wandb.ai/glennmatlin/huggingface/runs/ljvfrl9j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b57a8b-a390-483c-b9db-76864f6b0bed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248dabe-d89a-4800-baab-d64985c83f50",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8f947-63c4-40f5-b267-a476e2592e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload final model checkpoint and save\n",
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir / \"final_checkpoint\",\n",
    "    device_map=args.device_map,\n",
    "    torch_dtype=compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db431f-2ca3-4f1b-b658-9da05372572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dtypes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c88ae3-0dd2-4bc0-b848-96d7074492c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=args.device_map,\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    "    torch_dtype=compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d20c9-effc-4ec1-abc6-2774f978e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dtypes(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42375f0e-a798-4d8d-9273-94aeafd7b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method merges the LoRa layers into the base model. This is needed to use it as a standalone model.\n",
    "peft_model = PeftModel.from_pretrained(base_model, args.output_dir / \"final_checkpoint\")\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.output_dir / \"final_checkpoint\", pad_token=EOS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cda631-a219-4e0d-bd7d-bc26fe452661",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dtypes(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e52f15-e8b1-4d44-9854-1fbccd0fe14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save inference\n",
    "merged_checkpoint_dir = args.output_dir / \"final_merged_checkpoint\"\n",
    "model.save_pretrained(merged_checkpoint_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(merged_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6423cb2a-a1a5-4dcf-bee3-629837625d1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# push to hub\n",
    "peft_model.push_to_hub(repo_name, private=True, use_temp_dir=True)\n",
    "tokenizer.push_to_hub(repo_name, private=True, use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc76d0-8b92-49c7-80e8-77415587ffaf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c20714-ba12-4654-bf78-6fc1f539e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move to configs or args\n",
    "temperature = 0.0  # [0.0, 1.0]; 0.0 means greedy sampling\n",
    "do_sample = False\n",
    "max_new_tokens = 256\n",
    "top_k = 10\n",
    "top_p = 0.92\n",
    "repetition_penalty = 1.0  # 1.0 means no penalty\n",
    "num_return_sequences = 1  # Only generate one response\n",
    "num_beams = 1\n",
    "\n",
    "\n",
    "def generate(model=None, tokenizer=None, dataset=None):\n",
    "    input_ids = tokenizer(dataset[\"text\"])\n",
    "\n",
    "    # Ensure that input_ids is a PyTorch tensor\n",
    "    # input_ids = torch.tensor(input_ids).long()\n",
    "\n",
    "    # Move the tensor to the GPU\n",
    "    input_ids = input_ids.cuda()\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_beams,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "        ),\n",
    "    )\n",
    "    seq = generation_output.sequences\n",
    "    output = tokenizer.decode(seq[0])\n",
    "    return output.split(\"[/INST]\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e1e8c-b9cd-4b33-8cba-7118b90825a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(f\"{args.organization}/{args.task_name}\", str(args.seed))[\n",
    "    \"test\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8d781-e6d4-442a-a766-9ae456d3fd79",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d203a4-8ffc-4835-9292-f4ed71c34f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=args.device_map,\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    "    torch_dtype=compute_dtype,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = EOS\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "\n",
    "preprocessed_test_dataset = preprocess_dataset(\n",
    "    tokenizer=tokenizer, max_length=max_length, seed=args.seed, dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4208bdb-4d0f-4b0d-87f3-364bd8e874bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b576e3-4143-4a20-9b74-f38149fcea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"What is a large language model?\"\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=max_length\n",
    ")\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603f1d2-2f9b-46a2-a4bb-159ce7ff03fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4493b30-a580-4300-ba59-f04bd20dd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_GENS = preprocessed_test_dataset.num_rows\n",
    "N_GENS = 10\n",
    "\n",
    "output_list = []\n",
    "for i in range(N_GENS):\n",
    "    output_list.append(\n",
    "        generate(model=model, tokenizer=tokenizer, dataset=preprocessed_test_dataset)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "id": "178af452-6cdb-446f-9593-1743ae29ba58",
   "metadata": {},
   "source": [
    "output_list\n",
    "\n",
    ".replace(\n",
    "            \"</s>\", \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c99f9-57bf-46a9-914a-b4ae7c9df252",
   "metadata": {},
   "source": [
    "### Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a025ddd-fd45-44e3-a6a9-93cf76c3c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_checkpoint_dir,\n",
    "    device_map=args.device_map,\n",
    "    max_memory=CUDA_MAX_MEMORY,\n",
    "    torch_dtype=compute_dtype,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_checkpoint_dir)\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "\n",
    "preprocessed_dataset = preprocess_dataset(\n",
    "    tokenizer=tokenizer, max_length=max_length, seed=args.seed, dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27a41155-eec1-43f6-9645-84ac71c2609b",
   "metadata": {},
   "source": [
    "df_test_dataset = convert_dataset(dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a59ca28a-3f65-4e96-9aef-47a9149e51e2",
   "metadata": {},
   "source": [
    "input_list = df_test_dataset[\"prompt\"].to_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conference Environment",
   "language": "python",
   "name": "conference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
