{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb5afa9-a87c-463c-aaaf-ee44da5604be",
   "metadata": {},
   "source": [
    "# HuggingFacifying the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c30b4-f8c0-40a2-9b77-e5ba2e2743fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SRC_DIRECTORY = Path().cwd().resolve().parent\n",
    "DATA_DIRECTORY = Path().cwd().resolve().parent.parent / \"data\"\n",
    "\n",
    "if str(SRC_DIRECTORY) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIRECTORY))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "HF_ORGANIZATION = \"gtfintechlab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba7020-c5e5-4bff-8741-1dcb8b0cdf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelMapper:\n",
    "    def __init__(self, task):\n",
    "        self.mappings = {\n",
    "            'finer_ord': {\n",
    "                0: \"other\",\n",
    "                1: \"person_b\",\n",
    "                2: \"person_i\",\n",
    "                3: \"location_b\",\n",
    "                4: \"location_i\",\n",
    "                5: \"organisation_b\",\n",
    "                6: \"organisation_i\"\n",
    "            },\n",
    "            'fomc_communication': {\n",
    "                0: \"dovish\",\n",
    "                1: \"hawkish\",\n",
    "                2: \"neutral\"\n",
    "            },\n",
    "            'numclaim_detection': {\n",
    "                0: \"outofclaim\",\n",
    "                1: \"inclaim\"\n",
    "            },\n",
    "            'sentiment_analysis': {\n",
    "                0: \"positive\",\n",
    "                1: \"negative\",\n",
    "                2: \"neutral\"\n",
    "            }\n",
    "        }\n",
    "        if task not in self.mappings:\n",
    "            raise ValueError(f\"Task {task} not found in mappings.\")\n",
    "        self.task = task\n",
    "        \n",
    "    def encode(self, label_name):\n",
    "        reversed_mapping = {v: k for k, v in self.mappings[self.task].items()}\n",
    "        return reversed_mapping.get(label_name, -1)\n",
    "    \n",
    "    def decode(self, label_number):\n",
    "        return self.mappings[self.task].get(label_number, \"undefined\").upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596488a-a552-496b-b389-ec9c9e56d47c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a44d2-8b5c-4dfb-b2e2-1dc049f10578",
   "metadata": {},
   "source": [
    "## FOMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "201e4c76-365e-4804-97b3-9403940769a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggify_data_fomc(TASK=None, SEED=None, SPLITS=['train','test']):\n",
    "    try:\n",
    "        mapper = LabelMapper(TASK)\n",
    "        # Initialize the nested structure\n",
    "        hf_dataset = DatasetDict()\n",
    "        \n",
    "        # Load data\n",
    "        for SPLIT in SPLITS:\n",
    "            # Load and preprocess the dataframe\n",
    "            data_split = pd.read_excel(DATA_DIRECTORY/ TASK / SPLIT / f\"lab-manual-split-combine-{SPLIT}-{SEED}.xlsx\", index_col=0)\n",
    "            data_split.rename(columns={'label': 'label_encoded'}, inplace=True)\n",
    "            data_split['label_decoded'] = data_split['label_encoded'].apply(lambda x: mapper.decode(x))\n",
    "    \n",
    "            # Convert the dataframe to Hugging Face's Dataset and store it in the nested dictionary\n",
    "            hf_dataset[SPLIT] = Dataset.from_pandas(data_split)\n",
    "\n",
    "        # Push to HF Hub\n",
    "        hf_dataset.push_to_hub(\n",
    "            f\"{HF_ORGANIZATION}/{TASK}\",\n",
    "            config_name=str(SEED),\n",
    "            private=True,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "600b6703-56fa-47a9-999a-d4230f4a774f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b834257ce25d4946974dc38d170a560d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe8d03bd8fb472aaf04450958503591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801dd0ab1ece4d4494058e6e9b791b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4db1d697479499bbdbe03fefff8fd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c63d35f1d7f406583e78eaa4abcde36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8909100c114dfba39e06e3a16def45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c995eba4613c4ec5a2a033be41f6defa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/754 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142174c6524a4dfaabb90adafa122736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3a2b8d052f4f7ebf59f36252e32a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373a54ed1a454253a40631ed48f0d89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10b4bb32f9d4c39b9bb1c602e27b770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21595c3987024347b5f4ec1d088292a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SPLITS = ['train', 'test']\n",
    "\n",
    "TASK = \"fomc_communication\"\n",
    "\n",
    "SEEDS = (5768, 78516, 944601)\n",
    "\n",
    "for SEED in list(reversed(SEEDS)):\n",
    "    huggify_data_fomc(TASK=TASK, SEED=SEED, SPLITS=SPLITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434732f-ba98-4337-8f10-fd311876c696",
   "metadata": {},
   "source": [
    "## Financial PhraseBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e896fa91-6284-4ed2-9656-fd5c2f42273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fpb_hub_datasets(seed=None):\n",
    "    configs = [\n",
    "        \"sentences_allagree\",\n",
    "        \"sentences_75agree\",\n",
    "        \"sentences_66agree\",\n",
    "        \"sentences_50agree\",\n",
    "    ]\n",
    "\n",
    "    for config in tqdm(configs, desc=\"Configs\"):\n",
    "        try:\n",
    "            fpb_dataset = load_dataset(\"financial_phrasebank\", config)\n",
    "            config_short = config.replace(\"sentences_\", \"\")\n",
    "\n",
    "            texts = fpb_dataset[\"train\"][\"sentence\"]\n",
    "            labels = fpb_dataset[\"train\"][\"label\"]\n",
    "\n",
    "            splits = {}\n",
    "\n",
    "            # Splitting the data\n",
    "            train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "                texts, labels, test_size=0.2, random_state=seed\n",
    "            )\n",
    "\n",
    "            # Storing in the dictionary\n",
    "            splits[seed] = {\n",
    "                \"train\": Dataset.from_dict(\n",
    "                    {\n",
    "                        \"context\": train_texts,\n",
    "                        \"response\": list(map(decode, train_labels)),\n",
    "                    }\n",
    "                ),\n",
    "                \"test\": Dataset.from_dict(\n",
    "                    {\"context\": test_texts, \"response\": list(map(decode, test_labels))}\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            # Push to HF Hub\n",
    "            splits[seed][\"train\"].push_to_hub(\n",
    "                f\"{ORGANIZATION}/{DATASET}-{config_short}-{seed}\",\n",
    "                config_name=\"train\",\n",
    "                private=True,\n",
    "            )\n",
    "            splits[seed][\"test\"].push_to_hub(\n",
    "                f\"{ORGANIZATION}/{DATASET}-{config_short}-{seed}\",\n",
    "                config_name=\"test\",\n",
    "                private=True,\n",
    "            )\n",
    "\n",
    "            return splits\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing config {config}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba7a34d8-e212-429d-b001-44c662a8a53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615c6c3d4bd940f499f8a0e7958c4898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Configs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321bc93d571e46878fd4fb9f8be75368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683693180f404909bd9b952906e38011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/803 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15504c9e886a444ca6f4d94fe8d89914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute the function\n",
    "splits = make_fpb_hub_datasets(SEED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
